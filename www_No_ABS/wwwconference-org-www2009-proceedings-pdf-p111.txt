One of the great promises of the Web is to connect all the participating people, products, companies and institutions.
For these connections to be useful it is necessary to select the few fruitful connections and disregard the many worthless connections.
Automatic recommender systems can aid users in this selection [7, 19], some of the best known being found at Amazon, [18], Net ix [1], and Yahoo!.
The task at hand is to predict, for a particular user, which items they will be interested in.
There are two sources of information which are typically used to achieve this.
  Content (Meta-Data) The content-based approach makes use of descriptions (feature vectors) of both users and items.
Users may be described by properties such as age and gender and items may be described by properties such as author and manufacturer.
Typical examples of content based recommendation systems include web search engines and social matchmaking sites.
  Collaborative Filtering The collaborative  ltering approach uses only the identities (IDs) of users and items.
Implicit descriptions of the user and items are obtained from a (sparse) matrix of ratings of items by users [3].
We can learn about a user by the items they have previously rated and the users who have rated items in common with them.
Typically both of these types of information are available and ideally we would like to use both of them to make predictions.
When a user is relatively new to the system, predictions should be improved by making use of extra information about the user, thus addressing the well known  cold-start  problem [11].
However, once we have observed su cient user ratings we want to be able to make fully personalized predictions for the user based on their speci c ratings rather than their features.
In addition it is desirable that a recommender system can be applied  exibly in a wide variety of scenarios.
Frequently, data on user preferences is in the form of ratings on an ordinal scale (for example Net ix [1]) where each user s interpretation of this scale may be di erent.
In other cases we may only have data showing which items were clicked on by a user, where we assume that a user clicking on an item provides implicit evidence that it is interesting to them (for example the Google news recommender system [6]).
In order to provide up-to-date recommendations and react rapidly to a user s evolving tastes we need the system to be arrives.
It is crucial that the system is scalable, with memory and processor requirements increasing in proportion to the number of users and items and not in proportion to the total number of previous ratings.
In this work we present a model based on a compressed representation of the matrix of the values of items for users.
In the literature the prototypical method of this kind is based on a singular value decomposition (see, for example [12]).
We represent each user and item by a vector of features.
In order to take account of user-speci c and item-speci c tastes we can include a binary feature for every user ID and item ID in the set of features (only one of which is active for each user and item).
Each feature is associated with a latent  trait  vector and the linear combination of the trait vectors for a particular user or item, weighted by the feature values for that user or item, provides a total trait vector for the user or item.
We model the value that an item has for a user as the inner product between the trait vector for the item and the trait vector for the user.
Key contributions of this work are:
 good recommendations in a cold-start situation which will automatically become more personalised for longer term users (Section 2.1).
three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don t like) and observation of a set of ordinal ratings on a user-speci c scale (Section

 a user s taste or a user s personal rating scale to drift over time (Section 2.4).
tion of Variational Message Passing (VMP) and Expectation Propagation (EP) (Section 3.1).
an online training method that can incrementally take account of new data so the system can immediately re ect the latest user preferences (Section 3.3).
In Sections 4.1 and 4.2 we present experimental results for the Net ix and MovieLens data sets.
Initially, let us assume that the recommender system receives tuples (x, y,  , r) of user descriptions x   (cid:0) n , item descriptions y   (cid:0) m , other features describing the context of this rating     (cid:0) f and ratings r   (cid:0).
We de ne the K dimensional user trait vector as s = Ux where U is a K   n matrix of latent user traits where each element uki is the contribution of feature i to user trait dimension k. Similarly we de ne the K dimensional item trait vector as t = Vy for an K   m item trait matrix, V. In general we also model a bias, b =   w where w is a latent set of weights.
Frequently the features   will be a combination of the user description (cid:2) features x and the item description features y only so in these cases we may write b = x v where u and v are the latent user and item biases.
u + y (cid:2) (cid:2) Now, the rating, r, is modeled as p(r|s, t, b) = N (r|s (cid:2) t + b,  2 ) where   is the standard deviation of the observation noise.
Thus we adopt a bi-linear form in which the similarity between a user and an item is given by the inner product of a vector of user traits and a vector of item traits.
The expected rating is proportional to the lengths ||s|| and ||t|| of the trait vectors as well as the cosine of the angle between them.
The parameter K is chosen such that K (cid:3) n and K (cid:3) m so computational resource requirements (in time and memory) should scale as m + n rather than mn.
An important special case of this model is the case when only the identity of each user and item is known and no additional properties.
In this case, the model reduces to a collaborative  ltering approach if we represent user i by the feature vector x := ei and item j by y := ej, where ei denotes the ith unit vector, i.e., the vector with 1 in position i and 0 everywhere else.
If the bias is zero then the expression (cid:2) for the expected rating simpli es to E[r] = u i vj.
Here ui and vj are the ith and jth column of U and V respectively.
In other words, each user and each item is represented by a K-dimensional vector in trait space directly.
To make the connection to standard methods, suppose we have a complete matrix R of ratings for n users and m items.
Then the singular-value decomposition (SVD) yields the optimal solution for the matrices U and V in the least-squares sense, that is (USVD, VSVD) := argmin
 i vj   rij) (cid:2) (u
 n (cid:0)i=1 m (cid:0)j=1 This function can be directly minimized but due to the large sparsity of the matrix R in real-world tasks (for example,
 employed [17, 12, 2].
The model parameters to be learned are the variables U and V which determine how users and items are mapped to the K dimensional trait space and w, the value of bias features for the rating.
We represent our prior beliefs about the values of these parameters by independent Gaussian distributions on each of the components of the matrices U and V and the vector w. For example we have p(U) =
 (cid:2)k=1 n (cid:2)i=1 N (uki;  ki,  2 ki).
We choose this factorizing prior because it reduces memory requirements to two parameters (a mean and standard deviation) for each component and it allows us to perform e cient inference (see Section 3.1).
One advantage of the approach described in this paper is that it allows us to model di erent types of user-item rating data in a  exible way.
Up to this point we have assumed that the rating, r, is observed directly, however this need not be the case.
A common scenario is that users provide feedback about which items they like or dislike via an ordinal scale.
For example, on Net ix, users are asked to rate movies from one to  ve stars.
These ranks can only be compared, but not subtracted from one another.
In addition, each user s interpretation of the scale may be di erent and the mapping from rank to latent rating may not be linear.
We assume that for each user-item pair for which data is available we observe a rank l   1,    , L. We relate the latent rating r to ranks l via a cumulative threshold model [4].
For each user, u, we maintain user-speci c thresholds bu   (cid:0) L 1 which divide the latent rating axis into L consecutive intervals (bu(i 1), bu(i)) of varying length each of which representing the region in which this user gives the same rank to an item.
Formally, we de ne a generative model of a ranking as p(l = a|bu, r) = a 1 i=1 (r >  bu(i 1))	L 1 i=a (r <  bu(i 1)) if 1 < a < L if a = 1 if a = L i=1 (r <  bu(i 1)) i=1 (r >  bu(i 1))

 p( bui|bui,   ) = N ( bui; bui,   2 (cid:3)(cid:4) (cid:5) where (1) ) and we place an independent Gaussian prior on the thresholds so p(bui) = N (bui;  i,  2 i ).
The indicator function ( ) is equal to 1 if the proposition in the argument is true and 0 if it is false.
A special case of the ordinal regression feedback model is for L = 1,   = 0,  2
 observing a binary variable c   {TRUE, FALSE} which is related to the latent rating by, p(c = TRUE) = p(r > 0).
This is useful in the situation where we are only provided with binary information about whether or not a user likes a particular item.
For example, a user may provide feedback to a news recommender service by which stories they click on [6].
Ideally a recommender system needs to be able to track non-stationary data.
A user s tastes will drift with time and an item s popularity may rise and fall with changing trends.
In addition, if we use the user-speci c threshold model discussed in Section 2.3.1 a user s individual rating scale may also drift with time.
One well known phenomenon where this may occur is  anchoring  where a user tends to be more likely to give an item a high rating if they have recently given other items a high rating.
We model these dynamics by assuming that the latent variables U, V and w (and b if we use the ordinal regression feedback model) drift with time by the addition of Gaussian noise each time step.
For the example of the threshold model we have p(b(t+1) ,  2) where t is an index over time steps.
At time t0 we use the prior p(b(0) i ).
Analogous models may be used for each of the other latent variables.
|b(t) l ) = N (b(t+1) ) = N (bi;  i,  2 ; b(t) i l l l i = 1       n xi uki
 



 tk 

 j = 1       m yj vkj
 k = 1, .
.
.
, K  d
  b
 wd
 d = 1       f
 N (  ;   ,  2)

   sk zk  r r
 
 observation belief Figure 1: Factor Graph for Bi-linear Rating Model.
The large rectangles or plates indicate parts of the graph which are repeated with repetition indexed by the variable in the corner of the plate.
The factors labeled   are sum factors of the form [z = x+y].
The numbered arrows correspond to messages.
Messages (1) are from the sum factors mapping from the user/ item/ bias descriptions to the latent trait space, m sk (sk), m tk (tk), and m b(b).
Message (2) is m zk (zk), from the product factor.
Message (3) is m r( r), the message from the total sum factor to the latent rating  r.
Message (4) is the message from the Gaussian noise factor N (  ;   ,  2) to the noisy latent rating r. The rest of the messages are the reverse messages back to the prior.
Given a stream of rating tuples (x, y,  , r) we train the model in order to learn posterior distributions over the values of the parameters U, V, and w. This can be accomplished e ciently by message passing.
The algorithm sketched below involves computing a sequence of messages.
We used the Infer.net library to perform these computations [15].
The model described in Section 2.1 can be further factorized by introducing some intermediate latent variables zk to represent the result of each component, sktk of the inner product.
That is, p(zk|sk, tk) = (zk = sktk).
Now the latent rating (before adding noise) is given by p( r|z, b) = ( r = k zk + b).
From Section 2.1 we can see that p(sk|U, x) = (sk = i ukixi) and p(tk|V, y) = (tk = j vkj yj) and p(b|w,  ) = (b =i  iwi).
izes as p(s, t, U, V, w, z,  r, r|x, y,  ) = p(r| r)p( r|z, b)p(b|w,  )p(U)p(V)p(w) p(zk|sk, tk)p(sk|U, x)p(tk|V, y).
(cid:2)k=1 The posterior distribution over the U V and w variables given an observed rating tuple, (x, y,  , r), is given by summing out the latent variables: p(U, V, w|r, x, y,  )   p(s, t, U, V, w, z,  r, r|x, y, f )dsdtdzd r.
(2) (cid:8)s(cid:8)t(cid:8)z(cid:8) r The factor graph for this model is shown in Figure 1.
A factor graph is a bipartite graph with (square) factor nodes corresponding to factors in a function and (circular) variable nodes representing variables in the function.
The edges of the graph reveal the dependencies of factors on variables [10].
Message passing is used to compute the marginal of a joint distribution by assuming a full factorization of the joint distribution and in this way approximating each factor as follows (for an example distribution p(v) of a set of variables v): p (v)  (cid:2)f f (v)  (cid:2) f (cid:2)i V (  f) m  f i (vi) m  f i (vi) , =(cid:2)i (cid:2) f   F (i) (cid:9)  f (v) (cid:10)(cid:11) (cid:12) (cid:9)  p(vi) (cid:10)(cid:11) (cid:12) where V (  f ) is the set of all variable indices involved in the approximate factor  f and  F (i) is the set of all approximate factors in which variable vi is involved.
Message passing is about best approximating each factor f by a factor  f .
This optimization is achieved by minimizing an  divergence (a generalization of the Kullback-Leibler divergence) between  p(v)   f (v)/  f (v) (see [14] for details).
We approximate all factors by Gaussian densities so all messages have the functional form of a Gaussian density.
For all of the factors in the model (Figure 1) apart from the product factor ((zk = sk   tk), labeled * in the diagram) we choose to minimise the Kullback-Leibler (KL) divergence KL(f||  f ) between the true and approximate factors (alpha-divergence with     0).
This corresponds to the Sum-Product algorithm (for exact messages) and Expectation Propagation (for approximate messages) [10, 16].
For the exact factors we compute factor to variable messages according to the general update equation for a message from a factor f to a variable v: mf v(v) =(cid:8) f (v) (cid:2)vj V (f )\v mvj f (v)(vj )dv (3) (which follows directly from the distributive law of sums and products).
The messages are denoted on the factor graph by arrows and the numbers indicate the order in which we compute them.
Since all messages are Gaussian these computations are exact for all factors in the model apart from the product factor.
Firstly we pre-compute the sums for the linear Gaussian input models.
For example for the user model we have (from equation 3), m sk (sk) = n ukixi) (cid:8) (sk =(cid:0)i = N(cid:13)sk;(cid:0)i N (uki;  ki,  2 (cid:2)i=1 kixi(cid:14) .
 kixi,(cid:0)i  2 ki)dU (4) (5) This is also computed for the item model and the bias model and are labeled  1  in Figure 1.
Next we compute the message from the product factor (labeled  2  in Figure 1).
This factor presents us with a di culty because if we were to use the EP approximation here the variance of the message would always grow.
To see this, note that given mzk (zk) = N (zk;  k,  2 k) the most likely value for sk is sk =  k/tk.
This gives rise to a true bi-modal hyperbolic message density on both sk and tk (one mode for tk > 0 and one mode for tk < 0).
Since the Kullback Leibler distance attempts to capture the support of the distribution, the variance of the approximation messages m sk and m tk will grow to cover both modes of the distribution.
(cid:8)mz (cid:9) (cid:8)t(cid:9) Covering both modes is undesirable, we just need to make sure that the model chooses a single mode locally to break the symmetry.
Therefore, for this factor we choose to minimize the KL divergence with the arguments swapped, KL(  f||f ), which is equivalent to a local variational approximation [20,
 sages to and from the product factor f (s, t, z) = (z = s   t) m z(z) = N z;(cid:8)s(cid:9)(cid:8)t(cid:9) ,(cid:16)s2(cid:17)(cid:16)t2(cid:17)   (cid:8)s(cid:9)2 (cid:8)t(cid:9)2 , z (cid:17)   (cid:8)mz (cid:9)2 (cid:14)(6) m s(s) = N(cid:13)s; corresponding to messages (2) and (7) in Figure 1 respectively.
Here, (cid:8)t(cid:9) denotes the mean of the approximate (Gaussian) marginal distribution p(t) and (cid:8)t2(cid:9) denotes the non-centered second moment of the marginal p(t) (the message for m t is obtained from m s by swapping the roles of s and t).
These messages have the undesirable property that the variance of m s scales according to the precision of mt  and not the variance.
In other words, a large uncertainty in mt  leads to a large reduction in uncertainty in m s.
The approximate inference algorithm reduces uncertainty every time that these messages get updated.
,(cid:16)m2 (cid:8)t2(cid:9) (cid:8)t2(cid:9) Note that the inputs to the computation of m z are the current estimates of the marginal distributions p(t) and p(s).
The marginal distributions are given by the product of the incoming messages to the variable (labeled 7 and 1 in Figure 1), for example p(tk) = m tk (tk)  mtk (tk).
Since message m tk is initially unavailable we initialize it to a uniform distribution and as we iterate the message passing schedule (2..7) we can obtain better estimates of the value of this message until eventual convergence.
Message (3) from the total sum factor is calculated by an analogous equation to equation (4) (belief propagation) and message (4) corresponds to simply adding  2 to the variance of message (3) (this also corresponds to standard belief propagation).
See [8] for more details on these computations.
Message (5) is the message from the observation factor.
For a direct observation of the value of the rating this message would be a delta function centered on the observed r [di = r    bi] [dj = r    bj ] di [di > 0] dj [dj > 0] N (  bi; bi,   2)  bi bi N (  bj; bj ,   2)  bj bj i = 0       a   2 p(bi) p(bj) j = a   1       L   2 Figure 2: Factor graph for user-speci c ordinal regression output model.
This represents the model given in equation 1.
Inference is performed by message passing.
value, a, that is, (r = a).
If another output model is used then this message will result from performing a message passing schedule on that model and will be a function of the downward message (4).
Message (6) is the Gaussian sum-product algorithm message from a sum factor to its summonds (derived from equation 3, see [8]) and messages (7) are calculated using equation (6).
The message passing schedule (2...7) is iterated until the marginal distribution of the predicted rating, p(r), no longer changes.
Finally messages (8) (for example m vkj ) are calculated and the posterior distribution in each case is obtained by multiplying this message by the prior.
For example, for the variables vkj with prior N (vkj ;  kj ,  2 kj) the posterior is updated as p(vkj ) := m vkj (vkj)   N (vkj;  kj ,  2
 kj).
Inference for the ordinal regression observation model is performed by approximate Gaussian EP message passing on the factor graph (Figure 2).
The message update equations are described in [8] (and can be computed with Infer.net).
This calculation is performed for each iteration of the schedule described in section 3.1.
Note that now the marginal distribution for each user threshold must be stored.
The binary probit model is a special case of the ordinal regression model (see Section 2.3.2) [8].
Once the posterior marginals for the variables U, V and w have been calculated as described in section 3.1 then we can discard all of the messages and continue, using this posterior as the prior for the variables for the next rating.
In this way we pass once through the data, in an online fashion, incrementally incorporating each additional rating into the model beliefs.
This method is a form of Assumed-Density Filtering (ADF) [16].
There are several advantages of adopting this approach.
Firstly, the memory overhead is small as no messages need to be stored.
Secondly, the online algorithm can immediately take account of new data when it becomes available, which may be desirable if we wish to produce up-to-date recommendations in practice.
The approximation can be improved by passing over the data several times, using the full EP message passing schedule.
In this case we must store the messages into the U, V, w variables: m uki (uki), m vkj (vkj ) and m wn (wn) in addition to the posterior marginal distributions p(U), p(V) and p(w) (equal to the product of these messages with the priors).
The  rst time we pass through the data the updates are exactly the same as ADF except that we now store these messages.
In subsequent passes through the data we obtain the messages from the marginals by dividing out the stored incoming messages.
For example for the user messages: muki (uki) = p(uki) m uki (uki) .
Since all messages and marginals are Gaussian distributions this is a straightforward computation.
Once these messages are obtained for the data point in question we perform the iterative procedure of section 3.1 in order to compute updated messages (labeled 8 in Figure 1) to the user and item variables: m(cid:6)  wn (wn).
Then we update the marginal distribution to be the product of the outgoing and incoming messages.
For example for the user messages: p(cid:6)  vkj (vkj ) and m(cid:6)  uki (uki), m(cid:6) (uki) = m(cid:6)  uki (uki)muki (uki),  uki (uki), m(cid:6) and store the latest messages m(cid:6) and m(cid:6) for more details of this approach.
 vkj (vkj )  wn (wn) in place of the previous versions.
See [5] One downside of the full EP method is that the memory requirements scale in proportion to the number of ratings in the data set.
This means that for large data sets the full forward backward approach may not be feasible, unless the data is broken into batches which are trained in sequence.
However, we show in Section 4.1 that performance is still competitive even if on line ADF is performed.
Also, if we assume a continuous stream of data is arriving (e ectively an in nite data set) then an online approach makes even more sense from a practical point of view.
When using ADF for training, dynamics is trivial to implement: with each time step add the variance of the dynamics factor to the variance of the variable for which dynamics is being modeled.
For example with the threshold model, after each day we update the variance of threshold beliefs by  2(t+1) is the variance of the marginal belief for threshold level l for user u at time step t.
In a practical system ADF would probably be used so this is the method by which dynamics would be applied.
ul +  2 where  2(t) :=  2(t) ul ul For full EP with dynamics, we divide each variable into separate copies, one for each time step in which it is involved with an observation.
In this case we must cache the able as well as the marginal distributions for the copies.
After each sweep forwards through the data we must perform a backward pass when the backward messages and marginals are updated from future observations, thus smoothing backwards in time.
For a variable, say b, the backward messages from the noise factor, f (b(t 1), b(t)) = N (b(t); b(t 1),  2), are calculated by m f (b(t 1),b(t) ) b(t 1) (b(t 1)) = (cid:2) (cid:8) N (b(t) ; b(t 1),  2 ) p(b(t)) mf (b(t 1),b(t) ) b(t) (b(t)) db(t), and the marginal distribution for the variable b(t 1) is updated by EP Convergence.
Thresholds and MetaData enabled.
Lam et al.
p(cid:6) (b(t 1) ) = p(b(t 1) )   m (cid:2) f b(t 1) mf b(t 1) .
EP Iterations

 The procedure is described in more detail in [5].
In general, fully factorised message passing algorithms (as described in [14]) can be parallelised as long as we take care that messages and cached marginals are always consistent.
Frequently one would cache the marginal, p(vi), and calculate messages from a variable to a factor, mi f , by dividing a cache of p(vi) by the message mf i.
As long as both the cache p(vi) and the incoming messages mf i are updated in one atomic step, computations of messages can be paral-lelised.
We exploited parallelism in training on the Net ix data set (see Section 4.2).
For an X core machine we divided the movies into X partitions and processed the sequences of ratings for each of these partitions in parallel.
We used a monitor to ensure that two ratings by the same user are always processed in sequence to avoid a race condition.
This method gave a 4  speedup on an 8 core machine, allowing us to process the 100,000,000 Net ix ratings in 2 hours with



 Firstly we investigate the predictive performance on the MovieLens data set.
The data set contains 1,000,206 ratings of 3,952 movies by 6,040 users.
Ratings are on an ordinal scale from 1 to 5.
The data is 95.7% sparse.
In addition to the ID for each user and item some meta data is also provided for 88% of the users and 98% of the items.
The meta data provided is shown in Table 1.
In order to measure accuracy in a cold start situation we adopt the methodology of [11] and [13].
We randomly divide the users into two sets: a test set containing 10% of the users and a training set containing the rest of the users.
First the model is trained on all the ratings by the training users according to the procedure outlined in Section 3.1.
For each of the test users we train the model on a random subset of T % of their ratings for T = 5% and 75%.
Then we use the model to predict the rest of the ratings for that user.
Following [11] we report the Mean Absolute Error i=1 |  ri   ri|, where ri is the true rating and
  ri is the predicted rating.
We compare to their best result (MAE=0.6927 for T = 75%) in each of the  gures in this
 EP Convergence.
Thresholds and MetaData disabled (IDs only).
Lam et al.
EP Iterations

 Figure 3: MovieLens Test MAE as a function of number of EP iterations, T = 75%.
The best result presented by [11] is included for comparison.
section.
Note that we include the result of [11] on our plots for T = 5% although their training procedure had T = 75%.
The x and y vectors are sparse binary, with a 1 present for each true feature.
The bias uses the same features as the latent embedding so b = x u + y v.
(cid:2) (cid:2) We perform experiments using both the ordinal regression observation model and the direct observation model.
Where the ordinal regression observation model is used we use the median of the predictive distribution p(l|x, y,  ) in order to make the optimal decision to minimize MAE error measure.
If the direct observation model is used then we use the mean of the predictive distribution p(r|x, y,  ).
Figure 3 shows the e ect of performing additional forward-backward passes of EP.
The best result from [11] is included for comparison.
Firstly we can note that more EP iterations help improve performance, and the larger the number of latent dimensions, K, the more forward-backward passes are needed in order to achieve convergence.
In this  gure we also compare the vanilla version of the model with no meta data (only user ID and item ID and the direct observation model - no ordinal regression) with the full model including all meta data features (Table 1) and user-speci c ordinal













 other or none speci ed academic/educator artist clerical/admin college / grad student customer service doctor / health care executive / managerial farmer homemaker student Age< 18





 Job









 lawyer programmer retired sales / marketing scientist self-employed technician / engineer tradesman / craftsman unemployed writer

 Age > 55 Gender

 Movie Genre



 Thriller Sci-Fi

 War
 Action Adventure Animation Children s Comedy Crime










 Drama Fantasy Film Noir Horror Table 1: Meta data provided by the MovieLens data set, left: user meta data, right: item meta data.
In addition to these features we also use the IDs of the users and the IDs of the items.
The x and y vectors are sparse binary, with a 1 present for each true feature.
regression threshold model.
The full model strongly outperforms the vanilla model.
In addition the vanilla model bene ts more each additional EP iteration, taking more iterations before achieving best performance.
Each sweep through the 1,000,000 ratings takes approximately 5 minutes on a 3.6GHz Pentium 4, for K = 20.
Figure 4 compares the performance of the model with and without the user-speci c threshold model.
The thresholds appear to help enormously to improve performance.
Note that the threshold model outperforms [11] even with ADF (one EP iteration) and K=0 (zero latent dimensions!).
For the case where K=0 the model is a simple linear model of the features in combination with the user-speci c threshold model.
To understand why the model performs so well, even with K=0 consider Figure 5.
This shows a histogram of rating frequency for some randomly chosen users from the MovieLens data set.
Note how each user seems to mainly use only a few rating levels each so accuracy is greatly improved by using the user-speci c threshold model.
Figure 6 compares the performance of the model with and without the additional meta data features.
We compare performance for T = 5% and T = 75%.
The meta data features seem to improve performance, especially for higher values of K. The threshold features are especially helpful in the case where T = 5% as here we face a severe cold start challenge - making predictions having seen only 5% of a user s ratings.
In this case the meta data features push the performance above that of of [12] even though they used T = 75% for training and we only used T = 5%.
In October 2006, Net ix released a large movie rating data set and challenged the data mining, machine learning and computer science communities to develop systems that could beat the accuracy of their internal system Cinematch by a certain amount.
The data were collected between October
 ratings Net ix obtained during this time period.
The training data set consists of 100,480,507 ratings from 480,189 randomly-chosen, anonymous users on 17,770 movie titles.
Lim & Teh (2007) -















Table 2: Net ix RMSE scores for di erent settings.
ADF is a single EP pass through the data.
The EP3 run involves splitting the data into batches of 2,000,000 ratings and passing over each batch 3 times using EP before discarding the messages and moving on to the next batch (necessary to avoid running out of memory).
With only ID features, as is the case for Net ix, the model is similar to the approach of [12] and their results are included for comparison.
By using K = 50 and performing 20 EP iterations on 5,000,000 ratings, and then incorporating the rest of the ratings by ADF gives an RMSE of 0.910.
As part of the training data, Net ix also provides test data (the  probe  set), containing 1,408,395 ratings.
Since there is no information available per user and movie other than their IDs, we use unit vectors x := ei and y := ej to represent a user i and a movie j.
We report root mean squared error (RMSE) scores according to convention for this data set.
We use the ordinal regression prediction model and our prediction for each test data point is the expected rating value under the distribution over ratings5 Table 2 shows the performance of the model for various settings.
ADF corresponds to a single EP pass through the data.
The EP3 run was performed by splitting the data into batches of 2,000,000 ratings and passing over each batch 3 times using EP before discarding the messages m uki (uki), m vkj (vkj ) and m wn (wn) and moving on to the next batch (to avoid running out of memory).
Note that we achieve comparable results to [12] with the EP3 approach.
It is interesting to visualize the learned embedding of some users and movies into trait space for K = 2 (see Figure
 a=1 a  p(l = a|x, y,  ).
ADF, T=75%, thresholds on MetaData Off MetaData On Lam et al.
EP10, T=75%, thresholds on MetaData Off MetaData On Lam et al.
ADF, T=5%, thresholds on MetaData Off MetaData On Lam et al.
EP10, T=5%, thresholds on MetaData Off MetaData On Lam et al.
Figure 6: Movielens Test MAE, left:T =75%, right: T =5%.
Comparing performance with and without meta data features.
 Meta data o    means only ID features are included.
The top plot is produced by training the model by ADF (a single EP iteration) and the bottom plot is produced by training the model on 10 EP iterations.
plotted against the means for the second trait dimension for each user and each item.
Note that the learned embedding separates drama movies as much as possible from action moves in inner product distance.
For any given user i in this space, movies j in the  preference cone  {tj : s i tj + b > c} (cid:2) are preferred.
We have depicted such a cone in the graph.
Pictorially, a high ranking of a movie by a user leads to both a shift and rotation of the user towards the movie vector as well as a shift and rotation of the movie towards the user.
This will bring other movies into the preference cone.
This plot was produced by training with ADF.
As further EP iterations are performed the users and movies become more evenly distributed in a ball in trait space allowing the model to separate more categories of movies.
One training procedure which boosts performance while retaining an incremental training method for incorporating new ratings was to  rst perform a number of full EP iterations on the ratings of a random subset of 5% of the users (5,000,000 ratings), called the  seed  users.
Since we have a large amount of training data per movie, even 5% of the data is enough to learn a good embedding of the movies in trait space.
Then we incorporate the ratings of the rest of the users incrementally using ADF.
For 20 iterations of EP on the seed users and K = 50 we achieve an RMSE of 0.910 on the probe set.
It is worth pointing out that by the standards of many real world applications even the Net ix dataset is tiny.
In a practical online setting where new ratings are arriving continuously we e ectively have an in nite amount of data and, as the amount of data becomes very large, ADF could approach the same performance as full EP.
Finally, to illustrate a di erent type of application of a recommender system, we applied the model to the analysis of logs of clicks on adverts displayed above the search results on Live.com.
The data consisted of 8,000,000 page views, each corresponding to a single user search query.
At each page view three adverts are displayed ( impressions ).
For each of the 24,000,000 impressions we know whether or not the advert was clicked on.
We also have a large amount of meta data for the advert such as the ad title, advertiser account number and so on.
Besides the advert information, the only information we have about the user is their query.
We also have a number of features which are related to both the user and advert, for example, the  matchtype  feature which indicates the degree of match between the advert and query, depending on what keywords the advertiser has selected as being relevant to their products and services.
ADF, T=75%, MetaData on No Thresholds With Thresholds Lam et al.
EP10, T=75%, MetaData on No Thresholds With Thresholds Lam et al.
Figure 4: Movielens Test MAE, T =75%.
The ordinal regression observation model (thresholds) is compared to the direct observation model (no thresholds).
The top plot is produced by training the model by ADF (a single EP iteration) and the bottom plot is produced by training the model with
 There are a large number of features available, including ones which indicate the degree of correspondence between an advert and a user, so a simple linear model (K = 0) performs strongly.
Therefore most of the features were only used for the bias (the   vector).
For K > 0 we used user and item features derived from the words in the query and the ad title.
The x vector was chosen to be a sparse binary vector of the occurrence of words in the query from a lexicon of popular words and the y vector was chosen to be a vector indicating the occurrence of popular words in the advert title.
The feedback model is the binary probit model (Section 2.3.2) where  click  means positive feedback (c = TRUE) and  no click  means negative feedback (c = FALSE).
Figure 8 shows an embedding of query words and ad title words for K = 2 and training with ADF on a subset of the data.
With two latent dimensions, the model appears to learn to separate navigational queries in one dimension from other queries by putting words like  com  and  Site  in one direction.
After further EP iterations the adverts and queries become more distributed in trait space, leading to better performance.
l a t o
 f o n o i t c a r
 l a t o
 f o n o i t c a r












 User A




 Rating User C


 Rating

 l a t o
 f o n o i t c a r
 l a t o
 f o n o i t c a r











 User B




 Rating User D


 Rating

 Figure 5: Relative frequency of use of each rating level for four users chosen at random from the Movielens data set.
Notice that user A never uses rating 5 but users C and D both  nd 5 their most popular rating.
None of the users frequently use rating 1.
We have presented a large scale recommender system in which user and item meta data are integrated.
Experiments on the MovieLens data set show that meta data features are helpful, particularly for dealing with the cold-start problem with users new to the system.
The user feedback model is  exible and results show that an ordinal regression model for user feedback can greatly improve accuracy.
For training, inference is achieved by a novel combination of Variational Message Passing and EP.
We can achieve state-of-the-art performance if we use ADF for training which is an online, incremental method so recommendations can always be up to date.
Results are improved further by using full EP for training but in practice we anticipate using ADF as new data will continuously be arriving and the performance of ADF will approach that of full EP.
In order to predict what items may be of interest to a user we potentially have to consider all available items.
In order to make this process more e cient we are investigating two approaches: hashing [6] and Kd trees [9] and this is the main focus of future work.
The system presented here is being developed for deployment in a commercial online service.
