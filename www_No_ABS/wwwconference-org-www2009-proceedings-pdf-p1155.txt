In Wikipedia, each article describes one concept.
Meanwhile, one concept is usually described in multiple languages, each language corresponding with one article.
All documents associated with one concept (concept-unit) are similar in their topics.
This motivates us to use topic modeling algorithms to mine multilingual topics from Wikipedia.
We propose a novel approach to model multilingual topics from Wikipedia data.
All term by document matrices of L different languages are treated separately.
A group of  universal  topics are used for modeling documents from different languages.
The topics are types of representations and each representation corresponds with one language.
The links among documents describing the same concept are utilized to align topic representations: all these documents follow the constraint of sharing one identical topic distribution.
Based on this unified modeling framework, new documents of different languages can be represented within one same vector space using the universal multilingual topics.
Different from previous research work, our approach does not require additional linguistic resources like bilingual dictionaries or translation tools.
Also, we exploit Wikipedia to extract multilingual topics applicable across multiple languages, instead of aligning documents in word or sentence level.
With multilingual topics, it is very flexible to organize and utilize Web content written in different languages.
Our experiments on text classification and document recommendation task indicate our topic modeling approach is effective.
inherently multilingual: each has L Copyright is held by the author/owner(s).
We adapt Latent Dirichlet Allocation to model multilingual topics (ML-LDA).
We assume all the documents of a concept unit, although in different languages, share identical topic distribution.
Figure 1 presents the graphical model of ML-LDA.
Figure 1.
Graphical model representation of ML-LDA.
jLk, r The notations are similar to those in LDA [1][2].
Here Lj denotes denotes the word distribution for topic k in one language and Language Lj.
We modify Gibbs Sampling [2] method for the estimation of ML-LDA.
Here in one concept-unit, documents in different languages share the same topic distribution but use different word distribution for each topic.
Thus we compute zp ( by using: r w = ) r zk |   Lic ,, j Lic ), ,( , j
 j t k n
 v jL
 =   Lic ,( ), ,   n ( v Lk , j j t +  

 v +    
 j j   n
 p
 = k +   k ic m ),( ,   n
 ( p     + m p where t denotes the index of the current word in Gibbs Sampling jLV is the vocabulary size of language Lj.
m is the index procedure.
of concept-unit.
We compute by using: jLk, r   Ltk ,, j = n   =
 v jL
 j i Lk , n ( + v Lk , j j t  
 v +  
 ) j

 We have built a document-aligned comparable corpus from Wikipedia which was released on March 12, 2008.
We only use
 ML-LDA is used, we set the hyper parameters   and   to be 0.5/K and 0.1 respectively, where K is universal-topic number ranging from 50 to 600, with 50 as the step size.
For each value of K, the model is estimated using 200 Gibbs Sampling iterations.
Table 1 shows some example universal-topics produced by ML-LDA algorithm with K = 400.
You can find that each universal-distribution of Chinese words and the second line is associated with English word distribution.
Words on each line are ranked by probability score in decreasing order.
Table 1.
Sample of Universal-topics 1st:  (universe)  (theory)  (principle of relativity)  universe black relativity theory matter time gravitational   2nd: (football)  (year)  (ball)  (football player)   football team cup national season world league scored   We can also use another way to verify the effectiveness of our ML-LDA approach: word mapping between two languages.
Given two words from different languages, we can measure their distance through their probability distributions over universal-topics.
Table 2 gives two word mapping samples.
Table 2.
Word mapping samples  (computer): computer controller ibm plugged computers Computer:          

 With universal-topics, we can map the documents of interest in different languages into the universal-topic space.
In this section, we study how such representation can help cross-lingual applications.
For experiment purpose, we collected a group of English and Chinese Web pages from Open Directory Project (ODP) website.
8 first level categories are used for experiments.
Cross-lingual text classification (CLTC) addresses the problem of using texts labeled in one language to help classify texts in another language [4][5].
We have built two CLTC tasks: 1) classify Chinese pages by using the training data in English (En-to-Ch); 2) classify English pages by using the training data in Chinese (Ch-to-En).
Support Vector Machine (SVM) algorithm is used as the basic classifier.
Accuracy measure is adopted to evaluate the performance of the classification results.
We compare our universal-topic based approach with a translation based CLTC method ( Translation ).
In  Translation  approach, the target texts are first translated into the language of source text and are then classified by the classifier trained with the source texts.
Terms of Web pages are weighted by the Term Frequency (TF) scheme.
The two bilingual dictionaries (from Chinese to English and from English to Chinese respectively) downloaded from [3].
In this comparison, the ML-LDA model with K=400 is used.
is based on translation Table 3.
Comparison of Text Classification Results Universal-topic based Ch-to-En En-to-Ch

 Translation

 Table 3 shows the comparison results.
We can see that, on both CLTC tasks, the universal-topic based approach outperforms the translation based method.
The experimental results obtained in this section indicate that universal-topics learned from Wikipedia by ML-LDA models can indeed help CLTC tasks.
In cross-lingual document recommendation (CLDR) as: given a document, retrieve the this paper, we define the related documents in a different language.
Two tasks are experimented: 1) Given a Chinese Web page (source), recommend related English Web pages (Chi-to-En).
2) Given an English Web page (source), recommend related Chinese Web pages (En-to-Ch).
In this experiment, we adopt the following precision measure to empirically evaluate the recommendation results.
= nTxx )( qf |)}( xf )( |{|   Qq    Pr ecision =   n |
 | where Q denotes query set, T(n) denotes the set of top n most related pages and f( ) denotes class label.
We measure the relatedness between two pages written in different languages in universal-topic space.
In this experiment, the cosine similarity is utilized.
We also compare our algorithm with the  Translation  approach in our experiment.
In both CLDR tasks, we do recommendation for all source pages.
Table 4.
Comparison of Recommendation Results n





 En-to-Ch Translation Translation Ch-to-En the universal-topic based
 Universal-topic 0.49 0.48 0.47 0.46 0.46
 Universal-topic 0.49 0.46 0.46 0.46 0.45
 Table 4 gives the comparisons between two recommendation approaches on both CLDR tasks.
The recommendation precision will decrease when the value of n grows.
This is reasonable, because, when more pages are recommended, there is higher probability of recommending some irrelevant pages.
We can see that recommendation approach outperforms the translation based method regardless of n.
In this paper, we proposed a novel approach, ML-LDA, to mine multilingual topics from Wikipedia.
Our experiments showed that
 topics (universal-topics).
With universal-topics, documents in different languages can be represented within the same vector space.
Therefore cross-lingual similarity can be measured without machine cross-lingual applications feasible.
