In contrast to plain text, documents in the Web expose some speci c structural properties.
A Web page s text is not limited to the actual  main content  but usually consists of several additional segments.
These provide further information (site maps, headline lists, tables,  related article  links, text-ads etc.)
and are basically meant to augment the full-text.
This additional text provided seems only be partially useful or probably even counterproductive for search and classi cation; the common solution to the problem is simply erasing template content or at least ignoring it.
However, current approaches identify templates only heuristically or by machine learning.
In this paper, the textual Web content of several large corpora were subjected to a quantitative analysis.
By deriving a densitometric text model based upon techniques from the  eld of Quantitative Linguistics, it can be shown that the text corpus exposes two fuzzy classes of text, covering full-text and navigational information respectively.
The proportions of the two classes (in tokens) roughly is 1 : 2, whereas  template text  can be divided in equal shares into these classes (noisy, short navigational text hints vs.
frequently used full-text).
Copyright is held by the author/owner(s).
To understand the distinction between templates and main content, a large-scale statistical classi cation was performed on the level of intra-document segments, under the assumption that the segments are su ciently homogeneous (i.e., either template or main content).
The analysis was conducted on the representative Webspam UK-2007 dataset (on the ham part, 356,437 out of 106 million crawled pages).
As a manual segmentation appears infeasible at corpus-scale, the state-of-the-art BlockFusion segmentation algorithm was employed, which utilizes the text density measure (b), relating the number of tokens in a particular text block to the occupied text  area  determined by word-wrapping the character data at a  xed line width wmax.
It was shown that the resulting block structure closely resembles a manual segmentation [3].
Text density is a particularly useful measure when analyzing the Web s quantitative structure.
It does not depend on the notion of  sentence , which we could hardly de ne for the Web s content   many portions of text simply do not contain sentences, nor anything meaningful that could be separable by full stop (this is especially true for template text).
To reduce the impact of errors caused by a too  ne-grained segmentation, the amount of text (= number of tokens) contained in segments of a particular text density  is examined at corpus-level.
We can model this histograph-ically by rounding: (cid:48)(b) = [(b)].
Figure 1 depicts the retrieved token-level count/density distribution for the whole corpus.
Apparently, two modal scores are visible, at (cid:48) = 2 and (cid:48) = 12 respectively.
This indicates at least two classes of text within the corpus.
The superimposition of di erent classes ( strata ) of text is known in linguistics; from a theoretical perspective it may even be the normal case [1].
To con rm the presence of multiple classes we need to  nd a corresponding distribution function whose compound functions are already known in the theory, e.g.
the Beta distribution, which is the conjugate prior of the binomial distribution.
In fact, an almost perfect  t was achieved by combining two beta distributions with a normal distribution (R2 .
=
 4, 034, b2 = 54, 49; c = 0.015, d = 0.64; e = 78.87, f = 7.834,   = 28.65,  2 = 6.489; x scores (densities) have been normalized by xnorm = 36 to [0 : 1] before  tting): f (x) = c   (d   fbeta(x, a1, b1) + (1   d)   fbeta(x, a2, b2)) + (1   c)    , 2 (e   x + f ) (1) another parameter, resulted in a far less accurate  t (R2 =
 text densities can be divided into two fuzzy classes C1 and C2; the transition from C1 to C2 follows the normal distribution, which means that for blocks with particular densities it is rather undetermined to which class the contained text belongs.
The distribution parameter d reveals that C1 roughly covers one third of the tokens enclosed in the corpus and C2 covers two thirds; from Figure 1 we see that for 5   (cid:48)   10 the normal distribution dominates.
Rank









 Term sitemap bookmark accessibility misc skip shipping polls a liates username thu   -0.33 -0.29 -0.29 -0.29 -0.28 -0.28 -0.28 -0.27 -0.27 -0.27 Term spelled thousands temporarily gave tried aimed seem eventually unfortunately obvious  









 Table 1: The top-10 terms for  1 and  2 The resulting values are in the range of [ 1; +1]; the absolute score is the degree of typicality, the sign indicates the direction of typicality ( 1 means the term clearly belongs to C1, +1 states that the term clearly belongs to class C2).
In our setup, of the 2938 terms with w1 2   100, 589 terms (20%) expose a term typicality      0.05 (i.e., C1) and 1255 terms (42.7%) a term typicality of     +0.05 (i.e., C2).
Table 1 shows the top-10 typical terms for C1 and C2 respectively.
As one can see, C1 terms are very likely to appear in template blocks, whereas C2 terms are more likely for full-text.
To further con rm the observed dichotomy of Web text, two other features of  full text  and  template text  were contrasted with the text density distribution: 1. the presence of full stop characters in the segment and 2. the frequency of the segment.
As full stops indicate complete sentences, the number of tokens contained in segments with full-stop should be much higher for C2 than for C1 (and vice versa).
Indeed, text density has a fairly high information gain for predicting the occurrence of a full stop (0.711), which is substantiated by a classi cation accuracy of 91.4% using a simple linear classi er.
Finally, the token-level distribution of frequent templates [2] was analyzed (38,634 segments oc-curing at least 10 times, representing 28% of all tokens).
A majority of the tokens in segments with (cid:48)(b)   5 are contained frequent templates (63%).
With high chance the other 37% tokens in few-worded segments do also not describe the  main  conteent, and could be considered  template  (23% of all tokens in the corpus).
On the other hand, many boilerplate templates contain full sentences ((cid:48)(b)   9;
 obviously requires global information (such as segment  n-gerprints etc.
), identifying the low-density templates can be performed on-the- y without corpus-level statistics, which makes density-based template detection a clear choice as an upstream  lter within a sophisticated template removal strategy.
