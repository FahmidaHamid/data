Statistical learning based web spam detection has demonstrated its superiority for being easy to adapt to newly developed spam techniques[1][3][5].
The robust machine learning based web spam detection system requires large amounts of labeled training samples.
However, labeled samples are di cult, expensive and time consuming to obtain, as they require the e orts of experienced human annotators.
The scarcity of training data becomes one of the major challenges for web spam detection.
Meanwhile unlabeled data may be relatively easy to collect.
Semi-supervised learning addresses this problem by using large amounts of unlabeled data, together with the labeled data, to build better classi ers.
Self-training is a commonly used technique for semi-supervised learning[4].
However the learning ability of the Self-training is limited.
Based on the fact that the neighboring nodes are often having the same properties[1][3], we proposed link-based bootstrapping Copyright is held by the author/owner(s).
learning algorithms Link-training and LS-training, which make full use of the self-learning of classi ers and the topological dependency of web nodes.
Experiments on WEBSPAM-UK2006 benchmark show that the Link-Training and LS-training algorithm are e ective.
Link-training algorithm is based on the Self-training and link learning, which makes full use of the classi er s self-learning ability and the topological dependency on the Web graph.
Figure 1 is the  ow chart of Link-training algorithm1.
LS-Training Link-Training (6) According to the value of LS, s e le ct p large s t and n s malle s t e xample s with the ir pre dicte d labe ls According to the value of PS, s e le ct p large s t and n s malle s t e xample s with the ir pre dicte d labe ls Training Set(L) (1)Learning Classifier(C) (2)Classification (4)Annotation Unlabeled Set(U) (5)Link Learning WebG rap h (3)Prediction Pre dicte d Spamicity
 (4)Annotation Figure 1: Flow Chart of Link based Learning Algorithm.
In Link-training, a classi er is  rst trained with a small labeled data set.
The trained classi er is then used to classify the unlabeled data and give them a predicted spamicity(P S) value (P S is computed with Formula 1, where Pspam(x) represents the probability of x belonging to spam.).
Then annotate the WebGraph with the P S values of samples in U and L (the P S values of spam and non-spam samples in L are 1 and 0 respectively).
In link learning step, the link spamicity(LS) of unlabeled samples will be computed according to their neighbors.
Based on value of LS, p largest samples and n smallest samples are converted into labeled ones with their predicted labels.
The training procedure repeats until the number of iteration reaches a speci ed value.
P S(x) = Pspam(x) Pspam(x) + Pnormal(x) (1)
 included in Link-Training.
bor features in our previous work[3], we compute the link spamicity(LS) as follows: LS(h) = Pv N(h)(P S(v)   weight(h, v)) Pv N(h) weight(h, v) (2) where v, h are the hosts, weight(h, v) is the weight of host h and v, weight(h, v)   {1, n, log(n)}, where n is the number of hyperlinks between h and v. N (h)   inlink(h) S outlink(h) inlink(h) represents the inlink set of h, and outlink(h) is the outlink set of h.
Algorithm 1 is the detailed description of the Link-training.
Algorithm 1 Link-Training Algorithm Input : L:




 n, p: The number of selected non-spam and Labeled training set Unlabeled examples set Test set Classi er Iterations Host level hyperlink graph spam samples in each iteration 1: i = 0


 Train classi er C with L


 values with Formula 1 Annotate G with the P S values of samples in U and L (the P S value of spam and non-spam samples in L are 1 and 0 respectively) Perform link learning on annotated G; Compute the LS values with Formula 2 According to the values of LS, select p largest and n smallest examples as spam and normal respectively, put the n + p samples to L, and delete them from U (The principle of choosing p and n is to keep the ratio of non-spam and spam unchanged in training set.)
i = i + 1 9: end while

 Output : Web spam detection result on test set T Based on Link-training and Self-training, we further propose LS-training algorithm.
Compared with Link-training, LS-training also injects the samples with maximal and minimal values of P S to L in every iterations, which is descripted with dot line in Figure 1.
LS-training algorithm boosts the labeled set from two di erent perspectives, which is similar to the starting point of Co-training.
WEBSPAM-UK2006 [2] is used in our experiments.
The collection includes 77.9 million pages, corresponding to roughly
 page in the summarized samples.
The features extracted for learning include 72 content features and 81 link features (41 page-level link features and 40 host-level link features), which are the same as the features we used in Web Spam Challenge 2008[5].
The detection algorithm employed in the experiment is adaboost, and the weak classi er for adaboost is stump.
For the UK2006, we randomly select 25% data are kept as test samples.
From the remaining 75% data, we randomly select 100 samples as labeled set, while the rest samples are used as unlabeled set.
The experiment results is obtained with 20 times random partition.
The iterations K is set to
 and normal samples unchanged, n is set to 15, and p is set to 6.
In other words, for LS-training 42 unlabeled samples with the predicted labels are put into the labeled set in each iteration.
AUC and F1-measure are used to measure the performance.
Figure.
2 describes the performance of web spam detection with LS-training, Link-training and Self-training.
From the  gure, we can see that LS-training and Link-training are more e ective than Self-training algorithm.
With the increase in the number of iterations, both F1-measure and AUC have a signi cant improvement, while the performance of Self-training was poor.
The LS-training performance after 50 iterations with 100 labeled samples is F1-measure= 0.8037 and AUC=0.9385.
Compared with the best results achieved on the whole data set[1][3], the results are inspiring.
Baseline Self training Link training LS training e r u s a e m  



















 Baseline Self training Link training LS training

 Iterations






 Iterations Figure 2: Comparison of web spam detection performance with di erent algorithm

 In this paper, we proposed two link-based semi-supervised learning algorithms to detect web spam on small labeled samples set.
Experiments show that the algorithms can boost the performance of the classi er e ectively, which gives a feasible scheme for small samples detection.
Another potential application of the proposed algorithms is to assist the human with annotating the large-scale unlabeled Web nodes preliminarily.
