As one type of portals for fast-growing user generated content (UGC), community question-answering (CQA) sites have attracted a large number of users both seeking and providing answers to a variety of questions on a variety of subjects.
Since they allow anyone to post or answer any questions on any subjects, the quality of answers varies greatly.
Thus, the ability to automatically identify the best answers has significant impact on users  satisfaction.
The major challenge of identifying high-quality answers in CQA sites is the lexical gap between a question and its correct answer.
The gap is caused by at least two factors: 1) textual mismatch between questions and answers.
Words appeared in a question will not necessarily reappear in its best answer; and 2) user generated spam or flippant answers.
Previous work solved this problem by either generating complementary features provided by highly structured CQA sites [1], or finding textual clues using machine-learning techniques [3].
In this paper, we propose an analogical reasoning-based ranking technique which uses previous relevant knowledge to bridge the lexical gap, and discovers non-textual clues between questions and their answers to identify the correct ones.
In particular, we assume that a question has implicit positive links with its right answers, and negative linkages with other answers.
Given a new question, firstly we retrieve a set of positively linked (i.e. correctly answered) question-answer pairs (q-a pairs) whose questions *The work was done during the first author s internship in MSRA.
Copyright is held by the author/owner(s).
are similar to the new question from a knowledge base.
The retrieved q-a pairs construct a supporting set.
Secondly, we predict the analogy of the link between the new question and each of its candidate answers to the links in the supporting set.
The analogy is predicted by a logistic regression model learnt offline.
The answer which has the most analogous linkage is assumed to be the best answer to the new question.
There are two key aspects of the proposed idea: 1) Instead of either separating questions and their answers as independent information sources or mixing them as one object, we treat them as relational data and predict the property of the link.
Secondly, instead of directly predict the link with the link prediction model, we use a supporting set to bridge the lexical gap and facilitate the prediction.
Intuitively, not only the supporting set is more likely to share terms with the candidate answers, but also it provides the knowledge of how similar questions are answered, while the  way of answering  suggests the desired answers.
We adopt the Bayesian Analogical Reasoning (BAR) framework proposed by Silva et al. [3] to predict the latent q-a linkages.
Formally, let   = [ 1  ,   ,  2  ,   ,   ,  K  ,   ] be a  dimensional feature vector of the q-a pair of question   and answer   , where   defines the mapping  :          K .
Let     {0,1} be an indicator of the type of the link between   and   , where   = 1 indicates a positive linkage and   = 0 otherwise.
Let   = [ 1,  2,   ,  ] be the parameter vector of the logistic regression model to be learnt, we have     = 1   ,   =

 (1) where       = {  , 1         , 1         } is a training q-a pair and   ,   are the numbers of training questions and answers respectively.
Generally we have       .
We use both positive q-a pairs (i.e. good answer) and negative q-a pairs (i.e. noisy answers) to train this model, while the effect of negative ones is embedded in a Gaussian prior.
To ensure the preciseness of the prior, firstly we fit a BAR model using Maximum Likelihood Estimation (MLE) on the training data, and obtain an initial   .
Then we compute the covariance matrix   as a smoothed version of the MLE estimated covariance:  1
 =  

 (2) where   is a predefined number,   is the size of the training data-set,   is the       feature matrix of the training q-a pairs, either positive or negative.
  is a diagonal matrix with     =       (1       ), and     is the predicted probability of a positive link for the  th row of  .
The prior of   is then the Gaussian  (  ,   ).
positive population to balance the number of positive and negative training data.
In the testing stage, given a new q-a thread, we retrieve some relevant positive q-a pairs from 29.8 million q-a threads crawled from the Yahoo!Answers site.
We adopt traditional information retrieval techniques to find the supporting q-a set.
In particular, let   be a query question and its   , 1        } , we retrieve those positive q-a answer list be {  pairs   = { 1:  1,  2:  2,   ,  :  } with high cosine similarity above a threshold:   = { :  | cos   ,   >  ,     1,   ,   } (3) where   is the size of the crawled Yahoo!Answers database and   is a threshold.
Each question is represented in the bag-of-word model.
The effect of   is shown in Figure 2.
@



 BSets



 Figure 1.
Average precision of our method and the baselines.
Table 1.
MRR for our method and the baselines Method
 Method

 Cosine

 BSets [2] Our method



 Given  , we score the new q-a pairs   :   with Eq.
(4) by measuring a marginal:

     ,     =       = 1     ,  ,   = 1       = 1     (4)   is the  -th answer of the query question.
    where   represents   ).
  is the vector of link indicators for  , the features of (  ,   and   = 1 indicates that all the q-a pairs in   have positive links, i.e. { 1 = 1,  2 = 1,   ,   = 1}.
  = 1     ,  ,   = 1 , meas-The first term in Eq.
(4), i.e.     ures the probability of a positive query linkage when the support  , on the ing set is observed.
The second term,     = 1   other hand, evaluates this probability in the case that only the query q-a pair is observed.
The idea underlying Eq.
(4) is actually   ) would  fit into   , or to what to measure to what extent (  ,     ).
The more analogous it is to the sup-extent   explains (  ,   porting linked q-a pairs, the higher score a q-a pair obtains.
We use the learnt logistic regression model to predict the probability of a positive linkage in particular, which gives Eq.
(5) and Eq.
(6):       = 1     ,  ,   = 1 =     = 1     ,    ( |  ,   = 1)        = 1     =     = 1     ,    ( )  (5) (6)

 We crawled 29.8 million questions from the Yahoo!Answers website, all of which have user-labeled  best answers .
100,000 randomly selected q-a pairs were used to train the link prediction model, and 16,000 q-a threads were used for testing; each contains 12.35 answers on average, which results in about 200,000 testing q-a pairs.
We compared our approach with three baseline methods: Nearest Neighbor (NN), Cosine distance and Bsets [2].
The first two directly measure the similarity between a question and an answer, Figure 2.
The Effect of similarity threshold   and Prior scalar =  .
  gives best performance.
.
  =  .
 ,    

 while the Bsets method leverages a supporting set to measure the similarity, however it does not treat questions and answers as relational data.
Figure 1 illustrates the mean average precision of the four methods on top   = 1, 5, 10 results, and Table 1 gives the Mean Reciprocal Rank (MRR) performance.
Both show that our method significantly outperformed the baseline methods.
Figure 2 illustrates the effect of parameters and  , the threshold in Eq.
(3), on the MRR performance.
, the scalar in Eq.
(2),  

 This paper has presented a novel best answer identification method for CQA sites.
It not only uses a supporting set to bridge the lexical gap between questions and answers, but also treats them as relational data and learns an analogical reasoning-based model to rank the answers of a new question.
The best answer is identified as the top-ranked one which has the strongest analogy to the positive links in the supporting set.
Experiments based on 29.8 million real q-a threads showed the effectiveness of the proposed method.
