Nowadays, the User-Interactive Question Answering (QA) community has become a popular medium for online information seeking and knowledge sharing.
For example, Yahoo!
Answers1, one of the largest QA communities nowadays, has approximately 23 million resolved questions, which are posted and answered by users.
In addition, there are also thousands of questions posted daily.
However, with the exponential growth in data volume, it is becoming more and more time-consuming for users to  nd the questions that are of interest to them.
As a result, the asker would have to wait for a long time before getting answers to his/her question.
 Supported by National Key Technology R&D Program of China (NO.2008BAH26B02) 1http://answers.yahoo.com Copyright is held by the author/owner(s).
To help users  nd interesting questions and expedite the answering of new questions, some question recommendation attempts are seen in QA communities like Yahoo!
Answers, such as maintaining in user home pages a question list automatically generated based on features like posted time and ratings.
However, these systems are not typical recommender systems in essence in that they have not taken users  interest into account.
In our work, We employ PLSA [3] to analyze a user s interest by investigating his previously asked questions and accordingly generate  ne-grained question recommendation.
Meanwhile, because traditional evaluation metrics cannot meet the special requirements of QA communities, we also propose a novel metric to evaluate the recommendation performance.
Experimental results show the PLSA model works e ectively for recommending questions.
Aiming to improve a QA community s e ciency, question recommendation is to recommend questions to users who are interested in, and capable of answering them.
Therefore, the key to a question recommender is to capture users  interest.
In our work, we propose to analyze users  interest by investigating his previously asked questions.
In a typical question answering cycle, users always answer questions by  rst identifying the topics in an implicit way.
PLSA model [2], known for its ability of capturing underlying topics, suits our problem well.
The latent variables in PLSA denote the topics of corresponding questions.
Therefore, given a question collection, the distribution of users and their answered questions can be formulated as follows: Pr(u, q) =Xz Pr(u|z) Pr(q|z) Pr(z) (1) where u   u1, u2, ..., un are users, q   q1, q2, ..., qm are questions and z   z1, z2, ..., zk are k topic models, each capturing one topic u.
However, in a real QA community, each user can only answer a small percentage of the overall questions, which means that most observations (u, q) should be zero.
In order to deal with sparsity, we use a user-word aspect model instead, where the co-occurrence data represent the event that users type words in a particular question: Pr(u, w) =Xz Pr(u|z) Pr(w|z) Pr(z) (2) Note that the PLSA model allows multiple topics per user, re ecting the fact that each user has lots of interest.
Then the log likelihood L of the question collection is c(u, w) log Pr(u, w) (3) L =Xu,w where c(u, w) is the sum of word w s count in all questions the user u answers.
Model parameters can be learned using Expectation Maximization (EM) to  nd a local maximum of the log likelihood of the question collection: Pr(z|u, w) = Pz  Pr(u|z)   Xw Pr(w|z)   Xu Pr(z)   Xu,w Pr(u|z) Pr(w|z) Pr(z) Pr(u|z ) Pr(w|z ) Pr(z ) c(u, w) Pr(z|u, w) c(u, w) Pr(z|u, w) c(u, w) Pr(z|u, w) (4) (5) (6) (7) We then model recommending questions to users as the posterior probability Pr(u|q), that is, according to how likely it is that user u will access the corresponding question q. According to Bayesian law, we can compute Pr(u|q)   Pr(u, q), which is calculated as the product of the probabilities of the words q contains, normalized by the question length: Pr(u, q) = Yi Pr(u, wi)!1/|q| (8) where wi are words in the question q , and |q| is the length of q. Consequently, a ranking list of users will be maintained for the question q according to the score.
The recommendation can be conducted by recommending q to top-n users.
To obtain the data sets for experiments, we crawl questions of three categories of the Yahoo!
Answers: Astronomy, Global Warming, and Philosophy, and  lter out all questions which have only one answer.
Questions in each data sets are already labeled with the best answers.
The data set statistics are listed in Table 1.
For each category, a PLSA model is trained from 85% of the question sets (questions and their corresponding answers), and the left are used for testing.
We empirically choose the number of latent variables k = 100.
In traditional recommender systems, we can use the precision to evaluate the performance.
However, the precision metric fails to suit the QA context.
Users in a QA community can only access a small portion of questions of all.
While questions one accessed are those he/she is interested in, there is no guarantee that questions he/she has not accessed are those he/she does not like.
Here we propose a new metric for the evaluation of question recommendation.
For each question in testing data, we only recommend it to the users who actually answered it instead of all possible users in the whole data sets.
Then the accuracy for this question is de ned according to the rank of the user who provides the best answerer.
Since the choice of the best answer subjects to asker  s personal viewpoint, one may question whether the best answer is objectively the best Table 1: Yahoo!
Answers data set.
Category Questions Answers Users Astronomy Global Warming Philosophy








 Table 2: Comparison of recommending methods.
Category Cosine PLSA Astronomy Global Warming Philosophy





 of all, or just the asker  s prejudice.
Adamic et al. [1] check questions from di erent categories in Yahoo!
Answers, and draw the conclusion that answers selected as best answers are mostly indeed best answers for the questions.
Therefore, in this paper we use the best answerer s rank as the ground truth of our evaluation metric: accuracy =

 (9) where |R| is the length of recommending list, which is equally the number of answers in this question set, and RB is the rank of the best answerer.
As there is no previous work done on recommending questions to users according to their interest in QA communities, for comparison we implement Cosine Similarity between user and question vectors, with tf.idf weights: s(u, q) = tf.idf (q, w)2 (10) tf.idf (u, w)tf.idf (q, w) Pw tf.idf (u, w)2rPw rPw where tf.idf (q, w) is the word w s tf.idf weight in q, and tf.idf (u, w) is the sum of w s tf.idf weights in questions that u posts/answers.
Table 2 shows the experimental results.
We observe that our PLSA model outperforms the cosine similarity measure in all the three data sets.
It shows PLSA can capture users  interest and recommend questions e ectively.
In this paper, we introduce the novel problem of question recommendation in Question Answering communities.
We adopt the PLSA model to tackle this novel problem.
We also propose a novel evaluation metric to measure the performance.
The results show PLSA model can improve the quality of recommending.
In conclusion, our study opens a promising direction to question recommendation.
