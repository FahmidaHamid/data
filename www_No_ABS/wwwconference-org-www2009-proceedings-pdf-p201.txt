Automatic text categorization (TC) is a process of assigning some class labels to text documents.
In recent years, TC has been widely used in many web applications, for instance, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
query classi cation in search engines [2], deep classi cation of web documents [5, 38], and Blog classi cation [22].
For these web applications, it is often required that TC can be performed with a short training time and testing time, sometimes with incrementally updated data.
A number of TC techniques have been explored in the literature, for instance, Naive Bayes [6, 10], kNN [8], Neural Network [4], centroid-based approaches [2, 3, 16, 34], Decision Tree (DT) [17, 35, 7], Rocchio [26], and SVM [12,
 most popular supervised approaches, due to the computational e ciency.
However, previous work [31] has found that the performance of centroid-based classi ers is signi cantly lower than other approaches (e.g., SVM).
One of the reasons for the inferior performance of centroid-based classi ers is that centroids do not have good initial values.
To solve this problem, many methods have been using feedback-loops to iteratively adjust prototype vectors, such as Dragpushing method [34], Hypothesis Margin method [34], and Weight Adjustment method [29].
These improved clas-si ers perform competitively compared to SVM classi ers.
Motivated by previous weight-adjustment e orts for centroid-based classi ers, we design a Class-Feature-Centroid (CFC) classi er, which strives to construct centroids with better initial values than traditional centroids.
In our CFC classi- er, we  rst extract inter-class and inner-class term indices from the corpus.
Then both indices are carefully combined together to produce prototype vectors.
Our experimental results on the skewed Reuters-21578 corpus and the balanced
 has a consistently better performance than SVM classi ers.
In particular, CFC is more e ective and robust than SVM when data is sparse.
In summary, this paper has made following contributions:   We propose a novel weight representation for centroid-based classi ers, which incorporates both inter-class term distribution and inner-class term distribution to determine term weights in prototype vectors.
  In the testing phase, we adopt a denormalized cosine measure for similarity calculation.
Our experiments demonstrate that this approach is more e ective for CFC than normalizing prototype vectors, because the discriminative capability of features is preserved.
The remainder of the paper is organized as follows.
Section 2 reviews centroid-based text categorization methods.
Section 3 elaborates the design of CFC.
We evaluated the performance of our CFC in Section 4.
Section 5 summarizes future research directions.
In centroid-based TC, a text in a corpus is represented with a Vector Space Model (VSM), where each text is considered as a vector in term space [1].
A prototype vector (i.e., a centroid) is constructed for each category as a delegate vector for all documents belonging to that class.
When classifying an unlabeled document, the vector representing the document is compared with all prototype vectors, and the document is assigned to the class whose prototype vector is most similar [15].
In the vector space model, all documents in a class form a lexicon set F = {t1, t2, ..., t|F|}, and a document is represented as a vector.
Normalization by document length is often performed when calculating a term s weight and scaling the vector to have L2-norm equal one is the most commonly used method.
A traditional prototype vector is a delegate vector for each category, where a feature s weight should be a form of weight combination of all documents in the category.
To avoid over tting and high-computational complexity, many dimension reduction methods have been proposed for the term vector, such as stop words, stemming [37], word clustering [19], and document frequency [39].
Given a class Cj of a corpus, there are two classical methods to create Cj s prototype vector: (1) Arithmetical Average Centroid (AAC):   Centroidj =
   d ,
   d  Cj (1) where the centroid is the arithmetical average of all document vectors of class Cj.
This is the most commonly used initialization method for centroid-based classi ers.
After centroids of di erent categories are determined, an unlabeled document is classi ed by  nding the closest cen-troid to the document vector.
The category of this centroid is then assigned to the test document.
When the distance of two vectors is measured as the their dot product, the testing process is to calculate (cid:4)
 = arg max j ( (cid:2)d     Centroidj).
That is, the test document d will be labeled as class C .
Besides the above method, other distances, such as Pearson Correlation Coe cients [24], Euclidean-based similarity computed by an exponential transformation, and Euclidean-based similarity computed by division [3], have also been employed in the literature.
(cid:4)
 The performance of centroid-based classi ers depends strongly on the quality of prototype vectors.
To improve performance, many studies have attempted using feedbacks to adjust term weight in prototype vectors, such as Dragpush-ing [32], Hypothesis Margin [34], and CentroidW [29].
Combining Homogeneous [14] combined several centroid-based classi ers with di erent term distribution to improve the accuracy of classi cation.
The performance of these adaptive methods is generally better than the traditional centroid-based methods.
In particular, some of them can be comparable to SVM classi ers on micro-F1 and macro-F1 evaluations [32, 34].
The motivation of this work is from the observation that all of the above adaptive methods start with the same initial term weights obtained at the centroid construction phase.
During the training phase, term weights are adjusted to get better prototype vectors.
Di erent from these previous approaches, we try to obtain good centroids during the construction phase such that their classi cation capability is still competitive compared to those derived from adaptive methods.
The following sections discuss details of our centroid-based classi er.
(2) Cumuli Geometric Centroid (CGC):   Centroidj =   d ,
   d  Cj (2)

 where each term will be given a summation weight [15].
Compared to other TC methods, centroid-based approaches are more serious with the problem of inductive bias or model mis t [18, 36]   classi ers are tuned to the contingent characteristics of the training data rather than the constitutive characteristics of the categories.
Centroid-based approaches are more susceptible to model mis t because of its assumption that a document should be assigned to a particular class when the similarity of this document and the class is the largest.
In practice, this assumption often doesn t hold (i.e., model mis t).
In fact, many researchers have found that a centroid-based classi er should give more weight to term distributions among the corpus, i.e., inter-class, inner-class and in-collection distributions.
Centroids considering characteristics of term distribution have shown improved results [14].
Di erent from previous approaches for constructing cen-troids, CFC puts more emphasis on term distributions in order to improve the quality of centroid.
Speci cally, CFC  rst extracts inter-class and inner-class term distributions from the corpus, and then uses these distributions to derive the centroid.
In the rest of this section, we  rst give an overall description of CFC design, and then discuss our feature extraction method, followed by a discussion of our classi cation method.
Similar to other centroid-based approaches, CFC adopts the vector space model: all documents from a corpus form a lexicon set F = {t1, t2, ..., t|F|}; and the centroid for class Cj is represented by a term vector Centroidj = (w1j , w2j , ..., w|F|j), where wkj (1   k   |F|) represents the weight for term tk.
The main di erence between CFC and other approach is how term weight is derived.
In CFC, weight for term tk of wij = b j ti
 |Cj |   log(
 CFti ), (3) j ti is term ti s document frequency in class Cj , where DF |Cj| is the number of documents in class Cj , |C| is the total number of document classes, CFti is the number of classes containing term ti, and b is a constant larger than one.
j ti |Cj | In the above formula, the  rst component b is the
 ) inner-class term index, and the second component log( CFti represents the inter-class term index.
Both indices are discussed below.
The inner-class distribution of a term can help classi cation.
For instance, if a term appears many times in documents of category C, and then a test document containing the term is more likely to be of category C.
After comparing di erent forms for inner-class distribu-
j ti |Cj | tion, we  nally choose b , (b > 1) to be CFC s the inner-class term index.
The advantage of this form is to limit the inner-class feature weight within range (1, b].
The denominator |Cj| in the formula smoothes the di erence of document frequencies across categories.
The proposed inner-class term index can be easily computed by counting the occurrence of terms while traversing the corpus, which only incurs linear-time cost.
Previous work [21] has studied many mathematical forms for scoring a term s distribution information.
Intuitively, a good inter-class feature or term should distribute rather di erently among classes.
In other words, if a term only appears in a few categories, and then the term is a discrim-inative feature, thus a good feature for classi cation.
Conversely, if a term appears in every category, and then the term is not a good inter-class feature.
CFti Our method for extracting inter-class term index, i.e., log( ), produces more discriminative features.
Note that such a form is similar to IDF, but the di erence is that CFC is counting the number of categories containing the term.
When a term occurs in every category, the value becomes
 one category, the value becomes log(|C|).
In other cases, the value falls between them.
We can observe that such a method favors rare terms and bias against popular terms.
The inter-class term index can be e ciently computed in linear time.
By counting lexicons while traversing the corpus once, all inter-class term indices can be easily computed.
After the prototype vector of each class is obtained, CFC classi es a document with a denormalized cosine measure, i.e., (cid:4)
 = arg max ( j   di     Centroidj), (4)     di is the document vector for document i.
The term di uses the standard normalized where weight for each feature in cos     ||  TF-IDF score.
As Figure 1 showing, cos   is the standard similarity measure between two vectors, while we adopt Centroidj||2 as the similarity measure between a document vector and a prototype vector.
Because the prototype vector is not normalized, this similarity is called de-normalized cosine measure in this paper.
Centroid j di
 o   c o s   ||   c o s r o i d ||
 j C e n t Figure 1: An illustration of denormalized cosine measure.
This denormalized cosine measure preserves the discrimi-native capability of prototype vectors and improves the accuracy of classi cation (see Section 4.3.1).
We use an example to illustrate how classi cation is performed in CFC and to demonstrate the di erence between CFC and AAC.
Assume a corpus contains four categories C1, C2, C3, and C4.
Three feature terms appear in 14 documents, and each document only contains one term.
Table 1 gives the term distribution of this corpus.
Table 1: Term distribution for an example corpus.
Feature term1 term2 term3















 Then, Table 2 shows prototype vectors for each category.
For CFC, b is set e   1, and log uses function ln.
Table 2: Prototype vectors for the example corpus.
{0.2721,0.1361,0.9526} Classi er



 {0,1,0} {0,0,0} {0,1,0} {0,0,0} {0,1,0} {0,0,0} {1.5445,0,2.0250}

 Now, assume {0.6, 0.8, 0} is the document vector for a document (cid:2)d. We then calculate the similarity between (cid:2)d and all four categories and obtain (0.8000, 0.8000, 0.8000, 0.2721) for AAC, and (0, 0, 0, 0.9267) for CFC.
AAC would assign a class label C1, C2, and C3 to (cid:2)d, since the similarity for these three classes is 0.8000, the highest value.
For CFC, label C4 is assigned to (cid:2)d. For this pedagogic example, we can observe that:   AAC should favor popular words in a corpus, which 0.2721) for (cid:2)d.   CFC is more favorable to rare terms and biases against popular terms.
In the example, CFC gives (cid:2)d a score of (0, 0, 0, 0.9267), because term1 only appears in category C4 while term2 occurs in all of the categories.
This distinct trait of CFC contributes to the quite di er-ent classi cation performance between AAC and CFC.
Enlarging cos   by a factor of prototype vector s magnitude may seem to be unfair to some categories.
A prototype vector with large norms could increase false positives, because documents from other categories may receive higher similarity scores.
On the other hand, prototype vectors with small norms could have more false negatives.
In fact, such a phenomenon can happen for AAC or CGC (see Section 4.3.1).
l e u a
 y t i r a l i m
 i f o m r o




 Norm of Prototype Vector Max Similarity Value of Train Vectors Max Similarity Value of Test Vectors

 Category Number




 Figure 2: Norms of prototype vectors, similarity measure between documents (training and testing) and prototype vectors of CFC for Reuters-21578 corpus.
CFC, however, doesn t su er from the above problem of incorrect classi cations.
We have found CFC s similarity measures do not change signi cantly across di erent categories.
Figure 2 illustrates the comparison of the norms of prototype vectors, and similarity measure between documents (divided into training set and testing set) and prototype vectors for Reuters-21578 corpus.
Even though the variations for the norms of prototype vectors can be quite signi cant (the average is 40.3 and standard deviation is 2363.0) the maximums of similarity measure for training documents and testing documents do not change signi -cantly.
The average and standard deviation are 8.96 and
 is, similarity measures are roughly on the same scale for all categories.
In this section, we evaluate our CFC classi er on the TC task by comparing CFC s performance with previous centroid-based approaches and the state-of-the-art SVM methods.
Speci cally, we compare the performance of four different approaches:   AAC: centroid-based approach with arithmetical average centroids;   CGC: centroid-based approach with cumuli geometric centroids;   CFC: our class-feature centroids;   SVM: SVM-based tools.
We adopted SVMLight1, SVM-Torch2, and LibSVM3 in this paper.
We will use both skewed and balanced corpus for performance evaluation.
For the skewed corpus experiments, we use the Reuters-21578 dataset.
For balanced corpus, 20-newsgroup dataset is used.
In this paper, we focus on classifying multi-class, single-label TC task and remove multi-label texts from the Reuters corpus.
The performance metrics used in the experiments are F1, micro-averaging F1, and macro-averaging F1.
F1 is a combined form for precision (p) and recall (r), which is de ned as
 2rp r + p .
We used F1 to evaluate the classi cation performance for individual category.
The macro-averaging F1 (macro-F1) and micro-averaging F1 (micro-F1) were used to measure the average performance for the whole corpus.
Macro-F1 gives the same weight to all categories, thus is mainly in uenced by the performance of rare categories for the Reuters-21578 corpus, due to skewed category distribution.
On the contrary, micro-F1 will be dominated by the performance of common categories for the Reuters-21578 corpus.
Because the 20-newsgroup corpus is a much balanced corpus, its macro-F1 and micro-F1 are quite similar.
In the rest of this section, we use  -F1  and  M-F1  to represent micro-F1 and macro-F1, respectively.
Reuters-21578.
The Reuters-21578 dataset4 are based on the Trinity College Dublin version.
Trinity College Dublin changed the original SGML text documents into XML format text documents.
After removing all unlabeled documents and documents with more than one class labels, we then retained only the categories that had at least one document in both the training and testing sets and got a collection of 52 categories.
Altogether, there are 6,495 training texts and 2,557 testing texts left in this 52-category corpus.
The distribution of documents over the 52 categories is highly unbalanced.
After removing 338 stop words (also provided by Trinity College dataset), and unigram terms that occur less than three times, we get 11,430 unique unigram terms.
Words in titles are given ten times weight comparing to those in abstracts.
1http://svmlight.joachims.org
 3http://www.csie.ntu.edu.tw/ cjlin/libsvmtools 4http://ronaldo.cs.tcd.ie/esslli07/sw/step01.tgz (about one thousand text documents per category), and approximately 4% of the articles are cross-posted.
The stop words list [20] has 823 words, and we kept words that occurred at least once and texts that had at least one term.
Altogether, there are 19,899 texts (13,272 training and
 we only keep  Subject ,  Keywords , and  Content .
Other information, such as  Path ,  From ,  Message-ID ,  Sender ,  Organization ,  References ,  Date ,  Lines , and email addresses, are  ltered out.
The total number of unigram terms is 29,557 unigrams.
Words in  Subject  and  Keywords  are given ten times weight comparing to those in  Contents .
For both corpora, we used the tokenizer tool provided in the Trinity College sample.
IDF scores for TF-IDF are extracted from the whole corpus.
Stemming and word clustering were not applied.
Parameter Settings.
For all SVM tools (SVMLight, SVMTorch, and LibSVM), the linear kernel and the default settings were used.
All of SVM-based classi ers can cope with a sparse multi-class TC task directly with one-vs-others decomposition and default parameter values.
For the Reuters corpus,  2-test method [39] was adopted to perform feature selection (top 9,000) for SVMLight (this setting yields the best result in our experiments), while SVMTorch classi er used all 11,430 features.
For 20-newsgroup, we tuned parameter b for SVMLight for better performance.
In the experiments, the parameter b of CFC is set to e   1.7, unless speci ed otherwise.
We  rst compare the overall performance of di erent approaches.
The results for both the Reuters and the 20-newsgroup are shown in Table 3.
Table 3 shows that CFC performs the best among all clas-si ers for the Reuters Corpus.
Both micro-F1 and macro-F1 values are above 0.99, which is signi cantly better than SVM-based classi ers.
Two classical centroid-based classi- ers, AAC and CGC, perform the worst using normalized cosines and happen to have the same results.
For the 20-newsgroup corpus, Table 3 shows that CFC performs the best among all classi ers.
CFC s micro-F1 and macro-F1 are 0.9272 and 0.9275, respectively, which are sig-ni cantly better than all SVM classi ers.
Again, centroid-based AAC and CGC perform the worst.
Table 3: Overall performance comparison of di er-ent classi ers.
Classi er Reuters
 SVMLight SVMTorch* LibSVM

  -F1












 20-newsgroup  -F1












 *Previous work [33] has reported micro-F1 and macro-F1 for 20-newsgroup corpus could reach 0.8891 and 0.8876, respectively.
5http://kdd.ics.uci.edu/databases/20newsgroups s e i r o g e t a
 f o r e b m u

















 F1 Scores
 SVMTorch SVMLight




 Figure 3: F1 comparison of AAC, SVMTorch, SVM-Light, and CFC for the Reuters corpus.
SVMLight
 LibSVM SVMTorch









 s e i r o g e a
 t f o r e b m u





 F1 Scores

 Figure 4: F1 comparison of AAC, SVMTorch, SVM-Light, LibSVM, and CFC for the 20-newsgroup corpus.
To give a more visualized comparison, Figure 3 and Figure 4 give the distributions of F1 scores for the Reuters corpus and the 20-newsgroup corpus, respectively.
From both  gures, we can observe that CFC has much more categories within the range of [0.9, 1.0].
In fact, CFC consistently performs better than other approaches for most categories.
In Figure 3, both SVMTorch and SVMLight have a few categories with zero F1 score.
This is mainly for categories with sparse data, which is discussed below.
In this study, we selected the bottom 10 classes from Reuters corpus, where the number of training documents varies from one to  ve.
Previous work [7] has found that most of conventional learning methods, such as SVM and kNN, have poor performance for sparse training cases, resulting in low macro-F1 scores, because macro-F1 treats every category equally.
Table 4 illustrates F1 values for di erent classi ers.
Our all test documents are correctly classi ed.
This is mainly because CFC can e ectively exploit the discriminative capability of prototype vectors.
This experiment can help to explain CFC s excellent macro-F1 values in the overall performance comparison.
For SVMLight or SVMTorch, more than half of classes have F1 values less than one, and both obtained zero for category  jet  and  dlr .
The result could be caused by the lack of training data.
For AAC, all F1 values are well below one because AAC s prototype vectors don t have the same discriminative capability as CFC.
Table 4: Comparison of F1 metric for bottom 10 classes of Reuters corpus.
Class Train Test platinum jet potato tea cpu dlr nickel fuel lead instal-debt































 Light Torch






























 Reuters.
To give an elaborated exhibition for categories with su cient training data, F1 values of the top ten categories from the Reuters corpus were listed in Table 5.
For all these ten categories, we observe that CFC consistently has the best F1 scores.
In many categories, CFC is signi cantly better than SVM classi ers.
Table 5: Comparison of F1 metric for top ten classes of the Reuters corpus.
Class earn acq crude trade mny-fx interest mny-sp ship sugar co ee Train Test









































 Light









 Torch










 formance in a balanced and su cient dataset   20-newsgroup corpus.
Similar to results from the Reuters corpus, we observe that CFC also consistently outperforms SVMLight, SVMTorch, and LibSVM classi ers.
For most categories, the di erence is signi cant.
In summary, when the training data is su cient, our CFC classi er can consistently perform better than SVM classi- ers for both corpus tested.
We have attempted to improve SVMLight s performance for unbalanced training set by tuning parameter b, and such attempts can slightly improve F1 scores.
For 20-newsgroup, Table 6: Comparison of F1 for all classes of 20-newsgroup corpus.
# SVMLight




























































 LibSVM SVMTorch




























































 are: Categories 3:comp.os.ms-windows.misc, 5:comp.sys.mac.hardware, 8:rec.autos, 11:rec.sport.hockey, 15:sci.space, 18:talk.politics.mideast, 20:talk.religion.misc.
9:rec.motorcycles, 12:sci.crypt, 16:soc.religion.christian, 1:alt.atheism, 6:comp.windows.x, 2:comp.graphics, 4:comp.sys.ibm.pc.hardware, 7:misc.forsale, 10:rec.sport.baseball, 14:sci.med, 17:talk.politics.guns, and 13:sci.electronics, 19:talk.politics.misc, when we tune parameter b (default value is 1.0), micro-F1 increases from 0.8124 to 0.8304, and macro-F1 rises from
 work well for the Reuters corpus.
SVM-based classi ers are proved powerful when they are working in a suitable environment.
In our experiments, there are several factors that may contribute to the inferior performance of SVM.
First, the training samples for small categories are seriously skewed when one-vs-others policy is adopted.
Second, no feature extraction methods, such as latent semantic indexing (LSI) [11, 7] or linear discriminant analysis (LDA) [11], are performed.
Third, we use a quite simple tokenizer with no stemming or word clustering.
Features are kept if they appear more times than a threshold, i.e., based on term frequency.
All of above factors could limit SVM to  nd the perfect hyperplane, thus lowering classi cation performance.
On the other hand, the better performance of CFC could hint that CFC has a discriminative ability on those  raw  terms (without feature extraction and selection) for text classi cation.
Previous work [25] has found that document frequency is a powerful discriminative tool.
In CFC, such discriminative capability is preserved.
tor, i.e., cos     ||  Our CFC classi er adopts a denormalized prototype vec-Centroidj||2.
To study the e ects of this design, we compare the performance of denormalized and normalized prototype vectors for CFC and other centroid-based classi ers.
The results are illustrated in Table 7.
F1 have signi cant changes when switching from a normalized form to the denormalized one.
In fact, when prototype vectors are normalized, CFC s performance is even much lower than AAC and CGC.
This is because normalization smoothes the prototype vectors, thus damages their discrim-inative capability of enlarging selected term features in a text vector.
Table 7: Comparison of normalized and denormal-ized prototype vectors for the Reuters corpus.
Classi er normalized CFC 0.6058

 denormalized AAC 0.5604
 denormalized CGC 0.8510  -F1 M-F1





 AAC and CGC.
For AAC and CGC, Table 7 shows that denormalization can cause signi cant performance degradations.
For AAC, micro-F1 drops from 0.8647 to 0.5604 and macro-F1 fall from

 illustration.
Table 8: Comparison of denormalized CGC and CGC for the top  ve categories of the Reuters corpus.
Class earn acq crude trade mny-fx Train Test









 Pre




 DPre
















 Note:  Pre  means precision,  DPre  means precision for denor-malized CGC, and  DF1  means F1 evaluation for denormalized
 Table 8 compares the original CGC approach and denor-malized CGC for the top  ve categories.
We can observe that after denormalization, the precision for top four categories signi cantly declines.
The reason is that denormaliza-tion results in larger norms for prototype vectors, as top categories have more documents.
When calculating similarity, a big norm increases false positives for those top categories, thus lower precision.
The top  ve categories represent nearly 80% of the whole corpus, so a larger number of false posi- tives in these categories also correspond to the performance deterioration of small categories.
On the other hand, Table 9 compares original CGC and CGC using denormalized prototype vectors for the bottom ten categories.
The most prominent change after denormal-ization is that recall for many categories have dropped to zero.
The reason is that the norms of these rare categories are small, causing lower values of similarity measure for documents within these categories and increasing the number of false negatives.
Though the F1 value of the top  ve categories are not signi cantly declining, those rare categories have signi cant drop on F1 evaluation, which results in signi cant degradation of macro-F1.
Table 9: Comparison of denormalized CGC and CGC for bottom ten categories of the Reuters corpus.
Class Train Test Recall DRecall



































 platinum jet potato tea cpu dlr nickel fuel lead





















 Note:  DRecall  means recall evaluation for denormalized CGC.
instal-debt



 In summary, a denormalized prototype vector can help CFC to retain its discriminative capability.
On the other hand, the same denormalization actually hurts traditional centroid-based classi ers, because of a quite di erent method for constructing centroid.
Norm of Prototype Vector Max Similarity Value of Train Vectors Max Similarity Value of Test Vectors


 l e u a
 y t i r a l i m
 i f o m r o










 Category Number Figure 5: Norms of prototype vectors, similarity measure between documents (training and testing) and prototype vectors of CFC for 20-newsgroup corpus.
Figure 5 illustrates the norms of prototype vectors for the 20-newsgroup corpus, together with similarity measure between documents and prototype vectors.
Similar to the Reuters corpus, we can observe that the norms of prototype vectors have great variations: the average is 113.8 and standard deviation is 420.3.
However, similarity measures across categories do not have much variation.
The average and standard deviation are 18.1 and 35.7 for training set,
 features of prototype vectors are retained and their weights are enlarged, which contributes to the e ectiveness of CFC.
It s worth noting that variations of similarity scores in the
 pus, even though 20-newsgroup is a fairly balanced corpus.
The reason is that the length of texts has great variations  alt.atheism  varies from 1 KB to 51 KB, and this variation of text length is quite common among all categories.
As a result, the number of selected features in text vectors has much greater variations: 1 to 678 for 20-newsgroup and 1 to
 in 20-newsgroup and Reuters are 29,577 and 11,430, respectively, texts in 20-newsgroup corpus may select up to 2.29% features, while texts in Reuters may only use 0.38% features.
As a result, the similarity measure of 20-newsgroup is more varied.
The  rst construction, ln( We carried out a series of experiments to study the feature weight for CFC.
Table 10 illustrates the results of some representative combinations of inner-class and inter-class indices using the Reuters corpus.
CFt ), yields above 0.89 values for both micro-F1 and macro-F1 metrics, indicating that the inter-class term distribution works well for classi cation.
The second and third formulas show that a simple combination of inner-class and inter-class distributions may actually hurt the performance.
The fourth formula only uses inner-class index and yields values above 0.99, indicating the inner-class index is highly discriminative in the classi cation.
Combined with inter-class term index, the  fth formula shows slight improvement.
The sixth formula shows that changing parameter b can further improve the results.
We will discuss parameter b in the next subsection.
Table 10: Performance study on di erent forms of feature weight.
No.
ln(1 + DF DF j t |Cj|) j Formula
 ln( CFt )   ln(
 CFt ) t )   ln( DF j t |Cj| ) |Cj| )   ln( |Cj|   ln( exp( DF j t
 j t
 CFt )
 CFt )
 CFt ) exp( (e   1)  -F1 M-F1











 Note: |C| is the total number of categories in a corpus.
|Cj| is the number of documents within jth category.
DF j t is term t s document frequency within category Cj.
CFt is number of categories containing term t.
In this section, we  rst arrange the inner-class term index solely in the prototype vectors to exhibit the e ect of the inner-class term index for the Reuters corpus.
Then, we study the e ects of CFC s parameter b.
In this experiment, we only use inner-class index as feature j t
 |Cj| weight (i.e., b ) in a prototype vector.
Figure 6 shows that when b is between one and e, CFC s performance is generally good.
When b is e  1.8(  0.918), the performance is signi cantly lower than e   1.7 (  1.018).
s e r o c s


 e 1.8 Micro F1 Macro F1 e 1.7 e 1.6 b e 1.5 e 1.2 e 1.0 e Figure 6: Sensitivity study parameter b of formula DFt |Cj | b for the Reuters corpus.
s e r o c s


 e 1.8 Micro F1 Macro F1 e 1.7 e 1.6 b e 1.5 e 1.2 e 1.0 e Figure 7: Sensitivity study of CFC s parameter b for the 20-newsgroup corpus.
Previous experiment hints that parameter b performs best when slightly larger than one.
In this experiment, we study CFC s performance with varying values of b.
Figure 7 and Figure 8 shows the results for the 20-newsgroup corpus and the Reuters corpus, respectively.
We can observe values larger than one are signi cantly better than e   1.8.
When larger than one, increasing b lowers performance.
So, e  1.7 works well in practice and is used as the default value for parameter b in our experiments.
A number of automated TC approaches train their models from both positive and negative examples, such as Decision Tree (DT) [17, 35, 7], Rocchio [26], and SVM [12, 5].
DT employs a recursive growing and pruning branch method until every leaf node has only one class label.
The Roc-chio method builds a model that rewards a document for its closeness to positive examples and its distance from negative examples.
SVM selects support vectors in both negative and positive examples to  nd a hyperplane that optimizes decision surface.
Di erent from these approaches, CFC does




 s e r o c s


 e 1.8 Micro F1 Macro F1 e 1.7 e 1.6 b e 1.5 e 1.2 e 1.0 e Figure 8: Sensitivity Study of CFC s parameter b for the Reuters corpus.
not use the negative and positive examples directly in its model construction.
Instead, the distributions of terms are used to derive CFC s model, i.e., the centroid.
As a result, constructing centroids in CFC only incurs linear-time cost.
Previous work extracts many di erent features from documents, such as TF, IDF, information gain [13], mutual information [27, 23], Chi-square [39, 28], and odds ratio [30,
 dependency tuples [30]: 1.
(t, ci): presence of term t and membership in category ci; 2.
(t, ci): presence of term t and non-membership in cat egory ci; 3.
( t, ci): absence of t and membership in category ci; 4.
( t, ci): absence of t and non-membership in category ci.
CFC counts the occurrence of a term in a category, i.e., only the  rst tuple, while some metrics (e.g., Chi-square and odds ratio) also use other tuples.
CFC s inter-class term index is most similar to IDF [25, 9], but di er in the fact that CFC counts the number of categories containing speci c terms.
Unlike TF-IDF, CFC proposes a novel combination of inter-class and inner-class indices for computing feature weights.
Many web applications [2, 38, 22] have used TC for various purposes.
Xue et al. [38] discussed the problem of classifying documents into a large-scale hierarchy, which  rst acquires a candidate category set for a given document, and then performs classi cation on a small subset of the original hierarchy.
Search engines can use TC to improve their query classi cation results [2].
Our CFC classi er is fast on training and classi cation, and can be easily updated with incremental data.
Thus, CFC could be used in these web applications, providing TC support.
We designed the Class-Feature-Centroid (CFC) classi er for text categorization and compared its performance with SVM and centroid-based approaches.
Experimental results on Reuters-21578 corpus and 20-newsgroup email collection show that CFC consistently outperforms SVM and centroid-based approaches with both micro-F1 and macro-F1 evaluations on multi-class, single-label TC tasks.
Additionally, when data is sparse, CFC has a much better performance and is more robust than SVM.
Our CFC classi er proposes a novel centroid incorporating both inter-class and inner-class term indices.
Both can be e ciently computed from a corpus in linear time and can be incrementally updated.
In the testing phase, CFC adopts a denormalized cosine measure, instead of a normalized prototype vector.
This is to preserve prototype vectors  discriminative ability and enlarging e ect.
Experimental results demonstrate that this denormalized approach is more e ective for CFC.
We are applying the CFC approach to the multi-class, multi-label TC task, preliminary results have been promising.
We will continue this e ort in the future.
Additionally, we plan to investigate the performance of CFC for large-scale web documents.
Source code for this work is available at http://epcc.
sjtu.edu.cn/~jzhou/research/cfc/.
Acknowledgment We would like to thank Tao Yang and the anonymous reviewers for their insightful comments on this paper.
This work was supported in part by 863 Program of China (Grant No.
2006AA01Z172, 2006AA01Z199, and 2008AA01Z106), National Natural Science Foundation of China (Grant No.
jiang Program (Grant No.
07pj14049).
