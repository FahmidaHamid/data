The web has become an important medium to distribute information from various sources.
Web sites like Blogo-sphere, YouTube, Digg, Yahoo!, Google News, MSN; news feeds like Associated Press and Washington Post provide users with a wide range of choices to keep up with diverse content in a timely fashion.
However, explosion of content may make it di cult for users to select the right content to look at   a phenomenon that is also refereed to as information overload.
In fact, content publishers and aggregators select the best and most relevant content to attract and retain users to their site on an ongoing basis.
In general, a publisher page is composed of several sections; some serve personalized content, some are links to applications (e.g.
email), some allow users to add new content of their choices and so on.
In fact, several sites also publish a module with most popular content; Digg, Yahoo!
Buzz, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Top Stories on Google News, Top Picks on MSN are some examples.
Often, human editors select a set of articles from a diverse content pool.
While editorial selection prunes low quality content and ensures various constraints that characterize a site (e.g.
in terms of topicality) are met, the quality of a site may improve further if it adapts quickly to show the best articles.
One way is to constantly monitor the content quality (de ned through user feedback: clicks, votes, etc) of articles over time.
In this paper, we develop and illustrate novel statistical methods to track article quality for a prominent module, called the Today Module, published on the Yahoo!
Front Page.
Today Module Overview: Figure 1 shows the location of the Today Module on the Yahoo!
Front Page, that consists of four tabs: Featured, Entertainment, Sports and Video.
Each of the four tabs on the panel publish articles at four positions.
Unlike the other three tabs, the Featured Tab is not tied to a particular topic and selects articles from a diverse content mix.
We label the four positions on the Featured Tab as F1, F2, F3, F4.
Only one of the four footers on a tab is  active  as a story (see Fig 1).
The F1 story is displayed by default but a user can switch to another story (at F2, F3, F4) by clicking on the corresponding footer.
Since the F1 position has maximum exposure, it is the prime spot on the Today Module.
At any given time, the system picks the top four articles from an active set; these are displayed at F1, F2, F3, F4.
New articles created by editors are periodically pushed to replace some old ones; the procedure helps keep up with novel and important stories (e.g., breaking news) and eliminate the irrelevant and fading ones.
Several measures based on user feedback are generally used to measure article quality scores; those that rely on user clicks have strong signal and are readily available.
When normalized by amount of exposure that is typically measured by the number of times an article is shown at a location (e.g, at a certain position), we get a measure called click-through rate (CTR), i.e., number of clicks per display; throughout we use CTR to select the best articles in our application.
Other measures like votes, time spent reading the article, etc can also be used; the choice depends on the application.
Contributions: We provide a modeling approach to estimate CTR after incorporating spatio-temporal correlations and adjusting for user fatigue.
In particular, we propose dynamic models to: (a) Track article CTR at a single location through a dynamic Gamma-Poisson model.
(b) Combine information from correlated locations (e.g., other positions) through dynamic linear regressions to improve tracking per-us to accurately measure article CTR at fairly  ne temporal resolutions.
In this paper, we estimate CTR aggregated at 5-minute time intervals.
Serving-biased versus completely randomized data: We analyze data obtained from two buckets.
A bucket represents a disjoint set of randomly selected users who are served content according to some scheme (referred to as a serving scheme hereafter).
Since our goal in this study is to serve the most popular, we consider a bucket (estimated most popular, EMP hereafter) that continues to show the best stories at the F1-F4 positions unless article popularity scores change with recent data.
In addition, we create another bucket (random bucket, RND hereafter) that serves a randomly selected set of four distinct articles to each user visit.
Previous studies of content optimization systems [15, 16] only report  ndings based on data obtained from EMP bucket.
In fact, the completely randomized nature of data obtained from RND helps us study subtle user-content interaction patterns in an unbiased way; conducting such analysis from EMP data alone requires adjusting for confounding factors (e.g.
repeat exposure, selection bias in displayed articles), which is di cult to carry out in practice.
Almost every article CTR shows signi cant temporal variation and di erent behavior in the two buckets.
Figure 2 illustrates this for a randomly selected article.
We observe a sharp F1 CTR decay in the EMP bucket (discontinuities indicate periods when the article was taken out of F1) but a strong time-of-day, day-of-week pattern (TOD hereafter) in RND with almost no CTR decay; the diurnal article CTR is several times higher than the nocturnal one (relative to PST).
In fact, TOD in RND and decay in EMP are observed for almost all articles.
Next, we explain TOD variations and decay through a comprehensive data analysis.
For simplicity, we only analyze article CTR at the F1 position; we discuss multiple positions later in this section.
Regression analysis: To parsimoniously estimate TOD variations in article CTR, we perform a regression analysis separately for each bucket.
Let pitr denote the CTR of article i in time interval t with repeat exposure r, where r is the fraction of users in t who have previously seen article i at F1.
Through extensive o ine analysis, we converged on the following CTR model that provides a good explanation of CTR dynamics.
log pitr =  i + f (t) + g(r) (1) where  i s are main e ects that represent the adjusted popularity of article i; f (t) is a periodic (period of one week) adaptive regression spline function with the knots selected through the AIC criteria [9], and g(r) is a quadratic function that models article CTR decay.
In fact, we plugin the estimate of f (t) from RND (black curve in Figure 3) to obtain f (t) for EMP.1 The goal of this onetime o ine regression analysis is not to build an online model that tracks article CTR but to illustrate and estimate the TOD pattern, and provide an explanation of CTR decay as a function of repeat
 son regression with CTR given by Equation (1).
It has 707 parameters and provides an excellent  t (deviance   0) with few parameters.
Figure 1: The Today Module on Yahoo!
Front Page formance.
(c) Adjust for user fatigue in the recommendation process by modeling CTR drop through repeat-exposure features.
Our modeling assumptions are validated via extensive exploratory data analysis.
In fact, our analysis on completely randomized data provide new insights on various aspects of user-content interaction.
In particular, we show article CTR decay is explained better through amount of repeat exposure, instead of article lifetime as suggested by [15]; article CTR at di erent positions are correlated but the correlation is article-speci c and evolves over time   the conventional approach of using article-independent factors to translate CTR among positions can lead to inferior system performance (which will be shown in Section 4); there is strong time-of-day and day-of-week e ect in article CTR that can be largely attributed to the location of the user (US versus International).
To the best of our knowledge, a large scale study similar to ours on randomized data have not been reported for a web application before.
We note that although we present our models as enhanced methods of selecting popular stories, it is easy to extend our methods to perform personalization for user segments.
In particular, article CTR from our models can be used as informative features in regression models; one can also track article CTR separately in di erent user segments in order to provide segment-speci c popular stories.
A detailed analysis and illustration of such extensions is beyond the scope of this paper; we provide further discussion of this issue in section 5.
Our roadmap is as follows: In Section 2, we report on an extensive exploratory data analysis that motivate our modeling in later sections.
In Section 3, we describe our spatio-temporal modeling approach.
Section 4 report on extensive experiments (real and synthetic data) to show the e ectiveness of our methods.
We end with a discussion and scope for future work in Section 5.
We mention two key aspects of our data below.
Large volume of click-through data: The Yahoo!
homepage gets hundreds of millions of daily user visits; it is the most visited content page on the web.
The Today Module is an important component of the homepage that helps in improving user engagement, often re ected through clicks on articles displayed on the Featured Tab.
Large amount of (b) CTR in the Random Bucket Figure 2: Empirical F1 CTR of a random article in EMP and RD Figure 3: TOD pattern for overall tra c (black solid curve) vs. US-only tra c (red dashed curve) Figure 5: View volumes: US tra c (red solid curve) vs. international tra c (blue dotted curve) TOD.
Figure 5 shows the scaled number of page views per hour over a week (relative to PST) for US and international users separately.
Not surprisingly, the Yahoo!
Front Page is mostly visited by US users during the day and international users during the night.
We note that the CTR of each article for US users is several times higher than that of international users (articles are programmed mainly for US audience), hence the nocturnal CTR are substantially lower.
Re-estimating f (t) via model in Equation (1) for US only visits (red dashed curve in Figure 3) still shows a TOD pattern but with signi cantly smaller variations.
The regression model does not provide a good  t to international visit data due to data sparseness (low click and visit volumes).
Further analysis with user and article features failed to explain the TOD pattern in US tra c.
In general, it is dif- cult to construct temporal features related to population changes that can explain away the temporal variations in article CTR; capturing such latent population changes directly through a dynamic model that tracks article CTR (instead of population changes) is the approach we follow in this paper.
However, sharp temporal changes in CTR require the dynamic models to adapt more, forcing it to use a small amount of recent data which may increase variance and deteriorate tracking performance.
Hence, it is always useful to look for features that explain large temporal  uctuations in CTR.
In fact, all our analysis in subsequent sections will be based on US only user visits (we track international visits separately).
Explaining CTR decay through per-user features: Figure 4: Decay as a function of repeat exposure exposure.
The excellent  t of model in Equation (1) also supports a common TOD pattern and decay curve across articles.
Explaining decay pattern: Estimated decay (i.e., g(r)) in Figure 4 is largely due to the amount of repeat exposure and not the article lifetime as some earlier studies had indicated [15].
In fact, Figure 2(a) shows an example where article CTR increases with lifetime when the article is put back at F1 after being taken out of EMP for a while.
Indeed, the fraction of repeat exposure decreases when the article reappears and hence it gets a higher CTR, relative to when it was shown previously.
Explaining the TOD pattern: To explain estimated TOD pattern (i.g., f (t)) in Figure 3, we performed an extensive slice and dice analysis of the data that revealed tra c split by US versus international visits best explain
 (b) Views since last click Figure 7: F1 to F2 CTR ratio during a day Bucket
 Random Table 1: Percentage of clickers F1 only F2-F4 only Both F1 & F2-F4





 However, feedback received from footers provide additional source of information that is potentially useful in improving CTR estimates.
In fact, it is ideal to have a joint model that learns from all positions to improve individual positional estimates.
Prior work (see [13, 12]) assume a model that translates article CTR among positions through a common article-independent but position-dependent ratio, i.e., CT R(position x) =  x,yCT R(position y), where    s are unknown constants independent of articles.
In this section, we study correlation among positional CTR for articles over time by using the RND data.
We recall that for RND, each article is served at all positions, uniformly at random, in each time interval.
To study the correlation between F1 and Fx article CTR over time (x = 2, 3, 4), we  rst note that footer CTR are similar (but signi cantly lower than the F1 CTR); hence we only look at the relationship between F1 and F2.
In Figure 7, each curve is the F1-to-F2 CTR ratio of an article during its lifetime.
As evident, articles have di erent ratios that change over time, clearly violating the assumption of an article-independent ratio used in prior work.
Next, Figure
 articles over time.
Although there is a strong positive linear relationship for each article, there are substantial variations in the straight lines  tted to each individual articles (both in slope and intercept).
Figure 8(b) shows the distribution of slope as function of average F1 CTR across articles.
As evident, variation in slopes is not explained by F1 article CTR.
(Intercepts also show a similar behavior.)
In fact, none of the features we analyzed (e.g., article lifetime, time of day, article categories) were able to explain the article speci c F1 to F2 relationship.
We also note that there is a di erence in users who click on F1 and footers.
De ning a clicker to be a user who clicked the Today Module at least k times in a month, Table 1 shows the percentage of clickers who clicked on F1 only, clickers who clicked one of the footers only, clickers who clicked both, for k = 1.
(Other values of k provided similar results.)
As seen, overlap in F1 and footer clickers is not large.
Our exploratory analysis clearly shows that article CTR is dynamic.
Although we believe the root cause of dynamic (c) Previous F1 Views (d) Time since 1st view Figure 6: Relative CTR as functions of di erent user-activity features.
(relative to  rst-view CTR) Although aggregate repeat exposure explains CTR decay adequately, adjusting for user fatigue in the recommendation process requires a model that explains CTR decay through user level repeat-exposure features.
Prior exposure to articles may occur in several ways and with varying degrees of strength.
We consider features that are based on number of previous article views at the story position, at the footer position, number of previous story clicks, number of views since last click and time since  rst view.
One expects previous clicks to be a strong exposure that should substantially reduce the user s propensity to click on the article again.
However, this is not true since each article is composed of multiple links in our application; users who interact with an article generally engage more by exploring multiple links.
Figure 6(a) shows variation in CTR with number of previous clicks, relative to the  rst-view CTR (probability of a click on  rst exposure to the article); it shows an increased propensity of repeat click.
Number of views since a user s last click, shown in Figure 6(b), provides a better explanation for decay, but is noisy.
Out of all the features we looked at, number of previous F1 article views, shown in Figure 6(c), is the best predictor of decay; the ones based on time elapsed since  rst exposure of di erent types, e.g., Figure 6(d), were noisy and weakly predictive after the  rst  ve minutes.
Presentation bias a ects article CTR, i.e., the same article displayed at di erent positions under identical conditions will have di erent click rates.
In our scenario, the F1 position is prominent and receives more attention than footers.
the repeat-view features Ru of user u.
We note that  0i(cid:96)t is independent of the user u; we adjust for user fatigue by an  exponential tilt  to the  rst-view CTR through a function that only depends on the repeat-exposure features.
Thus, our modeling approach consists of: (a) Dynamic model to estimate  0i(cid:96)t for a  xed location over time.
(b) Regression model to enhance the estimate of  0i(cid:96)t at the target location (cid:96) by combining information from correlated locations.
(c) Estimating the function g(Ru) to adjust for repeat exposure to article per user.
We note that although we use the scoring function in Equation 2 to select the top-k articles in a content recommendation system in this paper, the estimation technique is general and has wider applicability.
We now describe our approach to select the top-k articles to be shown in the EMP bucket in the context of our application.
We note that locations in our application are the positions at which articles are shown.
Since one of the position (F1 in our case) is more important than others, we rank articles based on their predicted F1 CTR (or some monotone transform) in the next time interval.
This is a reasonable strategy if F1 article CTR is independent of CTR of other articles that are shown at footer positions.
This is indeed the case in our application as shown empirically in the Appendix.
The presence of RND bucket ensures we obtain data on all live articles at all times; thus we can always predict CTR of a live article in the next time interval.
As discussed earlier, we build models to track  rst-view article CTR and adjust for user fatigue through repeat-exposure features to provide each user with user-speci c most popular articles.
In the rest of the paper, we describe candidate models for each component of our uni ed spatial-temporal modeling approach that are motivated by our content optimization problem.
More speci cally, we address the following.
  E ective tracking of article CTR at a single position: As shown in Section 2, temporal dynamics of article CTR is di cult to model through known features   models that quickly adapt to temporal changes are attractive.
We note that articles that are in the explore mode (either in RND or some other small bucket) may receive lower views (and clicks) compared to those that are promising.
This may imbalance the article view distribution over time; our tracking model must be robust to such imbalance in sample size.
We provide a Bayesian time series model based on a Gamma-Poisson assumption (validated through empirical analysis) that is both accurate and robust.
  Improved tracking by combining information across positions: With small sample size, estimated article CTR at target position may improve substantially by combining information from other positions.
However, variation of position correlations across articles and time makes this a challenging modeling task which, to the best of our knowledge, has not been addressed before.
We provide a dynamic regression model to address this issue in Section 3.2.
  Adjusting for user fatigue: As shown in Section 2, article CTR decay with repeat exposure.
Developing user-speci c repeat-exposure features that can appropriately down-weigh article CTR per user is of paramount importance for good performance.
In Section 3.3, we (a) F1 vs. F2 over time (b) Regression slope Figure 8: Relationship between F1 and F2 CTR.
In plot (a), each curve starts with (F2,F1) CTR at time 0 at one of the end points and shows the temporal relationship for the article over time.
In plot (b), slopes of linear regression  ts for around 300 article curves are plotted against mean F1 article CTR.
CTR is user-population change over time, it is often hard to identify features that capture such population changes in practice.
Thus, the ability of tracking CTR over time at each position (enhanced by combining estimates from di er-ent positions) is useful for content optimization systems.
We also point out the importance of using completely randomized data for understanding the dynamics of such systems in an unbiased way.
Analysis based solely on non-randomized serving data (e.g., data from the EMP bucket) may be misleading.
For example, Wu and Huberman [15] analyzed the serving data obtained from digg.com where they empirically show that popularity of an article decays quickly over time and modeled this decay subsequently using article lifetime (time elapsed since  rst appearance).
As in our application, we conjecture that the true reason for decay in their application is not lifetime, but the amount of repeat exposure.
Also, since most articles in their context have short lifetime and they were not able to put an article at a prime position for long periods of time, they did not observe the TOD pattern we see in RND.
Furthermore, they conducted a followup analysis [16] comparing di erent serving schemes based on their previous model but using a global positional translation factor for each position (i.e., all stories have the same factor).
Here again, a global translation factor may not be correct as we observe in our application.
Finally, we note that some interesting temporal patterns were reported in the context of search in [10].
We describe our spatio-temporal approach to modeling CTR of a single article.
We shall refer to the spatial coordinates as locations, each location represents an independent source of information that is correlated with the CTR at the target location.
For instance, the target location may be the F1 position and the footers (F2-F4) may consist of other correlated locations.
We assume the CTR of article i when shown at location (cid:96) to user u at time t is given by  u,i(cid:96)t. To account for user fatigue, we assume the following  u,i(cid:96)t =  0i(cid:96)t exp{g(Ru)} (2) where  0i(cid:96)t is the  rst-view CTR (probability of click on  rst
 after adjusting for fatigue in a model based way.
Before we proceed further, a word about our notations.
Since decay is modeled using features, we drop the user index.
Throughout,  ixt, cixt and vixt denotes the true article CTR, the number of clicks and the number of views (respectively) obtained by showing article i at location x in time interval t, for users who see the article for the  rst time.
Some or all subscripts are dropped when not needed.
Unless otherwise mentioned, CTR would always mean  rst-view CTR in the rest of this section.
We begin with CTR tracking for a single article at a  xed location, and drop the article and location indices.
At any given time interval t, we have a prior probability distribution for the CTR  t that is based on data until time interval t 1; this is combined with the observed clicks and views (ct and vt respectively) at time t, to obtain the posterior probability distribution of CTR  t at time t.
Gamma-Poisson and Beta-Binomial: For occurrence (or count) data, Beta-Binomial and Gamma-Poisson are attractive choices [4].
For a Beta-Binomial model, the conditional distribution of clicks c given views v and CTR   is a binomial with mean v , and the prior for   is Beta( , ) that has mean  /  and variance  ( )  2(1+ ) .
The posterior of   is Beta(  + c,   + v).
For the Gamma-Poisson model, the prior of   is Gamma( ,  ) with mean  /  and variance  / 2, while the observation distribution of c given v and   is Poisson with mean v ; the posterior of   in this case is Gamma(  + c,   + v).
We work with the Gamma-Poisson model throughout the paper.
In fact, we validate the modeling assumption on our data in the Appendix.
Dynamic Gamma-Poisson (DGP) model: We now describe our dynamic Bayesian model that is based on a Gamma-Poisson assumption and  tted through the  discounting  concept pioneered by [11].
More speci cally, consider obtaining CTR posterior  t at time t. Assume the posterior of  t 1 (CTR at time t  1) is Gamma( t 1,  t 1) with mean  t 1 and variance  2 t 1.
Note that this posterior captures all information relevant to CTR based on observed data until time t   1.
The conditional distribution of ct for known  t is Poisson(mean=vt t).
Now, to compute the posterior of  t, we need to decide on a prior for  t; it is natural to use the posterior of  t 1 as the prior.
However, such an approach gives equal weight to all previous data points and adapts slowly to CTR changes over time.
The discounting concept solves this problem by using a dilated version of  t 1 as a prior (dilation increases variance; mean is unchanged.
), i.e., prior for  t is now Gamma( t 1,  t 1) t 1/ , 0 <     1.
The posterior of  t is with variance  2 given by Gamma( t =  t 1 + ct,  t =  t 1 + vt), where   determines the rate of adaptation of the tracker; it is estimated using a tuning dataset.
Note that small values of   adapt faster and may lead to high variance; large values of   adapt slowly and may lead to high bias in predictions.
In our application, since the TOD pattern for US tra c is not too sharp, values of   in the range of [.95, 1) worked well.
This model is simple to implement.
Starting with  0 and  0 as our initial prior parameters (which can be estimated based on historical data analysis), for each interval t, the update formula for ( t,  t) is given by  t =  t 1 + ct  t =  t 1 + vt (3) where vt = ct = 0 if the article is not shown in some interval t. The predicted CTR for the next interval has mean  t/ t and variance  t/ 2 In fact, the posterior mean  t/ t is t .
the predicted CTR for next interval t + 1.
A closer look k=0 ct k k +  t 0 and at Equation 3 shows that  t = k=0 vt k k +  t 0; thus the posterior mean is a ratio  t = of exponentially weighted sums (clicks and views).
(cid:80)t 1 (cid:80)t 1 Alternative models: Our DGP model is di erent from a commonly used model that tracks CTR through a exponentially weighted sum of per-interval click-to-view ratio (ct/vt); DGP smooths the clicks and views separately instead and provides a more robust model, especially for imbalanced data.
We note that the above update rule can also be applied to the Beta-Binomial model, but the variance dilation would be approximate, not exact.
In prior work [1], we applied the Gaussian Kalman  lter with logit transformation to dynamic CTR tracking.
This model is sensitive to noisy or sparse data.
Kalman  lters can also be de ned based on more appropriate distributions, e.g., Poisson observations with Gamma states.
However, the update formula is not closed-form and our update rule can be thought of as an approximation to that.
Our exploratory analysis in Section 2 clearly shows strong article speci c spatial (more speci cally, positional in this case) correlation that evolves through time.
With small sample size, estimated article CTR at target location may improve substantially by combining information from other correlated locations.
We propose a dynamic linear regression model (DLR) to accomplish this.
Dynamic Linear Regression (DLR): Let  xt denote the CTR of an article at location x in time interval t, for x = 1, ..., K. Without loss of generality, assume our goal is to estimate the CTR  1t at the target location by using data from all correlated locations  xt.
Let Dxt denote aggregated clicks (cx1, ..., cx,t 1) and views (vx1, ..., vx,t 1) for the article at x observed before time t. Our goal is to estimate the posterior distribution of  1t combining information from D1t, ..., DKt, The basic idea of dynamic linear regression (DLR) model is to use a base model (e.g., the Gamma-Poisson model) to separately track the article CTR  xt for each location x and obtain additional information for the target from location x through a dynamic linear regression ( tted using a Kalman  lter) of the target  1t on the location  xt.
Finally, information from all locations are combined to obtain a more informed posterior for the target.
Motivated by our exploratory analysis shown in Figure 8(a), we model the relationship between  1t and  xt by a linear regression.
Speci cally, we assume the following linear model:  1t =  xt +  xt  xt + xt, xt   N (0, s2 x), (4) where  xt and  xt are time-varying parameters of the linear model, and xt is the error term with variance s2 x.
To obtain information on  1t from location x, we  rst use a base model to track the distribution of ( xt | Dxt) for each location x separately.
Each base model outputs the CTR mean  xt and variance s2 xt.
Then, for a location x, we apply distribution of ( 1t | Dxt).
It can be shown that ( 1t | Dxt) has the following mean and variance:  xt = E[ 1t | Dxt] =  xt +  xt  xt xt = V[ 1t | Dxt] =  2 V[ xt] + V[ xt]s2
 x xt +  2 xts2 xt +  2 xtV[ xt]+ Finally, we combine each individual ( 1t | Dxt) through the following standard proposition[6].
Proposition 1.
Assume  1t has an uninformative prior.
xt), for x = 1, ..., K, and D1, ..., If ( 1t | Dxt)   N ( xt,  2 DK are mutually independent given  1t, then ( 1t | D1t, ..., DKt) is normally distributed with E[ 1t | D1t, ..., DKt] = V[ 1t | D1t, ..., DKt] = (cid:80) (cid:80) 1(cid:80) x(1/ 2 xt) xt x(1/ 2 xt) x(1/ 2 xt) The revised mean and variance formulae are used to get an updated CTR posterior for the base model through method of moments.
We now discuss estimation of time-varying regression parameters in Equation 4 through a Kalman  lter that uses output of the base model.
Speci cally, for each position x, we use the following state-space model: (cid:183) (cid:184) (cid:183)  1t =  xt +  xt xt + xt,  xt  xt =  x,t 1  x,t 1 + , (cid:184) xt   N (0, s2
 xs2 1t +  2 xts2 xt) 1t and s2 x is the model error variance; s2 where s2 xt are the variances of ( 1t|D1t) and ( xt|Dxt) that are outputs from the base models.
In fact, the variance formula for xt above can be derived using the formula of iterated variance calculation: V ar( 1t) = E 2t (V ar( 1t| 2t)) + V ar 2t (E( 1t| 2t)).
Note that we assume V ar( 1t| 2t) = s2 1t; this incorporates the uncertainty involved in  1t.
 x,t 1 and  x,t 1 are the regression parameters from the previous interval; and   is the variance-covariance matrix controlling how the regression relationship evolves over time.
Note that s2 x and   are article independent constants that are estimated using historical data in an o ine manner.
xs2 We apply the standard Kalman  lter update rule [11] to track  xt and  xt over time.
The Kalman  lter outputs  xt,  xt, V[ xt], V[ xt] and C[ xt,  xt] are used in computing  xt and  2 xt in Proposition 1.
Alternate Models: Our spatial modeling approach is simple, scalable and is motivated by methods used in meta-analysis[7] that improve estimate of a quantity by combining information available from di erent independent sources.
An alternate approach would be to use a multivariate Kalman  lter that assumes CTR from di erent locations are correlated and evolves over time.
One such model would be assuming that clicks are generated according to Poisson and the state (a vector of the true CTR s of L locations) evolve over time based on a L   L covariance matrix, which is dif- cult to estimate when L is large.
As shown in Section 2, article CTR decays with repeat exposure.
Developing user-speci c repeat-exposure features that can appropriately down-weight article CTR per user is of paramount importance for good performance.
In fact, Figure 6(c) clearly shows an exponential drop-o  in overall CTR with increase in amount of repeat exposure.
To model user fatigue, it is important to incorporate exposure features per user while estimating overall CTR; especially in EMP.
We present a novel dynamic model to incorporate fatigue that provides good performance in our application.
Let  Rt denote article CTR at time t corresponding to repeat-exposure feature vector R. We assume the following factorization:  Rt =  0t exp{R(cid:48)bt}, where  0t is the  rst view CTR at time t (probability of click on  rst article exposure).
Comparing this with Equation (2), we see this is a special case of our general spatial-temporal approach with g(Ru) = R If cRt and vRt denote clicks and views at time t corresponding to feature vector R, then the distribution of our observations conditional on the state at time t is given by bt.
(cid:48) cRt |  0t, bt   Poisson(vRt  0t exp{R (cid:48) bt}) (5) Indeed, Equation (5) provides the likelihood of the state vector ( 0t, bt) at time t. We assume  0t is known and equals the posterior mean obtained as ouput of our spatial model or Dynamic Gamma Poisson model at a single position.
We now focus on updating the posterior of bt.
The prior for bt is assumed to be Gaussian with mean equal to the posterior mean  bt 1 of bt 1, and variance obtained by dilating the posterior variance At 1 of bt 1 to At 1/ .
Combining the likelihood in Equation (5) with the prior provides the posterior of bt by Bayes theorem.
Since the posterior is not available in closed form, we apply Laplace approximation to obtain a Gaussian posterior.
In fact, the posterior is approximately Gaussian with mean given by the mode  bt of log-posterior and variance At given by inverse of negative hessian computed at mode of log-posterior.
In this section, we present results on experiments with both real and simulated data.
First, we show that the dynamic Gamma-Poisson model that tracks article CTR at a single position signi cantly outperforms several alternative models.
We then show that our spatial DLR model sig-ni cantly improves CTR tracking when the target position su ers from data sparsity.
In fact, we  nd methods that use article-independent translation ratios to estimate positional e ects may hurt performance in our application.
Finally, we report analysis of our fatigue model.
We  rst report experimental results on  replay  experiments that are based on log data collected from the Today Module.
In fact, we use US tra c data from RND bucket aggregated at  ve-minute intervals collected over a month, which contains about 180 million user visits.
Parameters of all models are estimated using training set ( rst 15 days) and results are reported on the test set (last 15 days).
We subsampled our test set to simulate performance for di erent tra c volumes.
Replay methodology: To evaluate the performance of an online model, we  replay  articles retrospectively using the model-predicted F1 CTR at the next time interval, and aggregate the total number of clicks that is received by the highest ranked articles over a 15 day test period.
We only report the F1 position performance since it accounts for the majority of click tra c and hence a good approximation to the system performance (results for other positions were be worse than using data from a single position if the model does not track non-stationary correlations properly.
Why DGP-F1 is good?
In the RND data used by our replay experiments, each article receives roughly the same volume of tra c at each position.
Since footers receive much fewer clicks than F1, the noise in data from footers is higher than that from F1.
It turns out that after having the less noisy F1 tra c for every article at all times, it is di cult to squeeze additional information from the weaker F2-F4 signal even if the model that predicts F1 CTR using correlation from F2-F4 is accurate.
In fact, when the tra c volume of the target position is su ciently large at all time, there is no need to use correlation among multiple positions.
However, in many scenarios, it is not feasible to have a constant stream of completely randomized data for each article at all times.
When the pool of live articles is large, we may only be able to explore a small fraction of articles at a time.
For some web sites, human editors may want to have full control over some important positions for some periods of time.
Also, to fully optimize the CTR of a system, complete randomization is not an optimal exploration scheme.
Thus, we conduct a series of simulation experiments to understand how multi-positional models would perform with di erent tra c patterns.
In this section, we investigate the behavior of DLR with di erent tra c patterns.
We apply the loess method described in the Appendix to smooth the CTR time-series of each article at each position using the RND data, and treat each smoothed CTR curve as the true CTR  ixt of article i at position x in interval t. A simulation run is similar to a replay run.
However, in each interval, instead of using the observed clicks and views, we control the number of views vixt to be generated and simulate the number of clicks according to P oisson( ixt vixt).
Since we know the true CTRs, we can measure the predictive error exactly.
Also, because only DLR is competitive to the single-positional DGP-F1, we only report results for these two models.
Constant F1 tra c: We  rst simulate a scenario in which the F2-F4 tra c is constant (with 100 views per article, per position, per interval), and the F1 tra c is also constant but may be relatively small.
We vary the ratio between the F1 tra c volume and F2-F4 tra c volume, and show the mean absolute errors of DLR and DGP-F1 in Figure 10(a).
As seen from the  gure, DLR outperforms DGP-F1 when the F1 tra c is less than 10% of F2-F4 tra c.
Larger than that, there is almost no di erence in performance.
Periodic F1 tra c: Next, we simulate scenarios in which the F2-F4 tra c is still constant (with the same volume as before), but the F1 tra c is observed only periodically.
We vary the fraction of time intervals in which F1 data is observed, and show the mean absolute errors of DLR and DGP-F1 in Figure 10(b).
As seen from the  gure, DLR outperforms DGP-F1 when the F1-observed fraction is less than 30%.
Simulation with the UCB1 serving scheme: Finally, we try to understand the performance of DLR in a system that seeks to maximize CTR by allocating di erent amounts of exploration tra c to articles (instead of using a small  xed-size random bucket with uniform randomization).
We implemented the UCB1 scheme [2] and use it to control the (a) Replay (b) UCB1 simulation Figure 9: Model performance in the replay experiment and simulation based on UCB1 serving scheme qualitatively similar).
The target position here is the F1 position.
Models: We report results for a range of models.
DLR is the dynamic linear regression model described in Section 3.2 that tracks F1 CTR using data from all positions.
Several variations of the dynamic Gamma-Poission models (DGP) are tested: DGP-F1 uses data only from F1; DGP-Naive assumes all positions are exchangeable and aggregates clicks and views from all positions; DGP-Ratio aggregates across positions but downweighs footer views by a constant ratio (overall F1 to footer CTR ratio).
We also  tted two Gaussian Kalman  lter (GKF) models to the empirical CTR and a logistic transformation of the empirical CTR.
GKF-F1 is a univariate Gaussian model that only uses the F1 data, while GKF-Multi is a standard multivariate Gaussian Kalman  lter model.
We also consider two other baseline models: EWMA-F1 (exponential-weighted moving average) that tracks the F1 CTR  1t for an article by using exponentially-weighted moving average of empirical CTR ratios:  1t = w 1,t 1 + (1   w)(ct/vt) where, ct and vt are the observed F1 clicks and views at t, and w is tuned using the training data.
Cumu-F1 (cumulative F1 CTR) simply divides the total number of F1 clicks by F1 views observed up to a given time point.
In fact, this is a special case of DGP obtained by setting discounting factor to unity.
Experimental Results: Figure 9(a) shows the results.
For CTR tracking using data from a single positon, DGP-F1 signi cantly outperforms alternatives GKF-F1 and EWMA-F1, that use instant CTR (ct/vt) in model updates.
For CTR tracking using multi-positional data, DLR performs signi cantly better than the alternatives: GKF-Multivar, DGP-Ratio and DGP-Naive.
To our surprise, the single-positional DGP-F1 is hard to beat, only DLR can provide a slight improvement over it.
The other models that are based on a  xed positional correlation assumption have inferior performance compared to DGP-F1 since the positional correlations are not constant and vary across time and arti-
(b) Periodic F1 tra c Figure 10: Behavior of DLR with di erent tra c patterns (x-axis is log-scaled) amount of F1 exploration tra c to be given to each article, while still keeping F2-F4 tra c the same as the RND data.
Figure 9(b) shows the CTR lifts of DLR and DGP-F1 over a model that predicts article CTR randomly for di erent traf- c volumes.
As seen, DLR provides better lifts especially when the tra c volume is small.
This result is encouraging, but future work is needed to understand the interaction between DLR and an exploration scheme.
In this section, we provide results to show the e ectiveness of our user fatigue model discussed in Section 3.3.
In particular, we show the predictive accuracy measured in terms of mean absolution error (MAE) and predictive log-likelihood for models with and without previous user exposure features.
After conducting preliminary exploratory analysis, we converged on the following repeat exposure feature vector R = (R1, R2) where, R1 is number of previous F1 article views and R2 is number of previous story views through footer clicks on Featured tab or other tab click or other tab click followed by a footer click.
Other features (number of previous footer exposures, time since  rst exposures of various types) were noisy and did not look promising.
We tested the following models: m0: Null model which assumes no repeat exposure features.
m1: which assumes R = R1, m2: which assumes R = (R1, R2) and m3: which assumes R = (R1, R2, R1R2).
m3 does not converge on a number of articles for which it provided poor performance, hence we only report results for m0, m1 and m2.
De ne M1 = M AE(m1)/M AE(m0) and M2 = M AE(m2)/M AE(m0).
Figure 11 shows the plot of MAE ratio of M2/M1 (representing error reduction of m2 over m1) versus M1 (representing error reduction of m1 over m0).
First, both m1 and m2 are signi cantly better than the null model m0 on all articles, m2 is marginally better than m1 for a large number of articles.
Results for predictive log-likelihood showed similar patterns.
We have discussed dynamic models for estimating CTR of articles that incorporate information from correlated locations and decay caused by repeated exposure.
Although we demonstrated the utility of the models using a content recommendation application based on overall popularity, the models can also be used to perform personalized recommen-Figure 11: Mean Absolute Error (MAE) of user fatigue models.
dation.
One approach to personalization is to create user segments based on analysis on a large amount of historical data (using clustering or other techniques), track article CTR for each segment separately and serve the segment-speci c most popular articles.
This is a reasonable solution as our experiments in Section 4 showed, spatial modeling performs well even with extremely sparse data.
Another approach is to build personalization models (e.g., regression models) that are based on both user features and dynamic article CTR estimates obtained from our models to combine relevance and popularity.
In our preliminary study, those dynamic CTR features are the most important features in such models.
Our models are motivated by rigorous exploratory analysis.
All the modeling assumptions have been veri ed.
Experimental results con rmed good performance of our models.
