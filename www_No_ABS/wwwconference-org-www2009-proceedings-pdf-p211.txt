The rapid growth of online content has made it critical to develop methods that facilitate easy storage, searching, and retrieval of such content.
One such methodology involves placing web data into hierarchies or catalogs.
While human editors can manually categorize data into taxonomies, such editorial e ort does not scale well to the size of online content.
Furthermore, di erent types of online content such as HTML documents, images, video, social network graphs, etc.
require domain-speci c handling.
Hence, the development of a scalable and automated categorization system is important.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
The problem of developing accurate text-classi ers has been well studied in the machine learning communities.
One such approach is Support Vector Machine (SVM) classi cation.
SVMs have been shown to have exceptional performance in the categorization of large quantities of data in very high-dimensional spaces [11].
Two aspects of online data, however, make it di cult to directly apply SVM cat-egorizers in that domain.
  The data is multi-class.
Textual data such as webpages can be classi ed into many categories.
The Yahoo!
Directory, for instance, has a few million nodes into which webpages are categorized.
  The data is multi-label.
Data such as queries, webpages, products, and even users can have more than one label.
For instance, a query such as  Jaguar  can have a label set containing Animals, Software, and Automotive classes.
A commonly used approach to address multi-class classi cation is to decompose the multi-class problem into a number of binary problems.
For instance, in the One-vs-Rest (also known as One-vs-All) approach a decision boundary is learned independently for each individual class by treating all the instances belonging to that class as positive and all the other instances as negative.
During scoring, a data point is assigned to the class for which it has a maximum positive score.
Despite its simplicity, the One-vs-Rest approach yields good performance in practice.
In fact, when tuned carefully, this method has been shown to outperform other complex output-space decomposition methods and even multi-class SVMs [20].
One-vs-Rest approach could be extended to multi-label prediction as well by choosing as the set of classes for which the data point has a positive score.
A drawback with this approach is that some of the binary classi cation problems can be highly imbalanced [9, 22].
The default threshold of SVM decision function is typically optimized for accuracy and could lead to a biased prediction toward the negative class.
One possible solution is to tune a threshold based on cross-validation according to some performance evaluation metric such as F1-measure.
This thresholding strategy is demonstrated to improve the performance of the vanilla One-vs-Rest SVM [15, 7].
However, learning thresholds for each class in large scale applications is computationally expensive as there are typically a very large number of classes.
Recently, there has been a lot of e orts in addressing the multi-label problem [26].
However, most of the proposed approaches [8, 25, 28, 33, 34] are either classi er-speci c real-world query categorization problem we have1 consists of roughly 1.5 million queries in more than 6000 categories, which requires a scalable and e cient multi-label classi er.
In this work, we propose a simple and e ective approach called MetaLabeler which in conjunction with an One-vs-Rest SVM handles multi-label prediction easily.
The proposed approach automatically identi es the right number of relevant labels for each instance without the need for expensive cross-validation.
Moreover, our model does not rely on the underlying classi er and scales well to the size of the data as well to the number of underlying classes.
We conduct extensive experiments to compare our model with other baseline methods both on benchmark data and a real-world query categorization application.
It can be seen that the proposed solution tends to outperform other methods, especially when the underlying classi er makes use of meta-information about the classes, such as class hierarchies.
Multi-label classi cation studies the problem in which a data instance can have multiple labels [26].
This phenomenon prevails in a variety of tasks including document  ltering, text categorization [15], web mining [17], tag recommendation in social bookmarking system [12], etc.
Many approaches have been proposed to address multi-label classi- cation, including margin-based methods, structural SVMs [25], parametric mixture models [28], k-nearest neighbors [33], maximum entropy models [8, 34], and ensemble methods [29,
 multiple labels for collective inference.
There have also been attempts to  nd a low-dimensional subspace shared among multiple labels [10, 31].
Unfortunately, most of the proposed methods do not scale to our needs with millions of data instances in thousands of categories.
For large scale multi-label categorization system, One-vs-Rest approach, which builds binary classi ers independently for each category, is still widely used [15, 16].
A post-processing step that can set thresholds according to some evaluation measure like macro-F1 or micro-F1 avoids the expensive training procedure of the above mentioned approaches.
Yang [30] studied three di erent thresholding strategies: rank-based cut, proportion-based cut, and score-based cut and compared the pros and cons for each strategy.
An exploration of various cross-validation schemes and heuristics for score-based approach is detailed in [7].
In real-world systems, data is typically organized into hierarchies [3, 17, 24, 23].
Utilizing the hierarchy has been shown to yield performance improvements over the  at classi ers [4, 14].
Margin-based methods that take into consideration the similarities of di erent categories in the taxonomy have been proposed [1, 25, 21].
Alternatively, smoothing methods that utilize the dependency information in the hierarchy have also been shown to yield performance improvements [18, 19].
However, the computational complexity involved in training and deploying these models is a deterrent to their adoption in large scale categorization systems.
The main focus of this work, is to classify the textual data found on the World Wide Web into a set of pre-de ned
 categories or classes.
The set of classes can be large and may or may not have dependencies, such as a class hierarchy, de ned on it.
Let K denote the set of classes associated with a given categorization task.
Each class has a set of data points associated with it, say X   RN  M where M represents the dimensionality of feature space.
Each data point can have one or more class labels associated with it.
Let y   {0, 1}K be a class-vector corresponding to a data point x.
Given a new data point x that was unseen at the time of training, the goal is to be able to identify (with good precision and recall) the set of classes  y associated with x, such that  y is close to the true class-vector y.
We break the above problem into two steps.
  Step 1: To obtain a ranking of class membership for each instance x into the set of K classes.
  Step 2: To predict the number of top classes to be returned from the ranking.
Given a data point x, we  rst want to obtain a vector f (x)   RK where the score fi(x) denotes the class membership of x in class i where i = 1,       , N .
While there exists several ranking classi ers, we choose the One-vs-Rest SVM because of its superior performance in the multi-class text categorization task [11, 20].
The scores output by the binary SVMs are then used to get a ranking of the class membership for x.
Given data instances X   RN  M with class-label vector Y   {0, 1}N  K , a decision function f is built for each of the K classes by considering the instances belonging to a class as positive and all the other instances as negative.
Restricting ourselves to linear SVMs, each of the decision functions predicts a score for an instance x as below: fk(x) = wk T x + bk (1) where wk is the weight vector and bk is the bias term associated with class Ck.
Given a test instance x, we classify it as category Ck if fk(x) > 0.
The score vector f (x) can also be used to rank the class membership of x in the K categories.
To determine the top-n classes from the score vector, we learn a function from the data to the number of labels.
This involves two steps.
  Constructing the meta dataset.
  Learning a meta-model.
We use the toy example in Table 1 to illustrate the construction of meta data.
Consider the data in Table 1a consisting of 4 instances in 4 categories.
The label of the meta data (shown in Table 1b) is the number of labels for each instance in the raw data.
For example, x2 belongs to C1, C3 and C4 in the raw data.
The corresponding meta label is then 3.
Assume that the raw data x has been transformed into the meta data  (x), a mapping from the meta data to the meta label then could be learned.
There are several approaches for constructing the meta dataset: Content-based MetaLabeler The simplest approach is to use the raw data as it is.
That is,  (x) = x (2) Data x1 x2 x3 x4 Labels



 Meta Feature Meta Label  (x1)  (x2)  (x3)  (x4)



 (a) Raw Data (b) Meta Data Score-based MetaLabeler The score vector of each label f (x) can be considered as a summary of the instance associated with the set of labels and could be used as the meta data.
Hence,  (x) = [(f1(x), f2(x),       , fK (x)] (3) where fi(x) is the prediction score for each category.
Rank-based MetaLabeler Intuitively, if the gap among the prediction scores of top-ranking categories is small, it is likely that the data instance belongs to all the top-ranking classes.
This suggests another method for constructing meta data based on ranked prediction scores:  (x) =  (f1(x), f2(x),       , fK (x)) (4) where   is a function that sorts the scores and  (x) is a vector of sorted scores.
Based on the constructed meta data, we learn a mapping function from the meta features to the meta label (yML), that is, the number of labels.
A natural choice is regression.
However, the prediction of the meta-model in such case is not necessarily an integer (say, 2.5 labels), and we need to determine whether 2 or 3 labels are actually associated with the data instance.
To avoid this dilemma, we consider the meta learning as a multi-class classi cation problem, and once again resort to the One-vs-Rest strategy as it is e -cient, scalable yet e ective.
The detailed algorithm is summarized in Figure 1, in which we have an optional parameter   to set the maximum number of labels for prediction.
The training data might include some outliers with a large class label set, the utility of which is suspect, and the cut-o  can help remove such outliers.
As for prediction, the One-vs-Rest SVM works in conjunction with the MetaLabeler as follows:
 ing based on the score vector f (x).
x using either the content-based, score-based or rank-based approach.
on the meta-model.
bels for prediction.
In this section, we have proposed a simple, yet e cient solution to the problem of multi-label classi cation.
Note that the proposed algorithm is classi er agnostic as both the Input: Multi-label data (X, Y); Maximum number of labels:  ; Output: Meta-model M (x).
yML(yML >  ) =  ;
 Figure 1: Algorithm for MetaLabeler ranking classi er as well as the MetaLabeler can be designed with any powerful classi er or ranking algorithm.
Our proposed MetaLabeler, especially the score-based Met-aLabeler might appear similar to stacking.
Stacking [5] is an ensemble method in which meta-model is learned based on the probability distributions of multiple heterogeneous clas-si ers.
However, the motivation of MetaLabeler is to predict the number of labels for each instance whereas stacking is proposed to combine di erent classi ers.
In this section, we sketch other approaches that have been proposed for the problem of multi-label classi cation.
All of these approaches assume that there exists a classi er that can provide scores indicative of class memberships.
The scores that are produced are then used to determine the set of labels that are to be predicted for the data instance x.
These thresholding strategies typically fall into three categories [30]: Rank-based cut (RCut) RCut assigns labels based on the ranking of class labels for each data point x.
The ranking could be obtained by sorting the prediction scores.
Based on the ranking, x is assigned to the top n categories, where n is a user speci ed parameter.
Typically, n is set to the average length of class labels in the training data.
Suppose instances in a dataset have 3.5 labels on average, n could be set to either 3 or 4.
Proportion-based cut (PCut) In this method, for each category Ck, the test instances are sorted by the scores for Ck and the top t instances are predicted as belonging to class Ck.
Here, t is computed based on the prior probability of Ck estimated on the training data.
PCut requires the prediction scores for all the test data and hence is seldom used in real-world applications where test data arrives in an online fashion.
Score-based local optimization (SCut) SCut tunes the threshold for each category based on improving a user de ned performance measure.
The SCut strategy was optimized for di erent evaluation criteria in [7].The basic idea is to split the training data into di erent folds and cycle through each category to tune the threshold based on the performance on the validation data.
Based on extensive experiments with the k-nearest neighbor classi er on 5 di erent text corpora, Yang [30] concluded that the SCut approach tends to over t and is unstable across di erent datasets while PCut is more stable.
However, PCut requires access to all of the test data to make a prediction, ruling out its utility in most real-world applications.
A comparison of SCut versus RCut for hierarchical SCut outperforms RCut in terms of macro and micro F1 [17].
Comparing MetaLabeler with the above three approaches, MetaLabeler can be considered as local RCut with n optimized for each individual data instance.
RCut mentioned above presents a scheme to select the top-ranking labels based on the parameter n, whose value is  xed globally.
On the contrary, the number of labels in MetaLabeler is determined dynamically by the supplied data instance.
To show the e cacy of MetaLabeler, extensive experiments were conducted on two benchmark datasets.
In this section we explain the evaluation measure used, the experimental setup, baseline methods, and details of the data used in our experiments.
The commonly used performance evaluation criteria for multi-label classi cation are exact match ratio, micro-F1, and macro-F1 [7].
Given test data X   RN  M , let yi,  yi   {0, 1}K be the true label set and the predicted label set for instance xi.
Exact Match Ratio is de ned as Exact Match Ratio =



 i=1 I[yi =  yi] (5) where I is the indicator function.
Exact match ratio is essentially the accuracy used for binary classi cation extended to multi-label case.
As seen in the above equation, exact match ratio does not consider partial match between the true labels and predictions.
The alternatives which count the partial match are macro-F1 and micro-F1.
Macro-F1 is the F1 averaged over categories.
Macro-F1 =



 k=1 F k
 (6) For a category Ck, the precision (P k) and the recall (Rk) are calculated as, P k = PN i  yk i=1 yk
 i=1  yk i i ; Rk = PN i  yk i=1 yk
 i=1 yk i i .
Then F1 measure, de ned as the harmonic mean of precision and recall is computed as follows: F k

 P k + Rk =

 i=1 yk i  yk i=1 yk i + PN i
 i=1  yk i Micro-F1 is computed using the equation of F k 1 and con sidering the predictions as a whole.
More speci cally, it is de ned as Micro-F1 =

 k=1 PN i=1 yk i + PK i=1 yk i  yk i
 k=1 PN k=1 PN i=1  yk i .
(7) According to the de nition, macro-F1 is more sensitive to the performance of rare categories while micro-F1 is a ected more by the major categories.
In our experiments, all the three measures are examined carefully.
Table 2: Characteristics of Yahoo!
Data Dataset Arts Business Computers Education Entertainment Health Recreation Reference Science Social Society























 K AveL MaxL

































 Three versions of MetaLabeler are tested: content-based (M etac), score-based (M etas), and rank-based (M etar).
We do not set any cut-o  for maximum number of labels for Met-aLabeler.
We also compare our method with other thresh-olding strategies:   Vanilla SVM (SV Mv).
The One-vs-Rest SVM without any post-processing procedure.
During prediction, all the labels with a positive score are selected.
  RCut with n equal to the average number of labels per instance.
Normally, the average number of labels is not an integer.
Hence, n can be set as either n =  AveLabel  or n =  AveLabel .
The two versions are denoted as RCutc and RCuta, respectively to indicate that the methods are conservative and aggressive in predicting the number of labels.
  SCut tuned based on micro-F1 (SCuti) or macro-F1 (SCuta).
We follow the threshold tuning presented as Algorithm 1 in [7].
Essentially, the threshold tuning process cycles over each category to optimize the desired performance measure.
The  nal threshold is the average of 5-fold cross validation.
Two other algorithms (SCutFBR.1 and SVM.1) presented in [7] require a heuristic fbr value to be provided by users or selected from a set of values based on another layer of cross-validation which could be computationally expensive.
We use linear SVMs [6] as the base classi er.
The parameter C in SVM is selected from a proper set of values.
Note that the vanilla SVM s performance is quite sensitive to this parameter.
We originally used the default value (C = 1) for the SVMs and it yielded extremely low accuracy on some of the datasets.
Typically, C = 200 to 1000 gives much better result for SV Mv.
The  rst benchmark dataset was extracted from the Yahoo!
directory.
This multi-topic web categorization data was used in [28, 10].The dataset consists of 11 independently compiled subsets, each of which corresponds to a top-level category in the Yahoo!
Directory.
In e ect, this gives us
 processed following the methodology in [10].
We removed match ratio, micro-F1 and macro-F1 respectively.
Each column denotes one data set.
The entries in bold denote the best one in each column.
All the results are averaged over 30 runs.
The last column Ave shows the results averaged over 11 data sets.
SV Mv RCutc RCuta SCuta SCuti M etac M etas M etar SV Mv RCutc RCuta SCuta SCuti M etac M etas M etar SV Mv RCutc RCuta SCuta SCuti M etac M etas M etar Arts























 Busi.
Comp.
Edu.
Enter.
Health







































































 Rec.
Ref.
Sci.
Social























 Society























 Ave























 those categories with less than 100 web pages, words occurring less than 5 times and web pages with no topics.
Each web page was represented as a bag of words using the TF-IDF encoding and was normalized to unit length.
Table 2 summarizes some characteristics of the data.
N, M, K represents the number of instances, dimensions and categories, respectively.
AveL denotes the average number of labels per instance, and MaxL denotes the maximum number of labels for an instance.
While the data has, less than 2 labels on average per data point, one instance could have up to 17 labels.
We randomly sampled 2000 instances for training and the remaining are used for testing.
This process was repeated 30 times, and the averaged result is reported.
The second dataset used was the subsets of RCV1 [15].
The data is publicly available at LibSVM site2.
As in the case of the Yahoo!
dataset there are 5 subsets, each with 3000 data instances for training and 3000 for testing, with in total 101 categories.
On average, each instance has around 3.22 labels and the maximum number of labels per instance is 14.
This dataset, di erent from previous data, is highly imbalanced with the largest category having around 1400 instances and the smallest with only 1 instance in training data.
Note that 5-fold cross validation is not applicable for the rare classes.
Thresholds for SCuti and SCuta, in such cases, were selected based on the tuning on the training data.
The performance of the di erent multi-label strategies is shown in Tables 3 and 4.
Each horizontal block in the table 2http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ multilabel.html corresponds to an evaluation measure that is exact match ratio, micro-F1, and macro-F1 respectively.
As observed in Table 3, RCutc and RCuta do not necessarily improve over the performance of vanilla SVM.
Specifically RCuta which assigns labels aggressively is very likely to make mistakes as indicated by the extremely low exact match ratio score.
SCuta, SCuti, M etac and M etas, on the other hand, achieve better performance than the vanilla SVM.
However, M etar, performs worse than SV Mv.
A strong pattern observed in Table 3 is that M etac tends to outperform other methods in terms of exact match ratio and micro-F1.
The performances of SCuta and M etac are comparable with respect to macro-F1, as SCuta is the winner in 6 out of the 11 datasets while M etac wins on 4.
Note that the SCut strategies require cross-validation for each class to choose thresholds whereas the MetaLabeler needs to be tuned only once while training and is much more e -cient.
The e ciency di erence between these two methods will be discussed in detail later.
The MetaLabeler could be considered as a local RCut with the number of labels determined automatically.
To examine the di erence, we plot the content-based MetaL-abeler (M etac) versus the conservative/aggressive RCut in Figure 2 in terms of both micro-F1 and macro-F1.
The x-axis denotes the 11 datasets.
It is evident that our method by determining the number of labels dynamically for each instance, achieves superior performance in most cases.
A similar trend is observed for exact match ratio as well.
Comparing the di erent meta labeling strategies, we observe that the content-based and score-based MetaLabeler are the exact match ratio, Micro-F1, and Macro-F1 respectively.
Ave denotes the averaged result over 5 subsets.
Sub1




 Sub2




 Sub3




 Sub4




 Sub5




 Ave






















































































































 SV Mv RCutc RCuta SCuti SCuta M etac M etas M etar SV Mv RCutc RCuta SCuti SCuta M etac M etas M etar SV Mv RCutc RCuta SCuti SCuta M etac M etas M etar are comparable with the former having a slight edge.
M etar performs even worse than the vanilla SVM.
This is possibly due to the fact that it is important to have the prediction scores associated with the class labels.
This information is lost after sorting thereby leading to a bad meta-model.
The performance on each RCV1 subset and the averaged result are summarized in Table 4.
For this dataset in almost all the cases the content-based MetaLabeler yields the best performance.
The vanilla SVM after cross-validation gives a reasonably good performance after careful parameter tuning of the SVMs.
The threshold based strategies, RCut and SCut deteriorate the performance of SV Mv mostly, except that RCuta and SCuta yield a higher macro-F1 by giving more predictions than necessary to boost the recall of each category.
Only our meta-model consistently improves the baseline SV Mv in terms of all the three measures.
In contrast to the Yahoo!
directory data, SCut strategies do not work well here.
This is due to the presence of rare classes with only one positive instance in the training data.
Threshold tuning which was done on the training data possibly over t the data and becomes quite unreliable for those classes with few instances.
As before, the content-based MetaLabeler is more robust and demonstrates superior performance over other approaches.
The content-based MetaLabeler works well in both benchmark datasets.
However, the performance of the score-based and rank-based MetaLabeler is quite unstable.
This could be due to the e ect of unbounded scores in SVM prediction.
Hence, we also experimented with building a meta-model based on calibrated prediction scores via isotonic regression as detailed in [32].
Simply stated, isotonic regression maps the unbounded SVM scores into the [0, 1] range.
Figure 3

   o r c
 i

   o r c a













 Global vs. Local Rcut Yahoo!
Web Data RCut c RCut a Meta c Yahoo!
Web Data Figure 2: M etac vs. RCut shows the micro-F1 of various meta-models on 11 Yahoo!
web datasets.
Cals and Calr represent the performance of M etas and M etar after calibration.
It can be seen that calibration resulted in a worse performance for M etas while improving M etar.
This makes sense as the gap between the ranked scores is more accurate after calibration.
Calibration in general has been reported to improve the clas-si cation performance, especially the rank-related measure, over uncalibrated scores [2].
This could also be observed for our rank-based MetaLabeler.
Unfortunately, the calibration does not help for score-based MetaLabeler.
Since the scores within certain interval are calibrated to the same value, this might become problematic for those prediction scores close to the interval boundary.
So it is best to use raw content as meta features because this would cause no loss of information.
Such a content-based MetaLabeler outperforms all the other variants of MetaLabeler across almost all the multi-label datasets.
) % (

   o r c
 i











 Meta s Cal s Meta r Cal r Figure 3: Meta-Models with Calibration Yahoo!
Web Data
 On both datasets the average number of labels tends to be smaller, with relatively a smaller subset of the data having a class label set of much larger size.
This imbalanced distribution in the number of class labels leads to a conservative prediction bias with the MetaLabeler.
For instance, consider the category Society in Yahoo!
web page data as it contains the largest number of samples.
The data is presented in Table 5, with l denotes the number of labels, T ruth and P red denote the number of instances that have l labels in the test data and predictions respectively.
As can be seen, the majority of the data have only one label.
Though there are some instances which contain more than 5 labels, this is a  rare class  in the meta-model learning.
The imbalanced distribution in the training data leads to a meta-model that favors predicting lesser number of labels.
This could be easily veri ed by the P red row in Table 5.
The number of instances with only 1 predicted label is much larger than the true number and for all other cases the predicted number is less than the truth.
This indicates that our MetaLabeler picks more labels only if the instance has a strong signal of having more than 1 label.
This bias helps maintain a relatively high precision in the resultant performance.
Table 5: Distribution of Number of Labels in Society l T ruth P red
























 It was observed on Yahoo!
web page categorization data, the SCuti and SCuta methods demonstrate performance comparable to that of M etac in terms of micro-F1 and macro-F1 (shown in Table 3).
However, SCut strategy requires cross-validation to select the optimal thresholds.
If cross-validation is not applicable, SCut could over t the training data as shown in Table 4.
MetaLabeler, in contrast, tends to be more reliable.
Here we use the Yahoo!
Society data again as an example to examine the scalability of SCut and MetaLabeler.
In particular, we increase the number of training samples gradually from 1000 to 10000 and record the computation time on an AMD 4400+ desktop.
The computation time for each method is plotted in Figure 4, where SV Mv denotes the computation time to construct One-vs-Rest SVM and SCuta and M etac denote the additional computation time after the vanilla SVM is built.
As SCuti and SCuta share the same time complexity approximately, we only present SCuta to make the  gure legible.
The computation times of the di erent methods are linear with respect to number of samples.
However, SCuta increases much faster than M etac.
In particular, when the training samples scale to 10000, the additional time needed to construct MetaLabeler is 148 seconds whereas SCuta is ten times as much at 1487 seconds.
Therefore, our content-based MetaLabeler outperforms SCut with respect to clas-si cation performance and e ciency, and is more favorable in large scale multi-label categorization system.
Note that the analysis so far examined the scalability of SCut and MetaLabeler only in terms of training samples.
In a large scale categorization system, the number of classes as well as samples could be huge.
Recall that MetaLabeler builds a meta-model from the content to the number of labels.
So as long as the maximum number of labels per instance is relatively small, which is typically true in practice, the training of MetaLabeler should be e cient.
SCut, however, has to sort the prediction scores of all the training data for each class which operation scales linearly with re-i e m
 n o i t a t u p m o





 v Meta c SCut a


 Number of Samples

 Figure 4: Computation Time on Yahoo!
Society data spect to the total number of classes.
Also, SCut further requires cross-validation which only increases the computational complexity.
As we will show in a real-world application in the next section, SCut requires days to train while MetaLabeler  nishes in two hours.
The content-based MetaLabeler was shown to perform well on reasonably sized multi-label classi cation tasks.
We claim the MetaLabeler is an e cient and reliable solution that should work well even when presented with very large datasets.
In the following subsections we validate this claim with experiments on a real-world large scale query categorization system.
Internet portals and search engines aim to deliver content and ads to users that are relevant to their interests.
In such applications, it is very useful to categorize the user search queries into relevant nodes in a taxonomy of user interests.
Such a taxonomy has been constructed by human experts in Yahoo!, with nodes in the taxonomy spanning a large variety of topics including for example,  nance to home garden decoration.
These nodes are organized in a hierarchy based on the semantic similarity of the user interests.
There are in total 6,433 interest categories in this taxonomy, distributed over 8 levels.
The nodes at deeper levels correspond to more speci c user interests.
A user query can typically be categorized into multiple nodes in the taxonomy.
For instance, a query  0% interest credit card no transfer fee  is labeled as belonging to the following three categories: Financial Services/Credit, Loans and Debt/Credit/Credit Cards/Credit Card Balance Transfers Financial Services/Credit, Loans and Debt/Credit/Credit Cards/Low-Interest Credit Cards Financial Services/Credit, Loans and Debt/Credit/Credit Cards/Low-No-Fee Credit Cards In this example, labeling the query as Financial Services/ Credit, Loans and Debt/Credit/Credit Card is acceptable; however, this ignores the extra information in the query viz., that the user is really interested in a balance transfer card with no interest.
In this taxonomy, there are 14 di erent daughter nodes under the parent node Financial Services/Credit, Loans and Debt/Credit/Credit Cards and hence the speci c interest of the user can indeed be captured in this taxonomy.
Our goal is to build machine learned classi ers into this hierarchical taxonomy such that the queries can be classi ed
 2 labels
 1 label
 Figure 5: Label Distribution on Query Data Table 6: Distributions with respect to depths of the taxonomy.
#nodes represents the number of categories of that depth in the taxonomy; #instances denotes the number of data instances with at least one label at the depth; AveLabel denotes the average number of labels per query.
Depth #nodes #instances AveLabel































 accurately into relevant classes.
For training purposes, we use a data set of 1.5 million unique queries that have been manually labeled into nodes in this taxonomy by human editors.
Figure 1 shows the distribution of the number of labels per query in this dataset.
About 20% of the queries in this dataset have more than 1 label as shown in Figure 5.
The maximum number of labels per query is 26, and the average number of labels per query is 1.23.
Table 6 shows the number of categories, queries and labels with respect to depths in the taxonomy.
It is observed that most of the instances reside at levels 2 to 6.
In this subsection, we build a one-vs-rest SVM for the problem directly without leveraging any hierarchical information during training.
For evaluation, we split the data into two parts: roughly 1 million for training and 0.5 million for testing.
Each query is represented using bag of words, resulting in total 120,000 features.
Here, we compare content-based MetaLabeler with SCut only as it is the winning thresholding strategy on the benchmark data.
We used a linear SVM based One-vs-Rest approach for the construction of the meta-model.
We emphasize the fact that both operations of obtaining ranking scores and meta-label prediction are classi er agnostic.
We use linear SVMs as our underlying model because of its ability to handle large quantities of training data and ease of deployment in online, real-world systems.
Evaluation of recently developed multi-class SVMs [13] in our problem is an ongoing process.
The vanilla SVM training alone takes 12 hours to  nish.
Note that one query can have at most 26 labels and so the MetaLabeler requires no more than 26 extra binary SVMs, which adds an additional 2 hours.
Due to the large size of the data, SCut training involves many I/O operations.
For instance, the size of the prediction score  le for the training Table 7: Performance of MetaLabeler on Flat Model Depth







 Micro-F1 Macro-F1 Flat Meta















 SCut







 Flat Meta















 SCut







 data in all the categories is 47GB large and takes 3 hours to output.
After spinning the machine for approximately 27 hours, we  nally obtain the threshold tuned on the training data.
Keep in mind that in cross validation, we have to train vanilla SVM and then conduct the SCut tuning in each trial, which is way too expensive.
Table 7 shows the performance at each depth in the taxonomy.
To calculate the performance, a query belonging to a category node is considered associated with all the ancestor nodes as well.
In the table,  Flat  denotes the vanilla One-vs-Rest SVM,  Meta  denotes the content-based Met-aLabeler.
 SCut  is tuned according to the corresponding measure.
The entries in bold denotes the best model.
Evidently, with MetaLabeler, we are able to improve the  at model in terms of both micro-F1 and macro-F1 across all levels in the taxonomy.
SCut, despite the expensive computational cost, has very low recall.
The threshold have been overestimated as many queries are predicted with no labels.
This could be due to the inter-dependency of the classes and extremely small number of samples in some nodes for training.
In contrast, our proposed MetaLabeler is quite reliable and e cient.
The query dataset has meta-information about the classes, in the form of a class hierarchy.
Leveraging such hierarchical information has been shown to yield better performance than classi ers that ignore such information [4, 14, 16].
In the following subsections, we investigate the e ectiveness of the MetaLabeler in a hierarchical model.
SCut, due to its expensive training time without guaranteeing better performance, is skipped in this setting.
In order to better leverage the class hierarchy information, we build a hierarchical classi er as follows:   An One-vs-Rest SVM is built at each node based on the training data at the subtree of that node.
  For prediction, the set of classi ers is traversed in a top-down fashion.
Starting from the root node, a data instance is  rst classi ed into the set of the level 1 nodes.
The winning node in level 1 then decides the set of nodes that are to be explored at level 2.
The data is pushed down the tree until a leaf node is reached.
  To account for the data instances that belong to an internal node, we construct an arti cial  parent  leaf-node under the internal node.
If a query is classi ed into such  parent  leaf-node, the prediction output is then the corresponding parent node.
% \ ( e c n a m r o f r e









 Table 8: MetaLabeler on Hierarchical Model.
hier precision hier recall meta precision meta recall



 Depth


 Micro-F1 Depth Hier Meta























 Macro-F1 Hier Meta















 Figure 6: MetaLabeler vs. Hierarchical Model The proposed MetaLabeler could be easily embedded into the training and prediction phase of a hierarchical classi er.
A content-based MetaLabeler was built at each node in the taxonomy.
During prediction, we explore multiple paths, depending on the prediction of the MetaLabeler, using either depth rst or breadth rst search.
The  nal output of the hierarchical model is a set of labels for each query.
The micro-averaged precision and recall of MetaLabeler versus the original hierarchical model is presented in Figure 6.
The precision, as shown in the  gure, actually decreases by 1   2%.
In contrast, the improvement of recall is drastic.
The recall at deeper levels increases from 70% to over 80%.
We also show the improvement in terms of micro-F1 and macro-F1 in Table 8.
With MetaLabeler, both measures improve by 1   9% at di erent levels.
Even though we take a top-down multi-path exploration method for prediction, our model typically predicts a limited number of labels due to the predictive bias of MetaLabeler.
This e ectively avoids the explosion of number of labels for hierarchical prediction as indicated by the high precision (> 90%).
A closer look at the performance of the hierarchical model in Table 8 versus the  at model performance in Table 7 reveals that hierarchical model consistently yields a higher micro-F1 and macro-F1 at deeper levels.
The best performance is achieved when MetaLabeler is utilized together with the hierarchical information of classes.
The content-based MetaLabeler works pretty well in our query categorization problem as well as the benchmark data.
In order to get some intuition into this behavior, we checked the features with the highest weight in the meta-model at the root node of the hierarchical model.
Words such as over-stock.com, blizzard, threading, etc., rank among the top features.
The word blizzard, for instance, is associated with labels like Video Game, Computer Game Software, Fast Food Restaurant, and Weather Information.
The content-based MetaLabeler is able to extract such generic features that are typically associated with multiple labels and weight them appropriately.
Multi-label classi cation appears in various web-related tasks such as web page categorization, query-categorization, and user pro ling.
Typically, the categories are organized into a hierarchy.
Unfortunately, classifying data into the set of relevant nodes in the taxonomy is not an easy task.
In this work, we propose MetaLabeler to determine the number of relevant labels automatically.
Three versions of MetaLabeler are presented: content-based, score-based and rank-based.
Based on extensive experiments on benchmark data and our large scale query categorization problem, we conclude that the content-based MetaLabeler in conjunction with a classi er that ranks class memberships tends to outperform other multi-label prediction systems.
The proposed solution scales to millions of samples and can easily be incorporated into hierarchical classi cation systems.
The MetaLabeler requires neither expensive cross-validation nor human interaction.
The simplicity of training and scoring coupled with its performance make the MetaLabeler a viable solution for the construction of large scale multi-label classi cation system.
One limitation of MetaLabeler is that it lacks the  exi-bility to be tuned for user-speci ed precision/recall levels.
A possible solution that could address this problem is to construct a meta-model that takes into account the label con dence in the training data.
We are also planning to apply MetaLabeler to social bookmarking system with large number of labels.
Most of the work was done when Lei Tang was an intern in Yahoo!
SDS Data Mining and Research Group during summer 2008.
We thank Varun Chandola for the implementation of hierarchical categorization system.
