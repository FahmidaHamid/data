Over the past decade, as the new  killer" Internet applications emerge, new content delivery mechanisms are invented to meet the demand of these new applications.
This is evidenced by the following two examples.
Web (text/image) is the  rst  killer" Internet application, thus HTTP protocol dominated Internet traf c usage in the  rst several years of the Internet [10, 29].
Web caches and Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
Content Distribution Networks (CDNs) were thus invented to improve Web performance.
When Peer-to-Peer (P2P)  le sharing became popular a few years later, it was shown to be dominant in studies conducted in 2002 to 2005 for DSL, cable, and  ber broadband networks [7,15,22].
A lot of approaches have been proposed to improve the delivery ef ciency of P2P.
With the recent emergence of user-generated video, social networking, and TV/Movie-on-demand services, most of which run on top of HTTP, we suspect that HTTP protocol has become the dominant content delivery protocol, especially for multimedia (video/audio) content.
For example, an informal report in a lightreading article in 2006 [1] indicates that  network operators are reporting a rise in overall web traf c and a rise in HTTP video streaming".
Furthermore, the multimedia content we see today might be just the tip of the iceberg of what is coming in the next decade as more providers join the business and make more, higher-de nition content available and more subscribers access it.
As such, we believe it is timely and important to investigate the new opportunities for more ef cient delivery mechanisms for HTTP traf c again.
A natural solution is forward caching (proposed in the 90s), which is to deploy HTTP caches within an Internet Service Provider s (ISP) network caching all cacheable HTTP traf c accessed by the customers of the ISP.
Caching makes intuitive sense in that Internet content popularity is often very skewed thus offering good opportunity for reusing.
This is especially true for video content, as shown in recent studies [5,6,16].
Unfortunately, most large US based ISPs currently do not operate forward caches within their network.
The main reason for this decision lies in the economics of deploying forward caches.
Forward caches are additional hardware components (typically UNIX servers) which have to be purchased, deployed and managed at a large number of locations.
In the US the bandwidth savings can often not justify the cost of such a server deployment.
To reduce the costs associated with forward caching in an ISP, we believe that the most cost effective way of deploying forward caches is to only deploy them in selected POPs (Point of Presences) caching only selected expensive-to-deliver content.
We call this approach Network Aware Forward Caching.
To motivate and justify our solution, we  rst provide a systematic measurement study of the characteristics of the content transmitted over the Internet today and shows the dominance of HTTP traf c, followed by the comparison of the ef ciency of existing delivery mechanisms (HTTP, CDN and P2P), and the cacheability of HTTP content.
We then formulate the cache placement problem, and propose and evaluate our heuristics.
To show how dominant HTTP content is compared to other types of content transmitted over the Internet today, we characterize the Internet traf c of 100K subscribers of a US residential broadband provider.
Using both layer 4 and layer 7 analysis, we con rm our to 68% of the total downstream peak traf c.
Furthermore, HTTP has become the workhorse for data delivery: 80% of multimedia content uses HTTP as its distribution protocol (mainly  ash video) and  le downloads via HTTP contributes 10% of downstream traf- c in contrast to 0.3% for FTP.
This is a drastic change since a few years ago, when HTTP contributed to 9% of the total traf c [22], P2P was dominiant [7, 15, 22], and 63% of total residential volume was user-to-user traf c [7].
Our results show that recently the volume of HTTP content per subscriber is increasing much faster than the 26% overall increase rate: multimedia streams over HTTP and other HTTP traf c exhibits a 83% and a 53% annualized growth rate, respectively.
This means that HTTP s share will even increase further.
Our study shows that among the existing delivery mechanisms, CDNs are currently serving already a signi cant portion (46%) of the large  le transfers in the network and are ef cient in bringing the content closer to the content consumers, much better than the existing P2P technologies or typical HTTP content providers by a factor of 2 to 3 when comparing average bit-distance.
Distance travelled on the network is strongly related to the network cost of the traf c.
Furthermore, the distance traversed on a network by different sources of traf c to different points of presence (POPs) varies signi cantly.
The traf c to some remote POPs can bene t signi cantly from better delivery mechanisms such as forward caching.
In the HTTP traf c cacheability study, we found that 60% of the traf c can potentially be reused overall because it is requested more than once.
However, only 33% of the content is suited for an optimized delivery infrastructure when adding more realistic constraints.
Finally, based on these observations, we introduce and evaluate our new proposal, network aware forward caching, that increases ef ciency, reduces backbone traf c and network costs and increases end-user performance.
Contrary to a simple all-or-nothing forward caching deployment in a network, we argue that, by being network aware, partial deployments of forward caches for a subset of the POPs and a subset of the traf c sources provides greater bene ts per dollar invested.
Indeed, we just showed the disparity in ef ciency and the fact that some sources, such as CDNs are already ef cient and, therefore, don t need to be cached.
In addition to that observation about the differences in ef ciency of each source to each POP, we also note that an Internet Service Provider incurs different costs based on the nature of the neighbor sending the traf- c (e.g.
transit traf c is more expensive than peering traf c, and customer traf c even generates revenue), and we include this additional dimension in our decision process when selecting the content that will be stored on the caches.
We formulate the problem of determining the optimal deployment, and shows that it is NP-hard.
We propose one pseudo-polynomial algorithm that solves the exact problem, and a greedy heuristic algorithm that is much faster.
Our evaluation of the heuristic algorithm using realistic data shows that the optimal deployment in terms of network costs occurs when only
 senstive to the backbone costs and the caching costs.
The remainder of the paper is organized as follows.
Section 2 describes our measurement methodology.
Section 3 presents overall content composition results.
Section 4 studies how current delivery mechanisms work, and Section 5 presents the content cacha-bility results.
Section 6 presents the formulation, algorithms, and evaluation results for our network aware forward caching approach.
Section 7 reviews related work, and Section 8 concludes the paper.
This section presents our network monitoring infrastructure and data sets used in the analysis of this paper.
To achieve our objectives in this paper of characterizing broadband traf c usage and evaluating our network aware caching approach, we obtained traces from multiple vantage points of a US Broadband Providers network to understand the different aspects concerning the delivery of content to a subscriber.
In particular, the three types of data sets that we utilize in our analysis are as follows: aggregated traf c records from broadband subscriber aggregation access points, aggregated net ow records of the core backbone traf c, and unsampled HTTP header records from a single aggregation point.
In the following subsections we elaborate on each type data set.
Our analysis is based on a US Provider and networks in different geographic locations may not exhibit the same characteristics.
Our network monitoring infrastructure [11] consists of  ve network monitors analyzing traf c from 100K subscribers of a US broadband provider.
The monitors are diversely located on  ve BRAS (Broadband Remote Access Servers; an aggregation point of Digital Subscribers Lines, or DSLs) in three states (California, Texas and Illinois).
The subscriber data we analyzed was from the week of February 25, 2007 to September 30, 2008.
For our study, network monitors summarized the observed traf- c volumes every 5 minutes into  ow records.
The  ow records measure the number of packets and bytes for each identi ed applications class (described below).
To reduce resource consumption, the network monitors perform a combination of packet sampling and  ow sampling when computing these  ow records [12].
The 5-minute  ow records are used to compute hourly summaries of the aggregate traf c from all the subscribers observed at the monitor.
Our application classi cation relies on application header, heuristic, and port number analysis to determine a  nal classi cation of a  ow into an application class.
Overall, we classify our traf c into 16 categories: Web (HTTP), Multimedia (HTTP, RTSP, RMTP, etc), File Sharing (P2P), Games (Quake, WoW), Net News (NNTP), Business (VPN, Databases), VoIP (SIP), Chat (IM, IRC), Mail (POP3, IMAP), DNS, FTP, Security Threats, Other, ICMP, Other-TCP and Other-UDP.
(The examples in brackets are non-exhaustive of what we identify.)
We base these categories on a determination of the use of the data and not explicitly on the protocol.
This is most signi cant for HTTP traf- c, which we classi ed into either Web or Multimedia category.
Therefore, we separate HTTP traf c by mime type if the mime type of a  ow is for a video or audio format we classify this  ow as HTTP Multimedia instead of HTTP Web.
In addition, Gnutella and BitTorrent tracker-based HTTP traf c is classi ed as those speci c P2P applications.
We adopt many of the same packet payload signatures described by Sen et al. [25] and Karagiannis et al. [20].
We also use additional signatures to identify application subclasses.
For instance, as outlined above we use the mime type information in HTTP  ows to further classify HTTP  ows.
In addition, we extract from the control channel the information needed to identify the data channel of the FTP, RTSP, and Skype protocols.
Additional P2P traf c was identi ed with other P2P speci c heuristics.
For example, we use the announced port in the tracker messages for BitTorrent to identify incoming encrypted  ows as P2P.
However, due to P2P applications such as BitTorrent using encryption to obfuscate their protocols, we believe much of our unknown TCP traf c (in the Other-TCP class) is P2P as well.
We have based this inference on additional analysis we performed on the  ow characteristics and the payload information in the  ows this traf c is consistent with encrypted P2P traf c.
Generating and validation of additional signatures for encrypted P2P traf c is left as future work.
Lastly, we use some layer 4 port numbers to identify any remaining traf c not classi ed using signatures and heuristics.
Another type of data utilized in our analysis are aggregated Net-Flow [9] records from the core backbone of the network.
This data allows us to measure the ingress and egress traf c on the various Peering, Transit or On-Net links of the network.
The data used in our analysis is from September 27, 2008 until October 5, 2008.
The data sets were obtained using a NetFlow collected on the backbone router of a US Broadband Provider.
These traces provide the amount of ingress and egress traf c volume between core routers.
To minimize the performance impact on the routers, we used deterministic packet sampling with a rate of 1 packet out of 500.
We then use smart sampling [12] to further reduce the data volume.
To facilitate the study of HTTP cacheability, we analyzed the HTTP header data from approximately 20K subscribers during the week of January 27, 2008 to February 2, 2008.
During our analysis we correlated the HTTP requests with the actual TCP  ows to obtain the actual  ow sizes.
This step was taken because not all content sizes are available in the Content-Length  eld of the HTTP header.
For instance, some objects such as dynamically generated pages or streaming content will sometimes have a Content-Length of 0.
In our analysis we used from the HTTP header  elds the GET, POST, Cache-Control, Pragma, and Content-Length  elds of the HTTP header and the IP address of the web server.
All other information such as the subscriber IP address and cookie values were not analyzed.
In total, we analyzed approximately 550 million requests representing 45 TB of traf c with an average request size of 91 KB.
Unlike the smart-sampled records in the previous two types of data sets we use, the HTTP data in our analysis is unsampled and includes all requests.
This section provides an overview of the growth and overall application breakdown in the broadband data we studied.
We show that the volume and fraction of multimedia content delivered using HTTP is increasing rapidly and that P2P traf c as a percentage of traf c is actually decreasing.
HTTP is more generally becoming the protocol of choice for various kinds of activities such as software distribution, multimedia and P2P applications.
Figure 1 shows that a large US broadband ISP saw a relatively stable growth rate of the average downstream traf c per subscriber of 26% per year, with an above average growth rate for the last 2 years.
An interesting observation when looking closer at the busy hour traf c per subscriber is that, while in the  rst years of this period, the average downstream traf c per subscriber was growing faster than the downstream traf c per subscriber, the opposite can now be observed in the last 2 years.
This indicates that synchronous applications (e.g.
instant watching multimedia streams) are recently growing faster than asynchronous applications (e.g.
P2P  le sharing).
It is also important to keep in mind that the behavior during the busy hour is what really matters.
The Internet infrastructure is engineered for the peak demand and that generally l e m u o
 c i f f a r
 d e z i l a m r o
 Average Peak Traffic f(x)=1.0075*exp(0.0037x) Average Traffic g(x)=1.0183*exp(0.0044x)













 Time Figure 1: Weekly average downstream traf c per subscriber during the busy hour based on several million subscribers l e m u o
 c i f f a r
 d e z i l a m r o














 Time HTTP Web HTTP Multimedia Multimedia Other FileSharing Other-TCP Other-UDP NetNews Figure 2: Normalized Weekly traf c per Application Class during the Busy Hour has spare capacity the rest of the time.
The busy hour is de ned as the 1-hour time span during a day which exhibits the largest average traf c.
Figure 2 shows the weekly normalized application traf c volumes during the busy hour based on our aggregate BRAS traf c records.
There are two major trends that can be seen.
The  rst is that HTTP traf c, both Web and Multimedia, shows consistent growth during the busy hour over the observed period.
In the  g-ure, HTTP Multimedia and HTTP Web traf c exhibit a 83.1% and a 52.9% annualized growth rate, respectively.
The second is that P2P traf c has remained steady and shows a decline in its percentage share of the overall traf c mix.
These may be important observations as they contradict reports claiming that P2P traf c is continuing to increase at dramatic rates.
The growth of HTTP traf c and especially the HTTP Multimedia traf c is the most signi cant cause of broadband subscribers traf c growth.
Next, we look at the characterization of the overall application mixture of the traf c.
Table 1 shows the application mixture of the downstream and upstream traf c at all  ve monitors during the week of January 28, 2008 to February 3, 2008.
The downstream traf c volumes are typically 3 times greater than the upstream traf- c volumes during the busy hour.
HTTP is the dominant protocol and accounts for the largest volume of traf c on the network and Table 3: Multimedia Breakdown (includes HTTP Multimedia) Class HTTP Web HTTP Multimedia FileSharing Other-TCP Multimedia Other Other-UDP Games NetNews Business Voip Chat Mail Dns Ftp SecurityThreat Other
 Downstream Upstream Busy Hour
















 Average
















 Busy Hour
















 Average
















 Table 2: HTTP Breakdown Class /http/video /http/text-image /http/download /http/javascript /http/audio /http/other /http/ ash /http/https /http/otherapp /http/xml /http/binary /http/of ce /http/rss % Busy Hour Traf c % Average Traf c

























 accounts for 68% of the downstream traf c during the busy hour.
HTTP has also taken over  le downloads and FTP is now only a very small percentage of the overall traf c.
P2P makes up at least 9% of the downstream traf c during the busy hour and 12.3% of all downstream traf c.
If we assume in part that much of the TCP-Other traf c is due to encrypted or uniden-ti ed P2P traf c, then P2P still makes up a maximum of 17% of the downstream traf c.
As we have already noted, P2P s percentage share of the overall traf c has been decreasing.
However, P2P is still the dominant protocol in the upstream.
Because P2P protocols are designed to exploit subcribers  sharing of data, this class of traf c has more symmetrical data  ows than Web/Multimedia protocols that are generally asymmetric [2].
The volume of P2P traf c has been quoted with various statistics in the media [4] and literature [19].
In many cases, the value quoted is the upstream volume.
Depending on the time and direction we could state a number between 17% to 58%.
In a DSL environment, upstream link capacity is dedicated per subscriber and shared backbone links have symmetric capacity.
Therefore, upstream traf c is not a problem and the traf c in the busiest direction in the busy hour is more of interest (i.e., the downstream direction).
However, for cable-based ISPs, the P2P upstream volume may be more of a factor as subscribers share the capacity of local cable lines.
Network News accounts for a surprising amount of the traf c.
This is due to NetNews being used to download large  les such as movies, music, and software.
We turn next to the categories of HTTP and Multimedia to get a better understanding of speci c protocols and applications that are used.
Table 2 shows the breakdown of the subcategories for HTTP traf- c (HTTP Web in Table 1).
When a  le is requested using the HTTP protocol, the HTTP header in the servers response includes a Content-Type  eld which contains the mime type of the  le.
We have based these subcategories primarily on the mime types ex-Class /http/video /http/audio /multimedia/rtmp /multimedia/rtsp /multimedia/rtp /multimedia/shoutcast /multimedia/ms-streaming /multimedia/other /multimedia/realaudio /multimedia/h323 % Busy Hour Traf c % Average Traf c



















 tracted from the HTTP stream.
For example, an image  le might have the mime type of image/gif and would be classi ed as /web/text-image.
In our categories, we group similar mime types together.
The /web/no-http subcategory is not based on mime type.
This category is for traf c on a standard HTTP port that does not use the HTTP protocol.
This unidenti ed traf c makes up 7.1% of the web traf c.
Though there is no traf c shaping to be evaded on the broadband network being studied, this could be the result of encrypted P2P applications using a default HTTP port to avoid  rewalls and traf c shaping on other ISP networks.
An interesting category is the /web/download.
This traf c is for the download of compressed (e.g., .zip, .rar., .tar.gz) or executable  les (e.g., .exe).
This shows that HTTP has replaced FTP as the distribution mechanism for these types of  les and software patches.
HTTP download traf c accounts for 10.2% of all traf c, whereas, the volume of FTP traf c is a quite small, 0.3%, as shown in Table 1.
Table 3 shows the breakdown of Multimedia traf c (HTTP Multimedia in Table 1) into subcategories.
HTTP is used to provide more than 80% of the multimedia data, substantially more than the 20% of multimedia traf c supported by traditional multimedia streaming protocols such as RTSP and RTMP.
Upon further investigation, we found that 85% of the /web/video is  ash video ( v) used by popular User Generated Content (UGC) sites like YouTube to deliver video content.
These UGC sites account for about 40% of total multimedia traf c.
Some of the possible reasons why HTTP is used to provide so much of the multimedia content on the Internet could be the result of no license fee costs for streaming servers (as required by traditional multimedia streaming protocols), compata-bility between operating systems, ability to easily traverse  rewalls, and ease of integration into CDN services.
The conclusions we draw from this section are that HTTP is the prevalent protocol on the Internet, accounting for 66% of the traf- c and is the main driver of per subscriber broadband traf c grow today.
It is very much the workhorse for data delivery and is very versatile.
HTTP has moved beyond its historical role in delivery of web text and image content and is increasingly being used to handle most of the Internet s tasks, such as distribution of software, updates, patches, and multimedia, and by P2P applications (gnutella, torrent trackers, torrent distribution).
An important consideration for the performance of a data distribution mechanisms is the distance the data travels to reach the broadband subscriber.
The distance traveled directly affects the ef- ciency of data delivery, and minimizing it is of interest to both the broadband subscriber and the ISP.
Average bit-distance traveled is strongly related to the direct network costs associated with transfer-e l i
 r i
 e v i t l a e






 On-Net All Traffic


 Content Provider 1 Content Provider 2 Content Provider 3 Web
 s e l i
 r i
 e v i t l a e












 Percentage of POPs s e l i
 r i
 e v i t l a e







 Highest POP by ASN Average POP by ASN Lowest POP by ASN




 % of ASNs/Prefixs Figure 3: Air Miles for Different Content Providers Figure 4: Average Air Miles Per POP Figure 5: Air Miles By ASN/Pre x ring the data.
We conducted this part of our study using NetFlow data from the backbone of a Tier-1 US ISP.
From the perspective of the broadband subscriber, increased travel distances affects the load time of web pages and  le downloads, and reduces throughput (e.g., TCP throughput is limited by the round trip time).
One method content providers use to enhance the quality of their data delivery is to outsource it to a CDN to place the data objects closer to the users, for fast, ef cient access.
From the perspective of the ISP, the network miles data travels re ects the direct cost of delivery of the data over their backbone, so shorter distances mean lower costs.
We can calculate the ef ciency and speed of data transfer by measuring the average bit-distance traveled assuming direct connectivity, which we call Air Miles.
We calculate the distance traveled as the direct physical distance between two end points.
Thus, Air Miles can be calculated as: AM = sum of distance each bit travels / total number of bits.
In order to remove the impact of interdo-main routing, we isolate the traf c exchanged between customers (On-Net traf c), following intradomain routing from the source to the destination.
We would like to note that the best metric to use would be layer 1 route miles; however, computation of this is dif cult and average bit-distance metric is a close approximate of route miles.
The data we used for our analysis is from September 27, 2008 until October 5, 2008.
Note that for this data we only used L4 application mappings to obtain application information.
Figure 3 shows ON-Net air miles of different anonymized content providers, CDNs, P2P, and Web.
The ALL traf c represents all the traf c including P2P and Web.
Overall, the distance traveled by P2P applications are typically 25% longer than the distance of HTTP traf c.
Currently, the CDN s air miles are 2 to 3 times lower than P2P and other content providers such as large web sites.
This indicates that CDNs are effective and are having a signi cant impact on the delivery of data.
There are several conclusions we can draw from this section.
The  rst is that the current generation of P2P applications are quite inef cient in air miles.
However, there have been some progress recently to address this issue with P2P.
For instance, the P4P Working Group has been working on P4P (Proactive network Provider Participation for P2P) to use topology information from ISPs to optimize P2P traf c on P2P networks.
Xie et al. show where broadband subscribers experience increased throughput using P2P applications due to signi cantly more data being served On-Net [8, 31].
The second conclusion is that CDNs are doing a good job of bringing data closer to the end user.
This allows the users to have a better multimedia experience because these large multimedia  les can be served from a CDN and obtain higher bitrates.
This allows more and better quality content to be consumed and show that large  les are typically served from CDNs.
Figure 4 shows the distribution of relative air miles travelled to POPs (Point of Presence) in the network.
Figure 5 shows for the POPs with the highest, lowest, and an average of all POPs the distribution of air miles to different ASNs.
The main observation to take away from graphs is that on the Broadband Providers network there are many opportunities for optimizations.
The traf c going to some POPs are signi cantly more expensive, on average, to transport than others.
In addition, at each POP there are some ASN that are signi cantly more expensive, on average, to transport as well.
These observations help to motivate our proposed caching solution later presented in Section 6.
Many applications on the Internet have data  ows that could potentially be reused to save network bandwidth because the same content is being requested more than once.
For instance, multiple requests to the same web page could be cached and either served from a local or network cache.
Another example is multiple users streaming the same video  le, which could be served using multi-cast or P2P.
The application classes of Web, Multimedia, P2P, and Network News are the most likely candidates for transferring content that is reusable.
These traf c classes were shown in Section 3.1 to represent 89% of the network traf c during the busy hour.
Other traf c classes such as VoIP, Chat and Business are less unlikely to contain any reusable content in their data  ows but represent only 11% of the network traf c.
The analysis in this section is based off of our HTTP traf c trace.
By using these HTTP records we can look at how much traf c is reusable for Web and Multimedia because 95% application classes are served using the HTTP protocol.
The Time-To-Live (TTL) length speci ed in Control-Cache  eld of the HTTP records indicates how long an object should be kept in a cache before it needs to be refreshed.
Figure 6 shows the CDF of the TTL length speci ed by different  le sizes.
We  nd noticeable amounts of records with TTLs set as: 1 hour, 1 day, and 7 days.
We also looked at the impact CDNs have on the temporal characteristics of the traf c.
To identify which requests are from a CDN we employ a similar methodology as Huang et al. [17].
To facilitate this analysis we used dumps of the DNS tables used by the subscribers at the broadband ISP.
These dumps were collected on February 2nd, 2008.
All Requests CDN Requests Non-CDN Requests













 All Bytes CDN Bytes Non-CDN Bytes
















 1000 10000 100000 1e+06 1e+07 TTL (minutes)


 1000 10000 100000 1e+06 1e+07 TTL (minutes)


 1000 10000 100000 1e+06 1e+07 TTL (minutes) Figure 6: TTL of Cache Directives for HTTP Requests Figure 7: TTL of Cache Directives for Requests Figure 8: TTL of Cache Directives for Weighted by Bytes Greedy Only No-Cache Directives TTL Directives and No-Cache Directives e h c a
 m o r f d e v r e
 s e t y
 %










 Request Bytes % o i t a
 t i
 e h c a












 Number of Requests (Millions)



 Cache Size (GB) Figure 9: Webcache Results Figure 10: Caching Results using Different Cache Sizes Table 4: Percentage of Bytes for Large Flow vs. Small Flows of CDNs and Other



 Overall






 Class
 Other Total









 We use a twofold approach to identify CDN requests.
The  rst step in our analysis is to look up the DNS entry for each hostname in the HTTP requests.
If the hostname resolves to a CNAME that belongs to a CDN provider then we label these requests as CDN traf c.
In the second step, for the remaining traf c we use the server IP address to resolve the AS number.
If the AS number is from a known CDN provider then we label these requests as CDN traf c as well.
Figure 7 and Figure 8 show the CDFs of the TTL lengths when weighted by requests and total bytes, respectively.
When comparing both graphs together we observed that while the distribution for requests and bytes are similar for small TTLs.
The TTLs for bytes is smaller indicating that large  le sizes have larger TTLs on average than smaller  le sizes.
A noticeable artifact in the graph is that for the byte distribution of CDN requests over 50% of the bytes have a TTL of 10 days.
However, overall we found that there were no other substantial differences in how the TTLs were set between CDN and non-CDN traf c that would indicate a difference in the cachability of the CDN content.
Table 4 shows the percentage of all bytes in the downstream traf- c broken down by source.
We found that a signi cant amount of the large  les are being served by CDNs.
In particular, over 46% of  ows greater than 100 Mbytes in size are originating from a CDN today.
Not all HTTP requests are cacheable.
This occurs for a variety of reasons the web page may contain private information like a cookie, or is dynamically generated.
In the HTTP 1.1 protocol there are two  elds, Cache-Control and Pragma, in the HTTP header that can be used to indicate cacheability [14].
The Content-Control  eld can be used by a server to indicate if the document is not cacheable or the time the document can be kept before it is stale.
The Pragma  eld can be used by the client to indicate a request for a fresh copy of the  le.
For instance, hitting the refresh button on most browsers causes the HTTP request to be marked as no-cache when sent.
We used both of these  elds in the HTTP records we analyzed.
If we remove from the HTTP traf c the 42% that is marked non-cacheable, in total, approximately 60% of all traf c is potentially reusable.
We de ne content as reusable if there is more than one request for a speci c object during the period the object is valid (e.g.
not stale or modi ed).
The results in Figure 9 assume that the cache has an unlimited size.
Thus, we do not assume removal of items from the cache.
These assumptions are more general in some cases and provide a potentially more optimistic result than if explicitly studying caching.
Our methodology of identifying uncacheable documents and using an unlimited cache size is similar to the approach taken by Feldmann et al. who completed a comprehensive studies of HTTP headers in relation to caching in 1999 [13].
We have tested three different algorithms for calculating reusability.
The  rst, which we denote as  Greedy , is where we ignore the Content-control and Pragma directives including TTLs.
In our second algorithm, denoted as  No-Cache , we only respect the directives that indicate the content should not be cached or not served from cache.
The third algorithm, denoted as  TTL-Cache , takes into account all directives and also the TTL values assigned to each request.
We have chosen these three algorithms to take into account different hypothetical scenarios.
The Greedy algorithm provides us with an upper bound for the potentially cacheable content, and the TTL-Cache algorithm the lower bound if all the optional parameters are followed explicitly.
However, in reality, caches op-jects after they become stale.
Figure 9 shows the caches byte hit ratio as we process our data set over time.
We found with our Greedy algorithm that 92.2% of requests and 67.9% of bytes could have been served from cache.
With the No-Cache algorithm we found 70.1% of requests and
 the TTL-Cache algorithm 62.0% of the requests and 32.5% of the bytes would be served from cache.
Notice that when taking into account the TTL of the objects that there is only a small 5% change in the bytes served from cache.
We also found that the total cache size using the Greedy and No-Cache algorithms was 17.4TB and 11.3TB, respectively.
These cache sizes may be economically infeasible to deploy in all scenarios.
However, in today s environment where disk space is relatively cheap and large ISPs have regional data centers (POPs) that serve upwards of 150,000 to 1,000,000 subscribers, the deployment of a
 Our results allow us to estimate that 30% of Web and Multimedia content is reusable and that 70% of the traf c during the busy hour is web and multimedia traf c, hence about 24% of network traf c could be optimized to take advantage of the fact it is being requested more than once.
(We have calcuated the bandwidth savings soley on caching Web and Multimedia traf c, however, a P2P-based cache could also deployed.)
In addition to simulating an unlimited cache size we implemented a caching program to test the TTL-Cache algorithm with various cache sizes.
For our simulation, we used Least-Recently-Used (LRU) as our caching policy.
The choice of LRU was made because most caching products utilize LRU.
In industry this is because it is simple, understandable, and works just as well as any other algorithm when you have enough disk space.
Figure 10 shows the results of our simluations using cache sizes varying from 1GB to 1.5 TB in size.
The parallel line in the graph shows the maximum amount cacheable bytes (32.5% calcualated previously) if we had an unlimited cache size.
Forward caching has been proposed in the 90s to address the issues of improved client performance and reduced network cost which we highlighted in the previous sections.
A forward cache is an HTTP cache deployed within an Internet Service Provider s (ISP) network caching all cacheable HTTP traf c accessed by the customers of the ISP.
In contrast to CDNs a forward cache is deployed for the customers bene t and under the control of the ISP, rather than for the bene t of the content owner.
As forward caches are quite frequently collocated with cooperate  rewalls caching HTTP traf c of the employees of the cooperation, most large US based ISPs currently do not operate forward caches within there network.
The main reason for this decision lies in the economics of deploying forward caches.
Forward caches are additional hardware components (typically UNIX servers) which have to be purchased, deployed and managed at a large number of locations.
In the US the bandwidth savings can often not justify the cost of such a server deployment.
To reduce the costs associated with forward caching in an ISP we propose Network-Aware Forward Caching which is motivated by the insights presented in the previous section.
Our goal is to  nd the set of addresses at each POP that when cached maximizes the cost savings for the network.
In particular we noticed that some traf c is already originating close to the ISPs customers (e.g.
CDN traf c) and, therefore, the bene ts of caching the content again using a forward cache in the ISPs POP is minimal both in terms of performance and cost savings.
On the other hand some traf c traverse expensive transit or backbone links and caching would be both cost effective and improve performance.
In a second dimension we also noticed that POPs themselves have a high variability in terms of distance to HTTP sources as well as peering links.
For example, a remote POP might be far away from a CDN server, whereas a metropolitan POP might be very close to a CDN server.
Considering these insights it becomes clear that the most cost effective way of deploying forward caches is to only deploy them in selected POPs that are caching only selected expensive-to-deliver content that is requested frequently.
This expensive content can be identi ed using a metric like air miles as we have done.
We call this approach Network-Aware Forward Caching.
In the remainder of this section we will formally state the problem of how to place caches and decide what content to cache, propose a solution and evaluate our approach in a case study using data from a large tier one US ISP.
We  rst de ne the notations used for the formulation of forward caching problem.
The network has a set of POPs P = {1, 2, 3, .
.
.}.
The distance (air mileage) between POPs are de ned as L = (li,j), where i, j   P .
The HTTP traf c are downloaded from a set of IP address sets S = {0, 1, 2, .
.
.}.
For example, an address set can be an address pre x (i.e., 100.200.0.0/24), or the collection of addresses that belong to the same organization or autonomous system (AS).
De ne V = (vi,j,s) as the monthly HTTP traf c volume from address set s that enter the network at ingress POP j and leaves the network at egress POP i.
The monthly transit cost per unit volume for address set s is de ned T = (ts), where ts > 0 for provider traf c, ts < 0 for customer traf c, ts = 0 for peer traf c.
Assume we have a total budget of N dollars to purchase and deploy caches.
Each cache costs   dollars, has a disk space of b, and can handle a traf c throughput of e Mbps.
We de ne a boolean variable to denote whether to cache s at POP i: C = (ci,s), where ci,s = 1 if yes, ci,s = 0 if not.
We further de ne U = (ui,j,s) as the monthly HTTP traf c volume from s with ingress POP j and egress POP i which cannot be possibly retrieved even from a cache at s with in nite computational power and disk space.
We de ne the disk space needed for caching address set s at POP i as X = (xi,s).
Note that X is different from U in that an object with size x might have to downloaded twice due to TTL expiration, but just needs x to store.
Cost

 We de ne the backbone cost unit as   dollars per mile-byte, and transit cost unit as   dollars per byte.
We can then compute the backbone cost, transit cost (T C), and upfront caching cost (CC), when caches are deployed.
HTTP traf c vi,j,s s contribution to backbone cost is     li,j   ui,j,s when the objects in s are cached at i (i.e.,ci,s = 1), and     li,j   vi,j,s when the objects in s are not cached at s(i.e.,ci,s = 0).
Thus the total backbone cost BC =     P  i P,j P,s S li,j   ((1   ci,s)   vi,j,s + ci,s   ui,j,s).
 
 Similarly, transit  i P,j P,s S ts   ((1   ci,s)   vi,j,s + ci,s   ui,j,s).
The total thus the number of cache units at POP i required by the computational power is (cid:3)P the number of cache units at POP i required by disk space is (cid:3)P  s S ci,s   xi,s/b)(cid:4).
The upfront caching cost at POP i is the maximum of that required by computational power and that re s S,j P ci,s   vi,j,s/e(cid:4).
 s S,j P ci,s   vi,j,s, traf c volume at POP i is =   Similarly,
  s S,j P ci,s   vi,j,s/e, quired by disk space.
Thus the total upfront caching cost CC =    
  s S ci,s   xi,s/b)(cid:4).
 i P (cid:3)max( the problem is to  nd ci,s such that minimize: BC + T C + CC subject to: CC   N
 After refactoring, the object function becomes:  i P,j P,s S vi,j,s   (    li,j +     ts)   minimize:
  i P,s S ci,s   P j P (vi,j,s   ui,j,s)   (    li,j +     ts)       (

  s S ci,s   xi,s/b)(cid:4))  s S,j P ci,s   vi,j,s/e,  i P (cid:3)max(
 j P (vi,j,s   ui,j,s)   (    li,j +     ts), which is let Bi,s = the bene t of caching s at i, excluding the upfront cost.
The objective function becomes:

 maximize:     X (cid:3)max(  i P,s S ci,s   Bi,s 
  i P  s S,j P ci,s   vi,j,s/e,
  s S ci,s   xi,s/b)(cid:4) (1)
 subject to : (cid:6)N/ (cid:7) = N(cid:4)  i P (cid:3)max(
  s S,j P ci,s   vi,j,s/e,
  s S ci,s   xi,s/b)(cid:4)  
 It is easy to see that our  nal Formulation 1 of the problem for the case that we have only one POP, i.e., |P| = 1, is as hard as the knapsack problem.
In the knapsack problem, given a set of n different items, each with a weight and a value (bene t), our goal is to determine the set of items to include so that the total weight is less than a given limit W and the total value is as large as possible.
It is well-known that the knapsack problem is NP-hard though it can be solved in pseudo-polynomial time1 using dynamic programming.
In addition, the problem has a polynomial-time 1   -approximation algorithm (an algorithm whose output has a value at least 1    times the optimum solution) based on dynamic programming, for arbitrary small constant  > 0.
First, let us observe that our problem formulation also has a pseudo-polynomial-time dynamic programming algorithm.
Consider any POP i.
For a content s, we denote its needed computational power by Cs = j P vi,j,s and its needed disk space by Ms = xi,s.
Now we  ll in a table T [s, C, M ] which determines the maximum bene t that we can obtain from contents 1, 2, .
.
.
, s with at most C total computational power and M total disk space, where (cid:3)C/e(cid:4) ((cid:3)M/b(cid:4)) is at most the maximum number of cache units that we can afford in our total budget, i.e., N(cid:4)
 for all feasible values of C and M. For s > 0, T [s, C, M ] = max{T [s   1, C, M ], T [s   1, C   Cs, M   Ms] + Bi,s}, for C   Cs and M   Ms, and   otherwise.
Next, de ne T (cid:4)i[U ] to be T [|S|, e  U, b  U ] which is the maximum bene t that we can obtain by caching of contents in POP i with at most 0   U   N(cid:4) (as the maximum of computational power or disk space) units of caches.
Finally having T (cid:4)i[U ]s, we compute our  nal table T (cid:4)(cid:4)[i, U ] which is the maximum bene t that we can obtain from POPs 1, 2, .
.
.
, i with at most 0   U   N(cid:4) units of caches.
T (cid:4)(cid:4)[0, U ] = 0 for all affordable values of 0   U   N(cid:4) , and for i > 0, T (cid:4)(cid:4)[i, U ] = max0 j U{T (cid:4)(cid:4)[i 1, U j]+T (cid:4)i[j]}.
Therefore, the maximum of our objective in Formulation 1 is max1 U N(cid:2){T (cid:4)(cid:4)[|P|, U ]   U}.
By using standard techniques analogous to those for the knap-it is not hard to transform the above pseudo-sack problem, polynomial-time dynamic programming to a polynomial-time 1 -approximation algorithm, for arbitrary small constant  > 0.
pseudo-polynomial time if its running time is polynomial in the numeric value of the input (which is exponential in the length of the input   its number of digits).
) Bis i s
 ( t i f e n e
 l t a o
 f o %





 1e-05


 % of i,s for ASNs

 Figure 11: CCDF of Total Bene t at Each i,s Pair
 It is worth mentioning that in practice with large values, dynamic programming approaches similar to the aforementioned one in this section are time-consuming and not desirable.
Due to this reason we consider a well-known greedy heuristic for the knapsack problem which sorts the items in decreasing order of value per unit of weight and then proceeds to insert them into the knapsack until there is no longer space in the knapsack for more.
This heuristic for the knapsack problem is not only very fast and easy to implement but also gives a guaranteed approximation factor 2 for some versions of knapsack.
Below we generalize this greedy algorithm for our purpose and report its evaluation results in the next section.
Our greedy heuristics is based on the idea that the total number of caches n is within the range of [0, N(cid:4)].
Therefore, we can  guess" and enumerate n. Thus the problem becomes:  i P,s S ci,s   Bi,s       n maximize: subject to : n   N(cid:4) By enumerating over all n,     n is just a  xed cost that can be ignored for the maximization purposes.
As we discussed in the previous section, Bi,s is the bene t of caching s at i.
On the other hand, the weight of content s to be cached on POP i is wi,s =   max(  j P vi,j,s/e, xi,s/b).2 Now, for a  xed number n of caches, we have essentially a knapsack problem that we want to maximize the bene t of selected elements (i.e., the cache assignment) while our total weight is restricted by the number n of caches.
Thus inspired by the aforemen-tion greedy algorithm for the knapsack problem, for a  xed n, our algorithm is to choose the most cost-ef cient (i, s) pair (i.e., cache s at i)  rst.
A formal description of our algorithm is as follows:



 ci,s = 0 for all i and s //clear all ci,s for (i, s) pairs ranked by Bi,s/wi,s descendingly do ci,s = 1 as long as the total number of used caches so far is not more than n;
 do end for
 6: end for
 the corresponding C = (ci,s);
 cache content s on POP i alone.
This weight might be smaller if we cache other contents on POP i as well.
However in some sense we  over-provision" the caches, which is often needed in practice, since we do not want to utilize the caches 100%.
i f e n e
 d e z i l a m r o









 -0.5 1 t i f e n e
 d e z i l a m r o









 -0.5 1 t i f e n e
 d e z i l a m r o
 ASN with 0K Server Cost ASN with 10K Server Cost ASN with 20K Server Cost ASN with 40K Server Cost ASN with 60K Server Cost














 % of Deployment % of Deployment % of Deployment Figure 12: Bene ts for Different Transit Costs Figure 13: Bene ts for Different Backbone Costs Figure 14: Bene ts for Different Server Costs ( xed $4 Transit, $4 Backbone, 400 Mbps, 4TB)


 h c a e n i d e h c a
 h d w d n a
 t i f o % Max Net Benefit








 % of POP Figure 15: Coverage of Caches at each Pop at Maximum Net Bene t
 We implemented a simulation program to evaluate our network-aware caching approach using the heuristic we described in Section 6.2.
To run the simulation we assume that transit ( ) and backbone ( ) costs are approximately $4 Mbps/month [30] and the upfront cost of a caching server is $20,000 which can be amortized over 36 month and costs approximately $555 a month to run.
We assume that this $20K server will be able to handle 400 Mbps of throughput (e) and as have a disk size of 4 TB (b).
We then vary these parameters to study the sensitivity of the results.
In the simulation we base our network speci c inputs P , S, Vi,j,s, and Li,j from our Core Backbone Traf c data set we previously analyzed in Section 4.
In our analysis, we have chosen to use AS numbers in S. However, our approach can use other levels of detail such as pre xes if more granular measurements are available.
To estimate the values of Ui,j,s and Xi,s we used our analysis from Section 5 that calculated the values Ui,s and Xi,s for a single BRAS in this network.
In the simulation we use the U and X values from this BRAS as estimates that are scaled appropriately for all other POPs.
Our simulation program calculates the net bene t (Equation 1) as the number of cache servers (n) increases.
We  nd the best deployment solution by selecting n which maximizes the net bene t.
In our program, the net bene t is calculated in dollars saved.
However, in our  gures we plot the relative net bene t.
This is done to preserve the anonymity of our data source.
Figure 11 shows the CCDF distribution of Bis.
This shows that most of the bene t can be obtained from caching a small subset of the i, s pairs.
Overall, we  nd in Figure 12 and Figure 13 that using our stated assumptions for backbone, transit and cache costs and the US Broadband Provider s network data that the maximum bene- ts would occur when only 68% of the caches are deployed that would have other otherwise been needed to cover all HTTP traf- c.
Our simulation results show that the relative net positive ben-e t increased from 0.501 to 0.688 (a 37% overall improvement in bene ts) when the network-aware caching approach s strategy was compared to the bene ts of deploying caches to cover all POPs.
Figure 15 shows the distribution of cache servers to each POP based on our maximal solution.
This shows that in the our optimum solution 25% of POPs do not any postive net bene t by having a cache server place there.
Only 15% of POPs have a maximumnet net bene t by having all HTTP traf c covered.
Figures 12, 13, and 14 show selected results of varying each of these factors by a couple magnitudes to see their overall affect on the  nal cost bene t analysis.
Figure 12 shows that transit costs in our simulated network minimally affect the overall maximum net bene t.
This is due in our case to the amount of transit traf c that is cacheable is quite low.
However, in other network this may not be the case and therefore have a large impact in the results.
Figure 13 shows that the backbone cost is a large factor in uenc-ing how much the net bene t is.
As the backbone cost increases the cost bene t curve is shifted upwards and maximum net bene t point shifts right.
Figure 14 shows the net bene t cost as the cost of each cache changes but all other parameters remain  xed.
When the server cost is set to $0 per server the theoretical maximum bene t obtainable is depicted.
As the server cost is increases the net bene t decreases and the optimum number of caches to deploy shifts to the left.
This follow intuition that as the cache cost increases that less traf c would have a positive net bene t to cache.
Forward caches which are also known as proxy caches or forward proxy caches have received a great deal of attention during the dot com boom.
As there exists a large volume of prior art on various aspects of forward proxies we refer the interested reader to [24] for a broader discussion of forward caching and focus the related work discussion in this section more narrowly on the problem of cache placement and selective content caching.
First there are multiple proposals of how to optimize the cache placement within a CDN.
For example [3, 21, 27, 28] explores this problem.
Our goal differs in that we are interested in how to place caches which cache only partial HTTP traf c to optimize the cost within an ISPs network and not how to optimize the placement of caches within a CDN.
Another paper optimizing placement of proxy servers in a CDN is [23].
This work does introduce a greedy algorithm similar in spirit to our approach.
However, it does not address the problem of deciding which content to cache on which proxy while solving the proxy placement problem in an access ISP.
It rather focuses on the optimal placement of proxies to server content from a given Web server through a CDN.
A second line of work looks at proxy placement under various topologies [3, 26] as well as proxy placement over multiple ASes [18].
Again neither of this work considers the problem of selective caching and cache placement within an access ISP to reduce deployment cost.
As CDNs cache only content for which they are paid.
We are unaware of any work which explicitly restricts which content to cache in a forward cache deployment to minimize the deployment cost of the cache infrastructure.
In addition to this new aspect of restricting cachability on forward proxies we also provide a detailed case study of our approach using real network traces and network topology of a US broadband ISP.
This paper proposed and evaluated a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network.
We  nd that this is an NP-hard problem.
A key advantage of our approach is that we can reduce the costs associated with forward caching to maximize the bene t obtained from their deployment.
In our case study, we show in our analysis that a 37% increase to net bene ts could be achived over the standard method of a deploying caches to all POPs and caching all traf c.
This maximal point occurs when only 68% of the total traf c is cached.
At this point, we  nd that 25% of POPs should not have a cache and that only 15% of POP% should have all traf c cached.
Another contribution of this paper is the analysis we use to motivate and evaluate this problem.
We characterize the Internet traf c of 100K subscribers of a US residential broadband provider.
We us both layer 4 and layer 7 analysis to investigate the traf c volumes of the  ows as well study the general characteristics of the applications used.
We show that HTTP is a dominant protocol and account for 68% of the total downstream traf c.
In addition, we show that multimedia content using HTTP exhibits a 83% annualized growth rate and other HTTP traf c has a 53% growth rate versus the 26% over all annual growth rate of broadband traf c.
This shows that HTTP traf c will become ever more dominent and increase the potential caching opportunities.
Furthermore, we characterize the core backbone traf c of this broadband provider to measure the ef- ciency content and traf c is delivered.
We  nd that CDN traf c is much more ef cient than P2P content and that there is large skew in the Air Miles between POP in a typical network and shows many opportunties in broadband provider networks to optimize how traf- c is delivered and cached.
Several opportunities exists for future work.
In this paper, we have focused on the fastest growing and biggest component to broadband ISP traf c.
However, additional work could be done in to determine strategies for caching or increasing the delivery ef- ciency of other applications such as P2P and multicast.
