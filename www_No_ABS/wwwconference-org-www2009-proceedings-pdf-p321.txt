In recent years, the global di usion of the Internet and the progress in developing Web multimedia applications are enabling the delivering of dynamic heterogeneous content such as news, blogs and audio/video podcasts.
This content is commonly published through RSS feeds.
Users can manage RSS feeds using a feed reader that periodically downloads the updated content from the subscribed feeds, displays the items in each feed and provides links to the related resources.
However, the basic functionalities of these readers return the unorganised list of all the items included in the subscribed  and University of Turin, Department of Computer Science, Turin, messina@di.unito.it Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
feeds, causing an information overload e ect.
Hence, e ec-tive solutions for intelligent information fusion and organisa-tion are becoming indispensable.
Here, the challenge lies in the ability of combining and presenting heterogeneous data coming from multiple information sources, i.e. cross-modal, and consisting of multiple types of content, i.e. multimedia.
This paper, a substantial extension of the original idea presented in [13], describes a framework for content-based, cross-media information aggregation, and its application to the real case of multimedia news retrieval.
The paper is organised as follows.
Section 2 reviews related work.
Section 3 introduces a model of the domain.
Based on this background, Section 4 describes our cross-modal clustering algorithm.
Section 5 provides a description of our prototype architecture, and Section 6 details the core technologies used in its development.
Section 7 presents the performance of the system.
Finally, Section 8 provides conclusive remarks and future plans regarding the presented research.
Information mashup is becoming a hot topic in the WWW community.
A mashup is a Web application that aggregates content from di erent data sources to deliver a new, hybrid service that was not originally supported.
Recently, many tools have been released for this purpose, such as Google Mashup1, Yahoo!
Pipes2 and Microsoft Pop y3.
Much of the current work involves grouping data from only a single domain and from only a single media, such as RSS items aggregated according to a taxonomy of concepts [12,
 of the referenced items  content, the aggregation process may not be a trivial task.
The works in [1, 6] employ either the user s interaction, or external knowledge sources, to improve the aggregation performance.
Nonetheless, the nature of the data to be aggregated cannot be in principle shrinked to be merely mono-media.
Therefore, tools to integrate multimedia data from mono-modal information sources were investigated.
A method for querying persons in Yahoo!
News images using the enclosed news captions is presented in [7].
In [9], a speaker recognition 1http://editor.googlemashups.com/ 2http://pipes.yahoo.com/ 3http://www.popfly.com/ sented.
In [18], a multimedia integration system to deliver personalised tourist Web services is proposed.
More challenging approaches are those employing both cross-modal information channels, like radio, TV and the Internet and multimedia data such as audio, video and text [19, 20].
More speci cally, these approaches aims at enriching traditional TV broadcasts with semantic metadata derived from nontraditional information sources as the Internet.
Similarly to our work, the authors in [3, 10, 15] address the problem of  nding news articles on the Web relevant to the ongoing stream of TV broadcast news.
Additional works particularly relevant to our application domain are those pursued under the NIST Topic Detection and Tracking (TDT) project.4 TDT aims at automatically locating, linking and accessing topically related information items within heterogeneous, real-time news streams.
Intuitively, a topic is de ned as an aggregation of information items that are semantically relevant to a real-world event.
As an example, a earthquake could be the event that triggered the topic.
Any information item, such as a newscast story or a newspaper article, that talks about the earthquake, or e.g.
the rescue attempts, the number of casualties, and so on, is semantically relevant to the topic.
The identi ed tasks of TDT are News Story Segmentation (NSS), First Story Detection (FSD), Topic Detection (TD), Link Detection (LD), and Topic Tracking (TT).
The news story segmentation task concerns the ability of automatically detecting semantically coherent parts of the news streams, such as a single news story.
The TRECVID5 initiative had news segmentation among its tasks in 2003 and 2004.
The common base of the approaches for automatically segmenting TV newscasts into individual news stories consists in using a combination of visual, audio and speech features.
Systems performance evaluation in the TRECVID news story segmentation task is presented in [2].
We perform the news story segmentation process by exploiting aural and visual cues, with the help of a three-layered heuristic framework inspired by the editorial rules used by newscasts producers, as it will be explained in Section 6.1.
The  rst story detection task is aimed at recognising information items linked to topics never seen before.
FSD is typically approached by representing each information item as a set of features (e.g., newswire text or closed transcriptions of radio and TV speech).
When an incoming item is received, its feature set is matched against those of all the past items according to a similarity measure.
If, for each past item, the similarity measure is above a  xed threshold, then the incoming item is marked as new.
Following the same approach, the topic and link detection tasks aim at aggregating and linking individual information items related to the same topic.
The last task aims at keeping track of information items similar to a set of example items.
Despite the great number of research activities, the current state of the art techniques for the development of a complete multimodal (i.e., cross-modal and multimedia) framework are still far from satisfying expectation.
Our work 4http://www.nist.gov/speech/tests/tdt/ 5www-nlpir.nist.gov/projects/trecvid/ innovates in this direction, providing a methodology able to exploit the potentialities coming from the integration of heterogeneous information sources and media modalities.
In particular, we adopt online newspaper articles and TV newscasts as information sources, to deliver multimodal search and retrieval services, integrating items coming from both contributions.
Closely related to our work are those presented in [3, 10,
 as they only provide one-way, one-to-many relationships, i.e.
from single TV news stories to multiple Web pages.
Instead, in our approach bidirectional, many-to-many relationships between TV and the Web are provided, thus augmenting the  exibility and versatility of the proposed framework.
Additional relevant works are those aimed at topic detection and tracking.
The fundamental problem of almost the current TDT approaches lies in the de nition of the similarity measure that is used to evaluate the distance between items.
If the feature sets extracted from two information items are homogeneous from the data representation and data semantics perspectives, it is possible to de ne a similarity measure among them.
Consequently, any clustering algorithm based on that similarity measure can be applied to discover aggregations of information items.
Unfortunately, in many practical cases it happens that information items are not homogeneous, e.g.
when text documents and multimedia objects have to be aggregated.
While in this case it is possible to de ne similarity measures in each of the two spaces, it is di cult to establish a similarity measure in the hybrid space constituted by the union of the two spaces.
A solution to this problem is provided by cross-modal (or hybrid) clustering [4].
Moreover, existing clustering methods typically output groups of items with no intrinsic structure.
This inhibits the possibility of de ning representative elements other than simple cluster centroids, as well as the ability of discovering deeper relations among grouped items like equivalence and entailment.
In fact, the existing methods mostly link information items symmetrically, e.g.
using the cosine similarity as distance metric [16].
This means that two linked items relate each other with the same strength.
However, in most cases two related items should be assigned di erent strength to link each other.
Our framework uses a cross-modal clustering algorithm whose kernel is an asymmetric relevance function between information items.
The function asymmetry guarantees that di erent strength of relations can be discovered among information items.
Summing up, the innovation of our work can be presented in two aspect.
First, we provide a general method for the aggregation of information streams based on the concept of semantic relevance and on a novel asymmetric aggregation function.
Then, we present a fully unsupervised framework that implements all the functionalities provided by the general method.
User experiments show the applicability and e ectiveness of our solution in a real-world business context.
This section presents a model of the information  ow in the news publishing process.
A typical news production and distribution cycle is shown in Figure 1.
The cycle starts with the news event e, any relevant fact happened at some time and place, along with all the information elements generated during its occurrence.
For example, the voting for a law or the presentation of a pe-i.e.
The occurrence of a real-world event e generates many information items Iij.
Information providers pij, e.g.
press agencies and broadcasters, merge and deliver these information items to the  nal consumers c through multiple distribution channels, e.g the Internet, radio and TV, and presentation platforms as speech, video or text.
tition could be the information elements generated during a parliamentary session.
The information elements are collected by a pool of primary providers, e.g.
the reporters of a press agency, P(cid:48) = {p1j},  j = 1, .
.
.
, m1, who in turn package them into information items I1j that represent coherent,  nite and consumable information units, e.g.
the press agency journalistic reports.
These information items can be afterwards caught by the secondary providers P(cid:48)(cid:48) = {p2j},  j = 1, .
.
.
, m2, who can unpack them and use part of the contained elements to build new information items, optionally enriched by additional information derived from some contextual events {eck}.
The process can be iterated until some editorial criteria is satis ed.
For example, a news agency can pick up di erent journalistic reports and produce an article to be published on a newspaper.
As a result, original information elements reach the  nal consumer c through a variety of delivery paths, e.g.
Web news, print and broadcast media, and packaged in distinct information items, e.g.
RSS feeds, newspapers articles or newscasts.
The task of an aggregation information system is detecting and reconstructing a surrogate of the original event e and of its most relevant contextual events, by combining the informative contributions coming from a subset of the information items sets in scope of the consumer c, i.e. a subset of {{I}N 1,{I}N 2, .
.
.
,{I}NmN}.
Since the nature of all the intermediate information providers and of the information elements cannot be in principle shrinked to be merely textual, we assume that information items may be multimedia, i.e. they can be presented in text, speech and visual formats.
Figure 2 illustrates schematically the role of a component, denoted with  , performing an aggregation process between two sources.
This can be viewed as an intermediate process that merges contributions {I}N 1 and {I}N 2, delivered respectively by the providers pN 1 and pN 2, to generate a Figure 2: Illustration of multimodal aggregation.
Heterogeneous information streams {I}N 1, {I}N 2 sharing common semantics are aggregated and presented to the consumers in an integrated form.
contribution {I}(N +1)1 delivered to the consumers c through the provider p(N +1)1.
Multimodal aggregation is performed by cross-modal clustering based on the concept of semantic relevance, which is inspired by the de nition originally proposed in [11].
De nition 1.
(Semantic relevance).
Let   and   be two information items reached to the consumers through information streams {I}N 1 and {I}N 2, respectively.
In this context, the secondary information item   is semantically relevant to the primary information item   if the fruition of   by consumers satis es the consumers expectations about  .
Semantic relevance is modelled by the linking function R( ) that measures how likely the secondary information items are relevant to the information needs expressed by the primary information items.
Cross-modal clustering is able to discover these semantic relations in heterogeneous data, thus providing facilities to e ectively retrieve desired information in cross-modal, multimedia information streams.
The algorithm is detailed in the following subsections.
Let   = { i}m i=1 and B = { j}n j=1 be two sets of information items, for which a distance metric in the space H =  B is not de ned.
Let R :  B   [0, 1] be a linking function such that:   R( i,  j)   1 (tends to 1) if  j   B is semantically relevant to  i    ;   R( i,  j)   0 (tends to 0) if  j   B is not semantically relevant to  i    .
Figure 3 shows an example in the context of Web and TV news.
Many online news articles often deal with the same fact.
In addition, these textual resources can be linked to a series of TV news reports, yet giving di erent updates or mantically relevant primary information items (e.g., Web assets  i) and secondary information items (e.g., TV assets  i) are merged in a new hybrid space H, thus generating a multimodal aggregation.
viewpoints on the same matter over time.
Consumers should be made able to exploit these semantic links and to track sequels of linked stories in a new, hybrid space, including both the Web and TV contributions.
We de ne the a nity matrix as A = (r1, .
.
.
, rm)T   [0, 1]m,n, where rk = (R( k,  1), .
.
.
, R( k,  n)), k = 1, .
.
.
, m (1) Intuitively, the construction of A can be seen as a space transformation process, which links the information items from the primary space   to the secondary space B, according to the semantic relevance between objects in such spaces.
Once the a nity matrix has been constructed, the similarity between primary information items  i, i = 1, .
.
.
, m is evaluated by exploiting their projection in the space B.
Let ( a,  b)     be a couple of primary information items represented by the a nity vectors (ra, rb), where ra, rb are the corresponding rows in matrix A.
The similarity between  a and  b in the secondary space B is de ned as follows: (cid:104)ra, rb(cid:105) (cid:107)ra(cid:107)2 , S( a,  b) = (2) where S( ) is the a nity vector similarity function and (cid:107)   (cid:107) is the norm induced by the inner product in [0, 1]n.
Intuitively, the function S( ) de ned in (2) measures how much the information item  a is explained by the information item  b, in the space of their a nity vectors.
The function S( ) has the following properties: S( a,  b) = cos(ra, rb) (cid:107)rb(cid:107) (cid:107)ra(cid:107) i  S( a,  b) >     S( b,  a) >   then Eq( a,  b) i  S( a,  b) >     S( b,  a)     then Ent( a,  b) (5) where cos( ) is the cosine similarity de ned in B, and   is a  xed threshold such that     [0, 1].
Equation (4) introduces the semantic equivalence relation between  a and  b, Eq( a,  b), while Equation (5) introduces the semantic entailment relation from  a to  b, Ent( a,  b).
Notice that (3) (4)   1, if a = b Figure 4: Example of the equivalence matrix between primary information items { i}6 i=1 and the corresponding connectivity graph for three values of  .
the latter relationship would not be discovered by using the plain cosine similarity measure.
Intuitively, the asymmetry given by Equation 2, introduces the possibility to have hierarchies among the aggregated objects, providing also means for a natural procedure to discover representative elements and to have a multilevel granularity of presented information.
The disadvantage w.r.t.
symmetric measurements is the introduction of extra computation.
The a nity vector similarity function is computed for each couple of a nity vectors (ra, rb), a, b = 1, .
.
.
, m. The result is the equivalence matrix E = (eab)   (cid:60)m,m, where: eab = S( a,  b), if a (cid:54)= b and S( a,  b)     0, otherwise.
(6)
 Induced Partitions Once E is calculated, the primary connectivity graph G = (V, E) is built.
Each node of the graph corresponds to a primary information item  i.
Two nodes va, vb are connected from va to vb if the corresponding element eab   E is greater than  .
The  cut value guarantees that every pair of linked information items has a semantic relevance of at least  .
Figure 4 shows an example.
Analysing the disconnected subgraphs included in G, a partition of the graph nodes D( ) = { 1, .
.
.
,  |D( )|} is built.
D( ) is the partition of the primary space   induced by the space B.
For example, from Figure 4(a), it would be D( ) = { 1,  2} = {( 1,  2,  3,  4), ( 5,  6)}, with   > 0.2.
Each part  i   D( ) constitutes a set of semantically related primary information items linked according to their semantic relationships.
The parameter   de ned in (6) governs the structure of the resulting partition, by relaxing (   ) or restricting (   ) the conditions under which the elements of   can be aggregated.
For each part  i   D( ), its representative element  i is chosen so that:  i = arg max  ij i (cid:88) k(cid:54)=j S( ik,  ij) .
(7) one whose a nity vector total similarity measurement is ma-ximised.
This criterion is based on the observation that the higher is the a nity vector total similarity, the higher is the number of elements in the aggregation that are semantically entailed by it, so that the item content is expected to be the most complete w.r.t.
the semantics of the partition, and therefore the more representativeness is conveyed by the item itself.
To stick with the example shown in Figure 4(a), the representative element for the part  1 = ( 1,  2,  3,  4) would be   =  1.
Given an induced partition D( ) we can  nally build the set of multimodal aggregations D( )  = {  |D( )|}  
 D( ), as follows (with K = |D( )|): 1 , .
.
.
,   j=1 ij  ij = {b   B : R( ij, b) >  } , (10) where   is a parametric threshold.
The function of D( )  is that of integrating the partition D( ) with the semantically relevant elements of B.
Notice that D( )  is not in general a partition of H =     B, because elements of B may be semantically relevant to elements of   belonging to di erent elements of D( ), and because some elements of B may be not semantically relevant to any element of  .
We applied our cross-modal aggregation framework to a concrete case: clustering Web and TV news streams.
The prototype architecture is shown in Figure 5.
The system is a processing machine having two inputs, i.e. digitised broadcast news streams (DTV) and online newspapers feeds (RSSF), and one output, i.e. the multimodal aggregation service (MMAS), that is automatically determined from the semantic aggregation of the input streams.
In connection with the model presented in Section 3, the prototype is thus a particular implementation of the general architecture shown in Figure 2, where RSSF and DTV are the two information streams {I}N 1 and {I}N 2.
The complexity introduced by considering digital television as an information source is primarily constituted by the necessity of automatically detecting and identifying information items in the real-time acquired stream.
For this purpose, in our system the DTV stream is at  rst analy-sed and partitioned into programmes using a visual pattern matching algorithm.
Video elements (shots) indicating starting and ending of programmes are used as reference prototypes to be searched through the acquired video stream [14].
On such detected programmes, automatic segmentation into elementary news stories is performed, as it will be presented in Section 6.1.
Once segmented, the audio track of each story is analysed by an automatic speech recogniser tool [5], providing text transcriptions of the spoken parts in the DTV stream.
Both English and Italian languages are supported.
Finally, the detected news stories are indexed in the TVi documents catalogue.
Summing up, the output of the DTV stream processing chain is an index structure,  i :   i =  i   Bi,   i = 1, .
.
.
, K Bi =  Nj Figure 5: Functional system architecture.
(8) (9) whose contents are the news stories automatically detected by processing broadcast TV programmes.
The RSSF stream consists of RSS feeds from several major online newspapers and press agencies.
Additionally, also users weblogs can be treated.
Quite similarly to the o cial information sources represented by online newspapers, users weblogs can be used to build the aggregations, provided that they are published and delivered as RSS feeds.
On each RSS item, a linguistic analysis is performed to identify meaningful linguistic structures, e.g.
verbs, nouns, adjectives, within the RSS items  content.
This information is used to build a set of lexical terms that capture the semantic concepts expressed in the articles linked by the RSS feeds.
The technical details underlying the functional interface of the RSSF processing chain are described in Section 6.2.
The outputs of the linguistic analysis are then employed by the query constructor to generate a set of representative query expressions, which are submitted to the index structure of the TVi documents catalogue.
For each item, the result of this search operation is a weighted set of newscasts stories of decreasing a nity to the target query.
The results of the queries on the TVi index structure are used by the cross-modal clustering process to aggregate information items based on their semantic similarities, as previously described in Section 4.
These aggregations are indexed and stored in the multimodal aggregation index (MAi).
For each aggregation, the MAi stores the list of the aggregated RSS items and news stories, as well as a text document including the RSS items S titles and description phrases and the news stories transcriptions constituting the aggregation.
Operationally, the MAi is a persistent data repository from which the multimodal services are delivered to the users.
The services currently supported by the system are outlined in the following subsections.
Multimodal navigation is the ability of providing links between heterogeneous information items.
Here, the users are able to browse the lists of broadcast news stories (i.e., the The system provides a recommendation service that helps users  nd the desired information according to their behavior and interests.
The delivering of the service employes the queries submitted by a user to build a list of related queries.
These system-generated queries can be then issued by the user to tune or redirect the search process.
The query generation process is based on the assumption that the relevant aggregations for a query q share some terms apart from the original terms used in q.
Details of how this query derivation process works are given in Section 6.4.
This section describes the core technologies used to implement our prototype system.
Segmentation of broadcast TV streams into programmes is performed by adopting an optimised video clip matching technique.
A set of feature signatures are extracted from each frame of the acquired video clips, including colour, texture and motion histograms.
Among all those extracted, features are selected that maximise the statistical divergence w.r.t.
a sample population of the event to be searched, e.g.
a programme s jingle.
The selected feature vectors are then matched against those indicating starting and ending of pro-grammes, using the histogram intersection distance.
Once detected, TV newscasts are automatically segmented into their elementary news stories.
The segmentation process is done exploiting aural and visual cues with the help of a three layered heuristic framework.
The used heuristics are based on the editorial rules employed by the TV editors in the news production process.
The basic heuristic H1 considers boundaries of shots containing the anchorman as equivalent to news stories boundaries.
The anchorman shots are detected using a second heuristic H2 that considers the most frequent speaker as the anchorman.
This heuristic allows to select the speaker who most likely is the anchorman, provided that a speaker clustering process labels all the speakers present in the programme and associates them to temporal segments of the content.
As the application of H1 and H2 is not yet enough to discern situations where e.g.
the anchorman presents several consecutive brief stories without interruptions  lled with external contributions, or where the beginning of a story does not correspond to an anchorman shot, we employ a third heuristic H3 based on the observation that the introduction of a new brief story is often accompanied by a camera shot change, e.g.
from a close up shot to a wider one.
Thus, to optimise the accuracy of segmentation, we use an adaptive threshold shot detection algorithm based on the displaced frame di erence (DFD) computed on luminance samples of contiguous video frames.
Adaptivity is based on the local statistics of the DFD, so that content having higher DFD variance is processed against higher thresholds.
Once the shot detection is completed, similar shots, i.e.
those sharing similar visual content, are grouped together through a shot clustering process [14].
This allows us to detect and classify shot clusters as pertaining to studio shots containing the anchorman following the same frequency heuristic used for detecting the candidate speaker (H2).
This double clustering process (both on audio and on video) enables a simple and e ective algorithm for speaker tracking Figure 6: Example of multimodal navigation.
Figure 7: Example of multimodal search & retrieval.
target elements) related to the RSS items (i.e., the source elements).
In this manner, the target elements are contex-tualised by the source elements.
Thus, a context-guided browsing of cross-modal and multimedia (i.e., multimodal) content is o ered to the users, as shown in Figure 6.
The system supports both simple queries (e.g., one or more search keywords) as well as more advanced queries (e.g., weighted queries, boolean operators) for searching and retrieving the aggregations.
As a simple example, Figure 7 shows the  rst result for the query  garbage AND Naples .
To facilitate the results visualisation, the system provides a browsable Web page showing the ranked results.
For each retrieved aggregation (also called  dossier ), the system lists the basic information, i.e. title, score and update time, and provides the links for the included news stories and newspaper articles.
In addition, as the search results are provided in the form of RSS feeds, users can subscribe to the submitted query, and automatically receive a noti cation when the results page is modi ed, i.e. when either an already included aggregation is updated or a new one is discovered.
and news story segmentation.
Figure 8 illustrates an example.
The anchorman shots a and b are detected according to the heuristic H2 because both contain the same speaker A.
As a shot boundary is detected between the shots a and b, the  rst two stories are segmented according to H3.
The succeeding stories are then detected according to H1.
Once detected, the spoken text of each news story is cate-gorised according to its main topic using the AI::Categorizer framework,6 and indexed by Lucene.7 More detail on the whole programme segmentation process is provided in [14].
RSS streams are analysed to get the list of included items.
Each item is represented by the tuple (cid:96) = (uuid, pubDate, link, title, description), where uuid is a identi er that univocally identi es the item, pubDate is the publication date, link is the URL of the related online newspaper article, title is the headline of the corresponding article and description is a short summary of the corresponding article s content.
We split the title and the description  elds into elementary phrases and tokenise them into words by applying the Tree Tagger tool8 that labels each word according to a taxonomy of grammar terms, e.g.
conjugating verbs, proper nouns, adjectives.
Thus, an RSS item   is represented by a vector of key/value pairs   = ((k1t, v1t)T t=1, (k2f , v2f )D1 f =1, .
.
.
, (kml, vml)Dm 1 l=1 ), (11) where the keys are the words in the phrases, the values are the corresponding grammar terms, T , D1 and Dm 1 are, respectively, the total number of tagged words in the title, in the  rst and in the last description phrase.
The use of   allows to extract elements of the title and description sentences that are important from the linguistic point of view, in opposition to statistical approaches (e.g., TF/IDF techniques) that simply rely on term frequency metrics, thus
 7http://lucene.apache.org/ 8http://www.ims.uni-stuttgart.de/projekte/corplex/ TreeTagger/ Figure 9: Example of query construction.
better simulating the human understanding of the semantic implied in the interpretation of short texts.
The vector   is then used to generate a full text query string Q.
The query construction process works in two steps.
First, for each subvector si of  , an elementary query qi is built, selecting the words in si tagged as either common noun or named entity or adjective.
Then, a combined query Q is generated, joining all the elementary queries as follows: m(cid:91)
 qwi i i=1 wi = 2(m i),  i = 1, .
.
.
, m .
(12) (13) This weighting schema associates higher weights to queries derived from phrases occurring earlier, in order to emphasise the title and the initial description phrases.
An example of the query construction process is illustrated in Figure 9.
For each RSS item  i the associated query Qi is launched on the set of the broadcast news speech transcriptions indexed in the TVi (see Figure 5).
The output of Qi is stored in the a nity results vector ri = (rij)n j=1, where rij is the query score of the news story  j to  i.
The set of all the a nity results vectors is then arranged in the a nity matrix A de ned in Equation (1).
From A, we compute the equivalence matrix E of Equation (6), and build the correspondent connectivity graph G. We then proceed to discover the induced partition following the process presented in Section 4.3, and select the representative element of each part of the partition as described in Section 4.4.
We construct the multimodal aggregations as de ned in Section 4.5 cutting o  elements for which rij <   = 0.5.
A text document is generated including the titles, description phrases and transcriptions of the RSS items and news stories constituting the aggregation.
Finally, all such documents are indexed by Lu-cene and made accessible through the MAi repository.
i }|A| As introduced in Section 5.3.3, our system implements a user recommendation functionality through a query expansion mechanism.
The expansion of user queries algorithm works as follows.
Let Q be a query submitted by the user u, and A = {  i=1 be the set of multimodal aggregations retrieved from the MAi (see Figure 5) for Q.
For each ag-i   A, a feature vector v = (s, c, p) is extracted gregation   from the analysis of the RSS items  sentences (titles and description phrases), the referenced news articles text, and the TV news items  transcribed speech content.
The sub-vector s stores the fraction of word occurrences in the aggregation, the normalised (w.r.t.
the total number of objects in the aggregation) scores of the categories to which the aggregation belongs, according to the same set de ned for the news story categorisation (Section 6.1).
The sub-vector p is the set of couples of the proper nouns found by Tree Tagger in the RSS items included in the aggregation   i , and their corresponding frequencies.
The k-means clustering algorithm is run on the set A using v as feature vector, until either the desired precision  is achieved, or the maximum number of epochs Niter is reached.
Because of the heterogeneity of the sub-vectors of v, we used the Euclidean distance to compare the sub-vectors in the s and c space, and the Jaccard distance to compare the sets in the p space.
Given va = (sa, ca, pa) and vb = (sb, cb, pb) two feature vectors, we de ne a combined distance used by the k-means clustering process: |pa   pb| |pa   pb| L2(sa, sb) + L2(ca, cb) + d(va, vb) = (cid:182) (cid:181)

 (14) Once the clustering process is completed, we select the cen-troid of the most populated cluster, CM = (sM , cM , pM ) and select the proper nouns p1, .
.
.
, pK , pi   pM , such that the corresponding frequencies are greater than a dynamic threshold calculated as the mean of all frequencies in pM .
We then derive the two queries Q   {p1 .
.
.
pK} and Q   {p1 .
.
.
pK}.
Let us to consider the following example.
Suppose a user submits the query  Donadoni contratto  (i.e., Donadoni s contract), presumably to  nd information about the contract of Roberto Donadoni.
Let us suppose that the described clustering and selection process discovers the following proper nouns: Abete, Federcalcio (i.e., football federation), Lippi and Marcello.
Then, the following derived queries would be proposed to the user: q1 := (lippi abete marcello federcalcio)   (donadoni contratto), i.e. a re nement of Q; q2 := (lippi abete marcello federcalcio)   (donadoni contratto), i.e. an expansion of Q.
This section presents the experimental evaluation of each part of our system.
The system was run from the end of November 2007 to the beginning of June 2008.
In this period of time, we collected about 88,280 online articles and 23,940 news stories, resulting from the segmentation of 3,670 newscast programmes.
The online articles were downloaded from 95 RSS feeds supplied by 16 online newspapers and press agencies Web sites.
The newscasts were acquired from the daily programming of seven national TV channels.
We set   = 0.8 in Equation (6), thus obtaining a total of 4,187 automatically generated aggregations.
Segmentation The processing conditions are characterised by intense bursts of activities concentrated around the main editions of the newscasts (around 2 pm and around 8 pm).
The system acquires 7 major national channels 24 hours/day and 365 days/year, and elaborates 16 programmes/day appro- ximately 8 for each burst period (total approximately 10 hours/day elaborated material).
To accommodate these requirements, the system has been implemented on a distributed multi-CPU architecture.
The programme segmentation task takes on average   3.74 times the programme duration, that is normally a newscast of 30 minutes.
Two distinct experiments were performed to test the pro-gramme boundary detection accuracy.
The  rst was aimed at identifying 11 di erent reference clips from a data set of 782 clips randomly acquired from daily television schedules.
The second consisted in detecting the starting and ending jingles of seven distinct news programmes (total 14 clips) in real-time, broadcast streams.
In the  rst experiment, the achieved precision and recall were   0.80 and   0.87, respectively.
In the second experiment the reached precision and recall were, respectively, 1.00 and   0.90.
In order to measure the quality of news segmentation we used an alignment measurement that takes into account starting and ending boundaries with di erent weights.
In addition, it considers under-segmentation e ects (i.e., when the detected story starts/ends after/before the actual story) as being more penalising than over-segmentation e ects (i.e., when the detected story starts/ends before/after the actual story) on the measurement [8].
The system was tested against a test set of 84 programmes (i.e.,   40 hours of material) achieving a precision of 0.76 and a recall of 0.73.
The news story subject categorisation task was performed using a naive Bayesian classi er.
A data set of 25,000 automatic speech transcriptions was collected.
The classi er was trained on four  fths of the available data using a standard subject taxonomy of 21 categories.
The remaining data were used for testing, reaching an accuracy value of   0.82.
Let P = {  i=1 be a set of multimodal aggregations, and let ti be the title automatically assigned to   i .
To test the overall e ciency of the multimodal aggregation service, we set up a pool of 25 users, taken from the employers of our organisation, who were unaware of the rationales of the system.
Each user was asked to perform evaluations in front of an optimised evaluation interface, designed and implemented on purpose.
The interface shows a random list of aggregations.
Each aggregation was evaluated through the following markers, using a judgement scale from 1 (i.e., disappointment) to 5 (i.e., full satisfaction): i =  i Bi}|P|
 i assign a cohesion index  i that re ects the overall consistency of the multimodal aggregation.
1, .
.
.
,| i|, assign a consistency index  ij to the concept expressed by the multimodal aggregation   i ;
 k = 1, .
.
.
,|Bi|, assign a consistency index rik to the concept expressed by the multimodal aggregation   i ; i choose a title Ti among those belonging to the RSS items  ij    i, and assign a representativity index  i to it;
 Index Score   SD   CI  start CI  end  1



  2



  3



  4



  5
 --- 6



  7



 The following performance indices can be then de ned:   Cohesion of the multimodal aggregations:  n (15)   Consistency of the Web and TV aggregations: |P|(cid:88) n=1  1 =

 |P|(cid:88) |P|(cid:88) n=1 | n|(cid:88) |Bn|(cid:88) m=1
 | n|
 |Bn| n=1 m=1  2 =

  3 =

  nm (16) rnm (17)   Title representativity of the multimodal aggregations:  4 =

  n (18)   System-settled and user-de ned title agreement: |{  i   P : ti = Ti}|  5 = 5 (19)   Relevance of the correctly and wrongly selected titles.
|P|(cid:88) n=1
 (cid:80) (cid:80)  6 =  7 =   i  P:ti=Ti
   i  P:ti(cid:54)=Ti
  iR(ti)  iR(ti) , (20) (21) where R(ti) is a function returning the index of the RSS item  i that was taken as the representative for the aggregation   i .
Indices  1 to  4 represent the mean values of their respective elementary markers.
 5 counts the fraction of aggregations for which the title settled by the system agreed with the title chosen by the user.
 6 (analogously  7) indicates on average how well the titles correctly (wrongly) settled by the system explain the topics of the assessed aggregations.
The aggregations were evaluated from March to June 2008, getting a set of 651 assessments.
Table 1 reports the score, the standard deviation (SD) and the 95% con dence interval (CI) for each of the seven e ciency indicators.
The full scale value is 5 for all indicators.
The reported values are statistical indices that take into account subjective assessments as over and under-voting.
Thus, they can be used as measures of the overall e ciency of the MMAS service.
The performance of the aggregation algorithm is in uen-ced by the e ectiveness of the news segmentation process.
In some cases, due to undersegmentation of the news stories, the cohesion of the aggregations decreases.
However, the system shows an outstanding performance, getting a global e ciency index (i.e., the mean of indicators  1,...,7) of 4.12 (over 5).
This value is mainly negatively a ected by the indicator  5, seeming to indicate that the algorithm used to choose the aggregations  titles should be further improved.
However, the index  6 indicates that the titles automatically settled as representative derive from RSS items that were scored as very relevant to the aggregation concept.
In fact, since  6 >  4,  6 <  4 and  6    4 =   we can state that the titles settled by the system are more representative than the average, and that there is a higher level of agreement among reviewers about their relevance score.
Furthermore, as  7 (cid:39)  4, it can be concluded that even if the title automatically settled is wrong, it still signi cantly explains the topic of the corresponding aggregation.
The e ciency of the multimodal search and retrieval service was evaluated using the mean average precision (MAP).
Let Q = {qk}N k=1 be the set of user-generated queries and Hk = {hik}Rk i=1 be the set of retrieved documents for qk, ranked according to the Lucene score.
Average precision (AP) is the average of the precision scores at the ranks where relevant hits (w.r.t.
the original query qk) occur.
AP depends on how the relevant hits are ordered in Hk.
In the best case all the relevant hits appear before any non-relevant ones, thus resulting in APk = 1.
The mean average precision is the mean of AP over the full set Q:


 APk =


 Rk k=1 i=1 gk(i)pk(i) , (22) where gk(i) is a binary function that returns 1 if hik is relevant to qk, and pk(i) is the precision after i hits of Hk.
In the experiments, users were asked to submit some queries to the system, and then mark each retrieved aggregation as relevant or not to the submitted query.
According to TREC speci cations, we evaluated 50 queries, achieving a MAP of 0.79.
This proves that the proposed approach, namely fusing contributions coming from television and the Web into a single document to be indexed, enables the delivering of an e ective search and retrieval service.
Analogously to multimodal aggregation e ciency, we used users  ratings to evaluate our query derivation method.
Let Q  = {q  j=1 be the set of derived queries from the set of original queries Q.
For each q  j   Q , we calculate: j }|Q |
 jects for q  j ; j w.r.t.
the set of retrieved ob-
j derives, expressed by a score from 1 (i.e., from which q   totally unrelated ) to 5 (i.e.,  completely related ).
The following performance markers can be then de ned:   Mean average precision of the derived queries: N(cid:88) k=1 N(cid:88) Rk(cid:88)  8 = M AP   =

   j
 (23) |Q |(cid:88) j=1 (cid:88)  j(cid:54)=0  9 =
 |q  j   Q |  j (24)   E ectiveness of the query derivation system:  10 =  8  9
 .
(25) Indices  8 and  9 are the mean values of their respective elementary markers.
 10 measures the e ectiveness of the query derivation process w.r.t.
the initial topics of interest to the users.
We set k = 64,  = 0.05 and Niter = 20 for k-means.
A set of 140 derived queries produced by the user panel was evaluated, getting the following results:  8 = 0.85,  9 = 4.3 and  10 = 0.73.
The results show that the use of the derived queries improves the number of relevant documents retrieved at the top of the results list.
Additionally, high relevance to the original query is provided.
Therefore, the method is helpful in  nding new relevant documents for the users who formulated the original query.
In this paper we presented a novel methodology to support the delivery of multimodal aggregation services.
Multimoda-lity is the capability of fusing and presenting heterogeneous data, such as audio, video and text, from multiple information sources, such as the Internet and TV.
The method is based on: (i) a semantic relevance function acting as a kernel to discover the semantic a nities of heterogeneous information items, and (ii) an asymmetric vector projection model on which semantic dependency graphs among information items are built and representative elements of these graphs can be selected.
To prove the applicability of our technique, we developed a system for aggregating and retrieving online newspaper articles and broadcast news stories.
Obtained results are very encouraging and demonstrate the robustness and e ectiveness of the proposed method.
Future work will focus on a comparative analysis on clustering performance using symmetric similarity functions, and on the optimisa-tion of the programme segmentation algorithms.
Additional work will explore the integration of further information sources such as images and radio data, and the use of more sophisticated query derivation models.
