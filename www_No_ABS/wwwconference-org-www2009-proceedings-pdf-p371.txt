In a traditional information system, a user composes a query, submits it to the system, which retrieves relevant Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
answers.
This information-access paradigm requires the user to have certain knowledge about the structure and content of the underlying data repository.
In the case where the user has limited knowledge about the data, often the user feels  left in the dark  when issuing queries, and has to use a try-and-see approach for  nding information, as illustrated by the following example.
At a conference venue, an attendee named John met a person from a university.
After the conference he wanted to get more information about this person, such as his research projects.
All John knows about the person is that he is a professor from that university, and he only remembers the name roughly.
In order to search for this person, John goes to the directory page of the university.
Figure 1 shows such an interface.
John needs to  ll in the form by providing information for multiple attributes, such as name, phone, department, and title.
Given his limited information about the person, especially since he does not know the exact spelling of the person s name, John needs to try a few possible keywords, go through the returned results, modify the keywords, and reissue a new query.
He needs to repeat this step multiple times to  nd the person, if lucky enough.
This search interface is neither e cient nor user friendly.
Figure 1: A typical directory-search form.
Many systems are introducing various features to solve this problem.
One of the commonly used methods is au-tocomplete, which predicts a word or phrase that the user may type based on the partial query the user has entered.
As an example, almost all the major search engines nowadays automatically suggest possible keyword queries as a user types in partial keywords.
Both Google Finance and Yahoo!
Finance support searching for stock information interactively as users type in keywords.
More and more Web sites are having this feature, due to recent advances in high-speed networks and browser-based programming languages and tools such as JavaScript and AJAX.
In this paper, we study a new computing paradigm, called  interactive, fuzzy search.  It has two unique features: (1) Interactive: The system searches for the best answers  on the  y  as the user types in a keyword query; (2) Fuzzy: When searching for relevant records, the system also tries to  nd those records that include words similar to the keywords in the query, even if they do not match exactly.
We have developed several prototypes using this paradigm.
The  rst one supports search on the UC Irvine people directory.
A screenshot is shown in Figure 2.
In the  gure, a user has typed in a query string  professor smyt .
Even though the user has not typed in the second keyword completely, the system can already  nd person records that might be of interest to the user.
Notice that the two keywords in the query string (including a partial keyword  smyt ) can appear in di erent attributes of the records.
In particular, in the  rst record, the keyword  professor  appears in the  title  attribute, and the partial keyword  smyt  appears in the  name  attribute.
The matched pre xes are highlighted.
The system can also  nd records with words that are similar to the query keywords, such as a person name  smith .
The feature of supporting fuzzy search is especially important when the user has limited knowledge about the underlying data or the entities he or she is looking for.
As the user types in more letters, the system interactively searches on the data, and updates the list of relevant records.
The system also utilizes a-priori knowledge such as synonyms.
For instance, given the fact that  william  and  bill  are synonyms, the system can  nd a person called  William Kropp  when the user has typed in  bill crop .
This search prototype has been used regularly by many people at UCI, and received positive feedback due to the friendly user interface and high e ciency.
Another prototype, available at http://dblp.ics.uci.edu, supports search on a DBLP dataset (http//www.informatik.uni-trier.de/ ley/db/) with about
 http://pubmed.ics.uci.edu, supports search on 3.95 million MEDLINE records (http://www.ncbi.nlm.nih.gov/pubmed).
In this paper we study research challenges that arise naturally in this computing paradigm.
The main challenge is the requirement of a high e ciency.
To make search really interactive, for each keystroke on the client browser, from the time the user presses the key to the time the results computed from the server are displayed on the browser, the delay should be as small as possible.
An interactive speed requires this delay be within milliseconds.
Notice that this time includes the network transfer delay, execution time on the server, and the time for the browser to execute its javascript (which tends to be slow).
Providing a high e -ciency on a large amount of data is especially challenging because of two reasons.
First, we allow the query keywords to appear in di erent attributes with an arbitrary order, and the  on-the- y join  nature of the problem can be computationally expensive.
Second, we want to support fuzzy search by  nding records with keywords that match query keywords approximately.
We develop novel solutions to these problems.
We present several incremental-search algorithms for answering a query by using cached results of earlier queries.
In this way, the computation of the answers to a query can spread across multiple keystrokes of the user, thus we can achieve a high speed.
Speci cally, we make the following contributions.
(1) We  rst study the case of queries with a single keyword, and present an incremental algorithm for computing keyword pre xes similar to a pre x keyword (Section 3).
(2) For queries with multiple keywords, we study various techniques for computing the intersection of the inverted lists of query keywords, and develop a novel algorithm for computing the results e ciently (Section 4.1).
Its main idea is to use forward lists of keyword IDs for checking whether a record matches query keyword conditions (even approximately).
(3) We develop a novel on-demand caching technique for incremental search.
Its idea is to cache only part of the results of a query.
For subsequent queries, un n-ished computation will be resumed if the previously cached results are not su cient.
In this way, we can e ciently compute and cache a small amount of results (Section 4.2).
(4) We study various features in this paradigm, such as how to rank results properly, how to highlight keywords in the results, and how to utilize domain-speci c information such as synonyms to improve search (Section 5).
(5) In addition to deploying several real prototypes, we conducted a thorough experimental evaluation of the developed techniques on real data sets, and show the practicality of this new computing paradigm.
All experiments were done using a single desktop machine, which can still achieve response times of milliseconds on millions of records.
Prediction and Autocomplete:1 There have been many studies on predicting queries and user actions [17, 14, 9, 19,

 Here we use it to refer to the case where a query (possibly with multiple keywords) is treated as a single pre x.
of partial input the user has already typed.
Many prediction and autocomplete systems treat a query with multiple keywords as a single string, thus they do not allow these keywords to appear at di erent places.
For instance, consider the search box on the home page of Apple.com, which allows autocomplete search on Apple products.
Although a keyword query  itunes  can  nd a record  itunes wi-fi music store,  a query with keywords  itunes music  cannot  nd this record (as of November 2008), simply because these two keywords appear at di erent places in the record.
The techniques presented in this paper focus on  search on the  y,  and they allow query keywords to appear at di er-ent places.
As a consequence, we cannot answer a query by simply traversing a trie index.
Instead, the backend intersection (or  join ) operation of multiple lists requires more e cient indexes and algorithms.
CompleteSearch: Bast et al. proposed techniques to support  CompleteSearch,  in which a user types in keywords letter by letter, and the system  nds records that include these keywords (possibly at di erent places) [4, 5, 2, 3].
Our work di ers from theirs as follows.
(1) CompleteSearch mainly focused on compression of index structures, especially in disk-based settings.
Our work focuses on e cient query processing using in-memory indexes in order to achieve a high interactive speed.
(2) Our work allows fuzzy search, making the computation more challenging.
(3) For a query with multiple keywords, CompleteSearch mainly caches the results of the query excluding the last keyword, which may require computing and caching a large amount of intermediate results.
Our incremental caching technique described in Section 4.2 achieves a higher e ciency.
Gram-Based Fuzzy Search: There have been recent studies to support e cient fuzzy string search using grams [7, 1,
 string that can be used as a signature for e cient search.
These algorithms answer a fuzzy query on a collection of strings using the following observation: if a string r in the collection is similar to the query string, then r should share a certain number of common grams with the query string.
This  count  lter  can be used to construct gram inverted lists for string ids to support e cient search.
In Section 6 we evaluated some of the representative algorithms.
The results showed that, not surprisingly, they are not as e cient as trie-based incremental-search algorithms, mainly because it is not easy to do incremental computation on gram lists, especially when a user types in a relatively short pre x, and count  ltering does not give enough pruning power to eliminate false positives.
Problem Formulation: We formalize the problem of interactive, fuzzy search on a relational table, and our method can be adapted to textual documents, XML documents, and relational databases.
Consider a relational table T with m attributes and n records.
Let A = {a1, a2, .
.
.
, am} denote the attribute set, R = {r1, r2, .
.
.
, rn} denote the record set, and W = {w1, w2, .
.
.
, wp} denote the distinct-word set in T .
Given two words wi and wj,  wi (cid:3) wj   denotes that wi is a pre x string of wj .
A query consists of a set of pre xes Q = {p1, p2, .
.
.
, pl}.
For each pre x pi, we want to  nd the set of pre xes from the data set that are similar to pi.2 In this work we use edit distance to measure the similarity between two strings.
The edit distance between two strings s1 and s2, denoted by ed(s1, s2), is the minimum number of edit operations (i.e., insertion, deletion, and substitution) of single characters needed to transform the  rst one to the second.
For example, ed(smith, smyth) = 1.
Definition 1 (Interactive Fuzzy Search).
Given a set of records R, let W be the set of words in R. Consider a query Q = {p1, p2, .
.
.
, p(cid:2)} and an edit-distance threshold  .
For each pi, let Pi be {p i (cid:3) w and (cid:2) i, pi)    }.
Let the set of candidate records RQ be (cid:2) ed(p {r|r   R,  1   i   (cid:3),  p i   Pi and wi appears in r, (cid:2) i (cid:3) wi}.
The problem is to compute the best records in (cid:2) p RQ ranked by their relevancy to Q.
These records are computed incrementally as the user modi es the query, e.g., by typing in more letters.
i| w   W, p (cid:2) Indexing: We use a trie to index the words in the relational table.
Each word w in the table corresponds to a unique path from the root of the trie to a leaf node.
Each node on the path has a label of a character in w. For simplicity, a node is mentioned interchangeably with its corresponding string in the remainder of the paper.
Each leaf node has an inverted list of IDs of records that contain the corresponding word, with additional information such as the attribute in which the keyword appears and its position.
For instance, Figure 3 shows a partial index structure for publication records.
The word  vldb  has a trie node ID of 15, and its inverted list includes record IDs 6, 7, and 8.
For simplicity, the  gure only shows the record ids, without showing the additional information about attributes and positions.
d a t a l u
 i
 n

 u i s

 v l d b
 Figure 3: Trie with inverted lists at leaf nodes.
In this section we study interactive, fuzzy search on a single keyword pre x.
For the case of exact pre x search using the trie index, the approach is straightforward: for each pre x, there exists only one corresponding trie node (if any).
The inverted lists of its descendant leaf nodes are accessed to retrieve candidate records.
The solution to the problem of fuzzy search is trickier since one pre x can have multiple similar pre xes (called  active
 only the last keyword is treated as a partial pre x, and the other are treated as completed keywords.
l

 l

 l

 l

 l
 i
 u

 i
 u

 i
 u

 i
 u

 i
 u

 n u





 i s

 n u





 i s

 n u





 i s

 n u





 i s

 n u





 i s
 (a) Initialize (b) query  n  (c) query  nl  (d) query  nli  (e) query  nlis  Figure 4: Fuzzy search of pre x queries of  nlis  (threshold   = 2).
nodes ), and we need to compute them e ciently.
The leaf descendants of the active nodes are called the predicted keywords of the pre x.
For example, consider the trie in Figure 4.
Suppose the edit-distance threshold   = 2, and a user types in a pre x p =  nlis .
Pre xes  li ,  lin ,  liu , and  luis  are all similar to p, since their edit distances to  nlis  are within  .
Thus nodes 11, 12, 13, and 16 are active nodes for p (Figure 4 (e)).
The predicted keywords for the pre x are  li ,  lin ,  liu , and  luis .
We develop a caching-based algorithm for incrementally computing active nodes for a keyword as the user types it in letter by letter.
Given an input pre x p, we compute and store the set of active nodes  p = {(cid:8)n,  n(cid:9)}, in which n is an active node, and  n = ed(p, n)    .
The idea behind our algorithm is to use pre x ltering: when the user types in one more letter after p, the active nodes of p can be used to compute the active nodes of the new query.
The algorithm works as follows.
First, an active-node set is initialized for the empty query , i.e.,   = {(cid:8)n,  n(cid:9) |  n = |n|    }.
That is, it includes all trie nodes n whose corresponding string has a length |n| within the edit-distance threshold  .
These nodes are active nodes for  since their edit distances to  are within  .
As the user types in a query string px = c1c2 .
.
.
cx letter by letter, the active-node set  px is computed and cached for px.
When the user types in a new character cx+1 and submits a new query px+1, the server computes the active-node set  px+1 for px+1 by using  px .
For each (cid:8)n,  n(cid:9) in  px , the descendants of n are examined as active-node candidates for px+1, as illustrated in Figure 5.
For the node n, if  n + 1    , then n is an active node for px+1, and (cid:8)n,  n + 1(cid:9) is added into  px+1 .
This case corresponds to deleting the last character cx+1 from the new query string px+1.
Notice even if  n + 1     is not true, node n could still potentially become an active node of the new string, due to operations described below on other active nodes in  px .
For each child nc of node n, there are two possible cases.
Case 1: the child node nc has a character di erent from cx+1.
Figure 5 shows a node ns for such a child node, where  s  stands for  substitution,  the meaning of which will become clear shortly.
We have ed(ns, px+1)   ed(n, px) + 1 = If  n + 1    , then ns is an active node for the  n + 1.
new string, and (cid:8)ns,  n + 1(cid:9) is added into  px+1 .
This case corresponds to substituting the label of ns for the letter cx+1.
Case 2: the child node nc has a label cx+1.
Figure 5 shows the node nm for such a child node, where  m  stands for  matching.  In this case, ed(nm, px+1)   ed(n, px) =  n    .
Thus, nm is an active node of the new string, so we add (cid:8)nm,  n(cid:9) into  px+1 .
This case corresponds to the match between the character cx+1 and the label of nm.
One subtlety here is that, if the distance for the node nm is smaller than  , i.e.,  n <  , the following operation is also required: for each nm s descendant d that is at most      n letters away from nm, we need to add (cid:8)d,  d(cid:9) to the active-node set for the new string px+1, where  d =  n + |d|   |nm|.
This operation corresponds to inserting several letters after node nm.3     n, n     px+1     +1 n , n ns , n+1 nm , n cx+1 px c1 c2 c3 .
.
.
cx cx+1     n d , n+|d|-|nm|     Deletion Match Substitution Insersion Figure 5: Incrementally computing the active-node set  px+1 from the active-node set  px .
We consider an active node (cid:8)n,  n(cid:9) in  px .
During the computation of set  px+1 , it is possible to add multiple pairs (cid:8)v,  1(cid:9), (cid:8)v,  2(cid:9), .
.
.
for the same trie node v.
In this case, only the one with the smallest edit operation is kept in  px+1 .
The reason is that, by de nition, for the same trie node v, only the pair with the edit distance between the node v and the query string px+1 should be kept in  px+1 , which means the minimum number of edit operations to transform the string of v to the string of px+1.
insertions, because if these descendants are active nodes of  px+1 , their parents must appear in  px , and they can be processed by substitutions on their parents.
rithm.
Lemma 1.
For a query string px = c1c2 .
.
.
cx, let  px be its active-node set.
Consider a new query string px+1 = c1c2 .
.
.
cxcx+1.
(1) Soundness: Each node computed by the algorithm described above is an active node of the new query string px+1.
(2) Completeness: Every active node of the new query string px+1 will be computed by the algorithm above.
For example, assume a user types in a query  nlis  letter by letter, and the threshold   is 2.
Figure 4 illustrates how the algorithm processes the pre x queries invoked by keystrokes.
Table 1 shows the details of how to compute the active-node sets incrementally.
The  rst step is to initialize   = {(cid:8)0, 0, (cid:8)10, 1(cid:9), (cid:8)11, 2(cid:9), (cid:8)14, 2(cid:9)} (Figure 4(a) and Table 1(a)).
When the user types in the  rst character  n , for the string s =  n , its active-node set  s is computed based on   as follows.
For (cid:8)0, 0(cid:9)    , we add (cid:8)0, 1(cid:9) into  s, since letter  n  can be deleted.
For node 10, a child of node 0 with a letter  l , (cid:8)10, 1(cid:9) is added into  s, as  l  can be substituted for  n .
There are no match and insertion operations as node 1 does not have a child with label  n .
 s is computed in this way (Figure 4 (b) and Table 1 (b)).
Similarly, pre x queries of  nlis  can be answered incrementally.
For each active node, the keywords corresponding to its leaf descendants are predicted keywords.
Consider the active-node set for the pre x query  nl  as shown in Figure 4(c).
For (cid:8)11, 2(cid:9),  li,  lin , and  liu  are predicted keywords accordingly.
We retrieve the records on the inverted lists of predicted words to compute answers to the query.
Table 1: Active-node sets for processing pre x queries of  nlis  (edit distance threshold   = 2) (a) Query  n  (b) Query  nl  (c) Query  nli 
 Delete Substitute Match Insert  n  n Delete Substitute Match Insert  nl  nl Delete Substitute Match Insert  nli  nli Delete Substitute Match Insert  nlis (d) Query  nlis  (cid:2)11,2(cid:3) (cid:2)14,2(cid:3) (cid:2)0,0(cid:3) (cid:2)0,1(cid:3) (cid:2)10,1(cid:3) (cid:2)10,1(cid:3) (cid:2)10,2(cid:3)     (cid:2)11,2(cid:3);(cid:2)14,2(cid:3)             (cid:2)0,1(cid:3); (cid:2)10,1(cid:3); (cid:2)11,2(cid:3); (cid:2)12,2(cid:3); (cid:2)14,2(cid:3) (cid:2)12,2(cid:3)       (cid:2)0,1(cid:3) (cid:2)10,1(cid:3) (cid:2)0,2(cid:3) (cid:2)10,2(cid:3) (cid:2)11,2(cid:3);(cid:2)14,2(cid:3)   (cid:2)10,1(cid:3)   (cid:2)11,2(cid:3);(cid:2)14,2(cid:3)           (cid:2)10,1(cid:3); (cid:2)0,2(cid:3); (cid:2)11,2(cid:3); (cid:2)14,2(cid:3) (cid:2)11,2(cid:3) (cid:2)12,2(cid:3) (cid:2)14,2(cid:3)                 (cid:2)10,1(cid:3) (cid:2)10,2(cid:3) (cid:2)14,2(cid:3) (cid:2)11,1(cid:3) (cid:2)0,2(cid:3)                 (cid:2)11,2(cid:3) (cid:2)14,2(cid:3)     (cid:2)15,2(cid:3)   (cid:2)12,2(cid:3);(cid:2)13,2(cid:3) (cid:2)11,1(cid:3); (cid:2)10,2(cid:3); (cid:2)12,2(cid:3); (cid:2)13,2(cid:3); (cid:2)14,2(cid:3); (cid:2)15,2(cid:3) (cid:2)11,1(cid:3) (cid:2)10,2(cid:3) (cid:2)12,2(cid:3) (cid:2)13,2(cid:3) (cid:2)14,2(cid:3) (cid:2)15,2(cid:3) (cid:2)11,2(cid:3)   (cid:2)12,2(cid:3);(cid:2)13,2(cid:3)                   (cid:2)11,2(cid:3); (cid:2)12,2(cid:3); (cid:2)13,2(cid:3); (cid:2)16,2(cid:3)     (cid:2)16,2(cid:3)                   In this section we study answering interactive fuzzy search when a user types in multiple keywords.
The goal is to ef- ciently and incrementally compute the records with keywords whose pre xes are similar to those query keywords.
(1) Inter-We focus on several challenges in this setting.
section of multiple lists of keywords: Each query keyword (treated as a pre x) has multiple predicted complete keywords, and the union of the lists of these predicted keywords includes potential answers.
The union lists of multiple query keywords need to be intersected in order to compute the answers to the query.
These operations can be computationally costly, especially when each query keyword can have multiple similar pre xes.
In Section 4.1 we study various algorithms for computing the answers e ciently.
(2) Cache-based incremental intersection: In most cases, the user types the query letter by letter, and subsequent queries append additional letters to previous ones.
Based on this observation, we study how to use the cached results of earlier queries to answer a query incrementally (Section 4.2).
For simplicity, we  rst consider exact search, and then extend the results to fuzzy search.
Given a query Q = {p1, p2, .
.
.
, p(cid:2)}, suppose {ki1 , ki2 , .
.
.}
is the set of keywords that share the pre x pi.
Let Lij denote the inverted list of kij , and Ui = j Lij be the union of the lists for pi.
We i Ui.
study how to compute the answer to the query, i.e.,


 i,j |Lij|).
The shorter the keyword pre x is, the slower Simple Methods: One method is the following.
For each pre x pi, we compute the corresponding union list Ui on-the- y and intersect the union lists of di erent keywords.
The time complexity for computing the unions could be
 the query could be, as inverted lists of more predicted keywords need to be traversed to generate the union list.
This approach only requires the inverted lists of trie leaf nodes, and the space complexity of the inverted lists is O(n   L), where n is the number of records and L is the average number of distinct keywords of each record.
Alternatively, we can pre-compute and store the union list of each pre x, and intersect the union lists of query keywords when a query comes.
The main issue of this approach is that the precomputed union lists require a large amount of space, especially since each record occurrence on an inverted list needs to be stored many times.
The space complexity of all the union lists is O(n   L   w), where w is the average keyword length.
Compression techniques can be used to reduce the space requirement.
There have been other approaches for answering pre x intersection.
For instance, Bast et al. [4] proposed a method that groups ranges of keywords and builds document lists separately for each range.
Intersection is performed between an existing document list and several ranges called  HYB blocks.  The limitation of this approach is that, for most queries, the ranges can include many irrelevant documents, which require a lot of time to process.
We will show our experimental results of this method in Section 6.
E cient Pre x Intersection Using Forward Lists: We develop a new solution based on the following ideas.
Among the union lists U1,U2, .
.
.
,U(cid:2), we identify the shortest union list.
Each record ID on the shortest list is veri ed by check-ascending order of their lengths).
Notice that these union lists are not materialized in the computation.
The shortest union list can be traversed by accessing the leaf nodes of the corresponding pre x.
The length of each union list can be pre-computed and stored in the trie, or estimated on-the- y.
To verify record occurrences e ciently, a forward list can be maintained for each record r, which is a sorted list of IDs of keywords in r, denoted as Fr.
A unique property of the keyword IDs is that they are encoded using their alphabetical order.
Therefore, each pre x pi has a range of keyword IDs [M inIdi, M axIdi], so that if pi is a pre x of another string s, then the ID of s should be within this range.
An interesting observation is, for a record r on the shortest union list, the problem of verifying whether r appears on (non-materialized) union list Uk of a query pre x pk, is equivalent to testing if pk appears in the forward list Fr as a pre x.
We can do a binary search for M inIdk on the forward list Fr to get a lower bound Idlb, and check if Idlb is no larger than M axIdk.
The probing succeeds if the condition `   holds, and fails otherwise.
The time complexity for process-((cid:3)   1)logL , where (cid:3) is the ing each single record r is O number of keywords in the query, and L is the average number of distinct keywords in each record.
A good property of this approach is that the time complexity of each probing does not depend on the lengths of inverted lists, but on the number of unique keywords in a record (logarithmically).
Figure 6 shows an example when a user types in a query  vldb li .
The predicted keywords for  li  are  li ,  lin ,  line , and  linux .
The keyword-ID range of each query keyword is shown in brackets.
For instance, the keyword-ID range for pre x  li  is [3, 5], which covers the ranges of  lin  and  liu .
To intersect the union list of  vldb  with that of  li , we  rst identify  vldb  as the one with the shorter union list.
The record IDs (6, 7, 8, .
.
.)
on the list are probed one by one.
Take record 6 as an example.
Its forward list contains keyword IDs 2, 4, 8, .
.
.. We use the range of  li  to probe the forward list.
By doing a binary search for the keyword ID 3, we  nd keyword with ID 4 on the forward list, which is then veri ed to be no larger than MaxID = 6.
Therefore, record 6 is an answer to the query, and the keyword with ID 4 (which appears in record 6) has  li  as a pre x.
vldb[8,8] li[3,6] li lin line linux vldb [3,6] [4,6] [5,5] [6,6] [8,8]


   Shortest Inverted List
 Probing

 Forward Lists Figure 6: Pre x intersection using forward lists.
(Numbers with underlines are keyword IDs, and numbers without underlines are record IDs.)
Extension to Fuzzy Search: The algorithm described above naturally extends to the case of fuzzy search.
Since each query keyword has multiple active nodes of similar pre- xes, instead of considering the union of the leaf nodes of one pre x node, now we need to consider the unions of the leaf nodes for all active nodes of a pre x keyword.
The lengths of these union lists can be estimated in order to  nd a shortest one.
For each record r on the shortest union list, for each of the other query keywords, for each of its active nodes, we test if the corresponding similar pre x can appear in the record r as a pre x using the forward list of r.
In Section 3 we presented an algorithm for incrementally computing similar pre xes for a query keyword, as the user types the keyword letter by letter.
Now we show that pre- x intersection can also be performed incrementally using previously cached results.
We use an example to illustrate how to cache query results and use them to answer subsequent queries.
Suppose a user types in a keyword query Q1 =  cs co .
All the records in the answers to Q1 are computed and cached.
For a new query Q2 =  cs conf  that appends two letters to the end of Q1, we can use the cached results of Q1 to answer Q2, because the second keyword  conf  in Q2 is more restrictive than the corresponding keyword  co  in Q1.
Each record in the cached results of Q1 is veri ed to check if  conf  can appear in the record as a pre x.
In this way, Q2 does not need to be answered from scratch.
As in this example, in the following discussion, we use  Q1  to refer to a query whose results have been cached, and  Q2  to refer to a new query whose results we want to compute using those of Q1.
Cache Miss: Often the more keywords the user types in, the more typos and mismatches the query could have.
Thus we may want to dynamically increase the edit-distance threshold   as the query string is getting longer.
Then it is possible that the threshold for the new query Q2 is strictly larger than that of the original query Q1.
In this case, the active nodes of keywords in Q1 might not include all those of keywords in Q2.
As a consequence, we cannot use the cached results of Q1 (active nodes and answers) to compute those of Q2.
This case is a cache miss, and we need to compute the answers of Q2 from scratch.
Reducing Cached Results: The cached results of query Q1 could be large, which could require a large amount of time to compute and space to store.
There are several cases where we can reduce the size.
The  rst case is when we want to use pagination, i.e., we show the results in di erent pages.
In this case, we can traverse the shortest list partially, until we have enough results for the  rst page.
As the user browses the results by clicking  Previous  and  Next  links, we can continue traversing the shortest list to compute more results and cache them.
The second case is when the user is only interested in the best results, say, the top-k records according a ranking function, for a prede ned constant k. Such a function could allow us to compute the answers to the query Q1 without traversing the entire shortest list, assuming we are sure that all the remaining records on the list cannot be better than the results already computed.
In other words, the ranking function allows us to do early termination during the traversal.
When using the top-k results of Q1 to compute the top-k results of Q2, it is possible that the cached results are not enough, since Q2 has a more restrictive keyword.
In this case, we can continue the un nished traversal on the the traversal stopped on the shortest list for query Q1.
Figure 7 shows an example of incrementally computing top-k answers using cached results.
A user types in a query  cs conf vanc  letter by letter, and the server receives queries  cs co ,  cs conf , and  cs conf vanc  in order.
(Notice that it is possible that some of the pre x queries were not received by the server due to the network overhead and the server delay.)
The  rst query  cs co  is answered from scratch.
Assuming the union list of keyword  cs  is the shorter one.
The traversal stops at the  rst vertical bar.
Each record accessed in the traversal is veri ed by probing the keyword range of  co  using the forward list of the record.
Records that pass the veri cation are cached.
When we want to answer the query  cs conf  incrementally, we  rst verify each record in the cached result of the previous query by probing the keyword range of  conf .
Some of these results will become results of the new query.
If the results from the cache is insu cient to compute the new top-k, we resume the traversal on the list of  cs , starting from the stopping point of the previous query, until we have enough top-k results for the new query.
The next query  cs conf vanc  is answers similarly.
cs co cs conf Stopping point Compute Traversal list Cached results Verify Compute Traversal list cs conf vanc Cached results Verify Compute Traversal list Cached results Figure 7: Computing top-k results using cached answers and resuming un nished traversal on a list.
In the case of cache miss, i.e., earlier cached results cannot be used to compute the answers of a new query, we may need to answer the new query from scratch.
We may choose a di erent list as the shortest one to traverse, and subsequent queries can be computed incrementally similarly.
A ranking function considers various factors to compute an overall relevance score of a record to a query.
The following are several important factors.
(1) Matching pre xes: We consider the similarity between a query keyword and its best matching pre x.
The more similar a record s matching keywords are to the query keywords, the higher this record should be ranked.
The similarity is also related to keyword length.
For example, when a user types in a keyword  circ , the word  circle  is closer to the query keyword than  circumstance , therefore records containing the word  circle  could be ranked higher.
In most cases exact matches on the query should have a higher priority than fuzzy matches.
(2) Predicted keywords: Di erent predicted keywords for the same pre x can have di erent weights.
One way to assign a score to a keyword is based on its inverted document frequency (IDF).
(3) Record weights: Di erent records could have di erent weights.
For example, a publication record with many citations could be ranked higher than a less cited publication.
As an example, the following is a scoring function that combines the above factors.
Suppose the query is Q = {p1, p2, .
.
.
}, p (cid:2) i is the best matching pre x for pi, and ki (cid:2) is the best predicted keyword for p i) be an (cid:2) i and pi.
The score of a record r edit similarity between p for Q can be de ned as: (cid:2) i.
Let sim(pi, p
 Score(r, Q) = [sim(pi, p i i)+ (|p (cid:2) i| |ki|)+ score(r, ki)], (cid:2) where   and   are weights (0 <   <   < 1), and score(r, ki) is a score of record r for keyword ki.
When displaying records to the user, the most similar pre- xes for an input pre x should be highlighted.
This highlighting is straightforward for the exact-match case.
For fuzzy search, a query pre x could be similar to several pre- xes of the same predicted keyword.
Thus, there could be multiple ways to highlight the predicted keyword.
For example, suppose a user types in  lus , and there is a predicted keyword  luis .
Both pre xes  lui  and  luis  are similar to  lus , and there are several ways to highlight them, such as  luis  or  luis .
To address this issue, we use the concept of normalized edit distance.
Formally, given two pre xes pi and pj, their normalized edit distance is: ed(pi, pj) max(|pi|,|pj|) , ned(pi, pj) = (1) where |pi| denotes the length of pi.
Given an input pre x and one of its predicted keywords, the pre x of the predicted keyword with the minimum ned to the query is highlighted.
We call such a pre x a best matched pre x, and call the corresponding normalized edit distance the  minimal normalized edit distance,  denoted as  mned .
This pre x is considered to be most similar to the input.
For example, for the keyword  lus  and its predicted word  luis , we have ned( lus ,  l ) = 2 3 , ned( lus ,  lui ) = 1
 = ned( lus ,  luis ),  luis  will be highlighted.
3 , and ned( lus ,  luis ) = 1 3 , ned( lus ,  lu ) = 1
 We can utilize a-priori knowledge about synonyms to  nd relevant records.
For example,  William = Bill  is a common synonym in the domain of person names.
Suppose in the underlying data, there is a person called  Bill Gates .
If a user types in  William Gates , we can also  nd this person.
To this end, on the trie, the node corresponding to  Bill  has a link to the node corresponding to  William , and vise versa.
When a user types in  Bill , in addition to retrieving the records for  Bill , we also identify those of  William  following the link.
In this way, our techniques can be extended to utilize synonyms.
We deployed several prototypes in di erent domains to support interactive, fuzzy search.
We conducted a thorough experimental evaluation of the developed techniques on real data sets, such as publications and people directories.
Here we report the results on the following two data sets mainly because of their relative large sizes.
(1) DBLP: It included about one million computer science publication records, with six attributes: authors, title, conference or journal name, year, page numbers, and URL.
(2) MEDLINE: It had about 4 million latest publication records related to life sciences and biomedical information.
We used  ve attributes: authors, their a liations, article title, journal name, and journal issue.
Table 2 shows the sizes of the data sets, index sizes, and index-construction times.
Table 2: Data sets and index costs Data Set
 Record Number Original Data Size # of Distinct Keywords Index-Construction Time Trie Size Inverted-List Size Forward-List Size 1 million



 50 secs






 4 million

 1.79 million 588 secs





 Two prototypes are available at http://dblp.ics.uci.edu/ and http://pubmed.ics.uci.edu/.
For each of them, we set up a Web server using Apache2 on a Linux machine with an Intel Core 2 Quad processor Q6600 (2.40GHz, 8M, 1066MHz FSB) and 8G DDR2-800 memory.
We implemented the backend as a FastCGI server process, which was written in C++, compiled with a GNU compiler.
To make sure our experiments did not a ect the deployed systems, we did the experiments on another Linux machine with an Intel Core


 We evaluated the e ciency of computing the pre xes on the trie that are similar to a query keyword.
For each data set, we generated 1,000 single-keyword queries by randomly selecting keywords in the data set, and applying a random number of edit changes (0 to 2) on the keyword.
The average length (number of letters) of keywords was 9.9 for the DBLP data set, and 10.2 for the MEDLINE data set.
For each pre x of each query, we measured the time to  nd similar pre xes within an edit distance of 2, not including the time to retrieve records.
We computed the average time for the pre x queries with the same length.
We implemented three methods to compute similar pre- xes.
(1) Incremental/Cache: We computed the active nodes of a query using the cached active nodes of previous pre x queries, using the incremental algorithm presented in Section 3.
This algorithm is applicable when the user types a query letter by letter.
(2) Incremental/NoCache: We used the incremental algorithm, but assuming no earlier active nodes have been cached, and the computation started from scratch.
This case happens when a user copies and pastes a long query, and none of the active nodes of any pre x queries has been computed.
It also corresponds to the traditional non-interactive-search case, where a user submits a query and clicks the  Search  button.
(3) Gram-Based: We built gram inverted lists on all pre xes with at least three letters using the method described in [15].
We used the implementation in the Flamingo release4, using a gram length of 3 and a length  lter.
The total number of such pre xes was 1.1 millions for the DBLP data set and 5 millions for the MEDLINE data set.
The index structure can be used to compute similar pre xes for keywords with at least four letters.
Figure 8 shows the performance results of these three methods.
The method Incremental/Cache was most e cient.
As the user types in letters, its response time  rst increased slightly (less than 5 ms), and then started decreasing quickly after the fourth letter.
The main reason is that the number of active nodes decreased, and the cached results made the computation e cient.
The method Incremental/NoCache required longer time since each query needed to be answered from scratch, without using any cached active nodes.
The method Gram-Based performed e ciently when the query keyword had at least seven letters.
But it had a very poor performance for shorter keywords, since the count  lter had a weak power to prune false positives.
Incremental/Cache Incremental/NoCache Gram-Based ) s m
 ( i e m
 h c r a e
 g v





 ) s m
 ( i e m
 h c r a e
 g v





 Incremental/Cache Incremental/NoCache Gram-Based




 Prefix Length (a) DBLP




 Prefix Length (b) MEDLINE Figure 8: Computing pre xes similar to a keyword.
Keywords We evaluated several methods for intersecting union lists of multiple keywords, as described in Section 4.1.
For each data set, we generated 1,000 queries by randomly selecting records from the data set, and choosing keywords from each record.
We implemented the following methods to intersect the union lists of the keywords.
(1) ForwardLists: It is the algorithm presented in Section 4.1, which traverses the shortest union list and uses the other query keywords to probe the forward lists of records on the shortest list.
The union list was traversed on the  y without being materialized.
(2) HashProbe: The shortest union list was materialized as a hash table at query time.
Each record ID on the other union lists was searched on the hash table.
(3) MaterializedUnions: We materialized the union lists of all the query keywords and their pre xes, and computed an intersection by using the record IDs of the shortest list to probe the other union lists.
We measured the intersection time only.
(4) HYB: We implemented a structure called  HYB  as described in [4].
We used an in-memory implementation, and all IDs were stored without any encoding and compression, since decoding and decompression will introduce additional time overhead during query answering.
The number of HYB blocks 4http:// amingo.ics.uci.edu/releases/2.0 and a new query can be incrementally computed using earlier results.
All these methods required a relative long time when the pre x had 6 letters due to the cache miss.
) s m ( i e m
 h c r a e
 g v



 NoCache CompleteTraversal PartialTraversal








 Prefix Length


 Figure 10: Performance of pre x intersection

 We evaluated the scalability of our algorithms.
As an example, we used the MEDLINE dataset.
Figure 11(a) shows how the index size increased as the number of records increased.
It shows that all the sizes of trie, inverted lists, and forward lists increased linearly.
ForwardLists HashProbe MaterializedUnions
 ( i e z
 x e d n


 )

 Forward Lists Inverted Lists Trie





 Record Number (million) (a) Index Size






 Prefix Length (b) MEDLINE Queries with errors Queries with no errors was 162 for the DBLP dataset and 285 for the MEDLINE data set, using the parameters recommended in [4].
We evaluated these methods on queries with two keywords, assuming no previous query results were cached.
Figure 9 shows the average time of each method as the length of the second keyword increased.
The intersection operation was very time consuming when the second keyword had no more than two letters, since the union lists of the pre xes were long.
The HashProbe method performed relatively poorly due to the cost of building the hash table for the shorter list and traversing the other list.
The HYB method s performance was even worse for most cases.
The main reason is that this method was designed to have a pre- x overlapping with few HYB blocks (usually one or two) to avoid merging too many answer lists.
However, it has a side e ect that an HYB block could include too many keywords compared to the query pre x, forcing the method to process long lists.
This drawback was not a main issue in a disk-based setting, as the algorithm can bene t from list compression and sequential disk IOs time.
The Materialize-dUnions method performed well, but with a high memory cost as discussed in Section 4.1.
The ForwardLists algorithm achieved an excellent performance, at the cost of storing the forward lists.
An interesting  nding in the results is that ForwardLists even outperformed MaterializedUnions on the MEDLINE data set.
This is because as the data set became larger, the average time of each binary search on the union lists increased, while the average time of each binary probe on the forward lists did not change much.
i (
 ) s m
 e m

 h c r a e

 g v



 ForwardLists HashProbe MaterializedUnions







 Prefix Length (a) DBLP ) s m
 ( i e m
 h c r a e
 g v





 Figure 9: List intersection of multiple keywords.
We evaluated the performance of di erent methods of cache-based pre x intersection, as described in Section 4.2.
We allowed at most one typo for each pre x with at most  ve letters, and two errors for pre xes with more than  ve letters.
As a consequence, for a query with two keywords, when the sixth letter of the second keyword was typed in, a cache miss occurred.
We implemented the following methods.
(1) NoCache: No query results are cached.
A query is computed without using any cached query results.
Early termination is enabled.
(2) CompleteTraversal: It traverses the shortest union list completely to compute the results of the current query.
(3) PartialTraversal: It traverses the shortest union list partially until it  nds the top 10 results for the current query (as discussed in Section 4.2).
Figure 10 shows the query time of the methods on the DBLP data set.
CompleteTraversal outperformed the No-Cache method for relatively long pre xes (with more than 6 letters) mainly due to the smaller set of cached results.
The PartialTraversal method was the most e cient one in most ) s m ( i e m
 h c r a e
 g v










 Queries with errors Queries with no errors


 Record Number (million)



 (b) Single Keyword




 ) s m ( i e m
 h c r a e
 g v







 Record Number (million) (c) Multiple Keywords

 Figure 11: Scalability (MEDLINE).
We measured the query performance as the data size increased.
We  rst evaluated queries with a single keyword.
We considered two types of queries: the  rst type was generated by randomly selecting keywords in the data set; the second type was obtained by modifying the  rst type by adding edit errors (0 to 2).
Each query asked for the 10 best records.
For each type, we measured the query response time for each keystroke.
Figure 11(b) shows the results for the MEDLINE data set as we increased the number of records.
It shows that the algorithms can answer a single-keyword query e ciently (within 3 ms), for both types of queries.
We next evaluated the algorithms for queries with multiple keywords, which asked for 10 best records.
We generated queries with two keywords, and measured the average ure 11(c) shows that our algorithms can process such queries very e ciently.
For instance, when the data set had 4 million records, a query without errors was processed within 20 ms, while a query with errors was answered within 55 ms.
The round-trip time of interactive fuzzy search consists of three components: server processing time, network time, and JavaScript running time.
Di erent locations in the world could have di erent network delays to our servers in southern California.
We measured the time cost for a sample query  divsh srivstava search  on the DBLP prototype from  ve locations around the world: US west, US east, China, Israel, and Australia.
Figure 12 shows the results.
We can see that the server running time was less than 1/5 of the total round-trip time.
JavaScript took around 40 to 60 ms to execute.
The relative low speed at some locations was mainly due to the network delay.
For example, it took about 4/5 of the total round-trip time when our system was tested from China.
For all the users from di erent locations, the total round-trip time for a query was always below 300 ms, and all of them experienced an interactive interface.
For large-scale systems processing queries from di erent countries, we can solve the possible network-delay problem by using distributed servers.
) s m ( i e m
 p i r
 d n u o
 Network Javascript Server



 US/West US/East China Israel Australia Location Figure 12: Round-trip time for di erent locations.
Interactive search can also save user typing e orts, since results can be found before the user types in complete keywords.
To evaluate the saving of typing e ort, we constructed six queries on the DBLP data set as shown in Table 3.
The keywords in italic font are mistyped keywords.
Each query was typed in letter by letter, until the system found the expected records.
We measured how much letter-typing e ort the system can save for the user.
For each query Qi, let N (Qi) be the number of letters the user typed before the relevant answers are found.
We use 1   N(Qi) |Qi| to quantify the relative saved e ort.
For example, for query Q6, the user could  nd relevant answers right after typing in  divsh sri sea .
The saved e ort of Q6 is 1  13
 as the user only needed to type in 13 letters, instead of 22 letters in the full query.
Table 3 shows that this paradigm can save the user 40% to 60% typing e ort on average.
We studied a new information-access paradigm that supports interactive, fuzzy search.
We proposed an e cient incremental algorithm to answer single-keyword fuzzy queries.
Table 3: Queries and saved typing e ort.
Query Saved typing e ort sunta sarawgi surajit chuardhuri nick kodas approxmate flostsos icde similarity similarty search icde divsh srivstava search












 We studied various algorithms for computing the answers to a query with multiple keywords that are treated as fuzzy, pre x conditions.
We developed e cient algorithms for incrementally computing answers to queries by using cached results of previous queries in order to achieve an interactive speed on large data sets.
We studied several useful features such as ranking answers, highlighting results, and utilizing synonyms.
We deployed several real systems to test the techniques, and conducted an thorough experimental study of the algorithms.
The results proved the practicality of this new computing paradigm.
