Large web search engines need to process thousands of queries per second over tens of billions of pages.
Moreover, the results for each query should be returned within at most a few hundred milliseconds.
A signi cant amount of research and engineering has gone into addressing these tremendous performance challenges, and various optimizations have been  Current A liation: CSE Dept.
Polytechnic Inst.
of NYU Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
proposed based on techniques such as caching, data compression, early termination, and massively parallel processing.
We focus on one important class of optimizations, index compression.
Inverted index compression is used in all major engines, and many techniques have been proposed [26, 29].
Informally, an inverted index for a collection of documents is a structure that stores, for each term (word) occurring somewhere in the collection, information about the locations where it occurs.
In particular, for each term t, the index contains an inverted list It consisting of a number of index postings.
Each posting in It contains information about the occurrences of t in one particular document d, usually the ID of the document (the docID), the number of occurrences of t in d (the frequency), and possibly other information about the locations of the occurrences within the document and their contexts.
The postings in each list are usually sorted by docID.
For example, an inverted list It of the form {56, 1, 34}{198, 2, 14, 23} might indicate that term t occurs once in document 56, at word position 34 from the beginning of the document, and twice in document 198 at positions 14 and 23.
We assume postings have docIDs and frequencies but do not consider other data such as positions or contexts.
Many techniques for inverted index compression have been studied in the literature; see [26, 29] for a survey and [1, 2,
 place each docID (except the  rst in a list) by the di erence between it and the preceding docID, called d-gap, and then encode the d-gap using some integer compression algorithm.
Using d-gaps instead of docIDs decreases the average value that needs to be compressed, resulting in a higher compression ratio.
Of course, these values have to be summed up again during decompression, but this can usually be done very e ciently.
Thus, inverted index compression techniques are concerned with compressing sequences of integers whose average value is small.
The resulting compression ratio depends on the exact properties of these sequences, which depend on the way in which docIDs are assigned to documents.
This observation has motivated several authors [7, 23, 25,
 mizes compression.
The basic idea here is that if we assign docIDs such that many similar documents (i.e., documents that share a lot of terms) are close to each other in the docID assignment, then the resulting sequence of d-gaps will become more skewed, with large clusters of many small values interrupted by a few larger values, resulting in better compression.
In contrast, if docIDs are assigned at random, the distribution of gaps will be basically exponential, and small values will not be clustered together.
In practice, IR systems may assign docIDs to documents in a number of ways, e.g., at random, in the order they are crawled or indexed, or based on global measures of page quality (such as Pagerank [9]).
to change the way docIDs are assigned, but there are many other scenarios where reordering of documents could be used to improve index compression.
In this paper, we follow the document reordering approach studied in [7, 23, 25, 6, 24].
However, while previous work has focused on  nding the best ordering of documents in a collection, we focus on the next step, how to optimize actual index compression and query processing given some suitable document ordering obtained from previous work.
In particular, we extensively study and optimize state-of-the-art compression techniques for docIDs, and propose new algorithms for compressing frequencies, under such optimized orderings.
Frequency values tend to be small compared to docID gaps (on average when a word occurs in a web page it occurs only 3 to 4 times), and thus di erent techniques are needed to improve their compression.
We further study the impact of docID reordering on query throughput, and propose and study a new index optimization problem motivated by the trade-o  between speed and compression ratio of the various methods.
Our experimental results show very signi cant improvements in both overall index size and query processing speed in realistic settings.
To our knowledge, no previous work has looked at frequency compression or query processing performance under the document reordering approach.
The remainder of this paper is organized as follows.
In the next section, we provide some technical background and discuss related work.
Section 3 describes our contributions in more detail.
In Section 4 we study techniques for docID compression, while Section 5 focuses on compression of frequencies.
Section 6 evaluates query processing performance, and Section 7 studies hybrid schemes that apply di erent compression techniques to di erent lists based on query load.
Finally, Section 8 provides concluding remarks.
In this section, we  rst outline several known index compression techniques that we use in our work.
We then discuss previous work on reordering for better inverted index compression, and discuss the applicability of this approach in real systems.
Subsection 2.4 describes block-wise compression and skipping in IR query processors, and Subsection 2.5 introduces the TREC GOV2 data set used by us.
Recall that in inverted index compression, our goal is to compress a sequence of integers, either a sequence of d-gaps obtained by taking the di erence between each docID and the previous docID, or a sequence of frequency values.
In addition, we always deduct 1 from each d-gap and frequency, so that the integers to be compressed are non-negative but do include 0 values.
We now provide brief descriptions of some basic index compression techniques to keep the paper self-contained; for more details, please see the cited literature.
All methods except IPC were recently implemented and evaluated in [27], and we will reuse and extend these highly tuned implementations.
Var-Byte Coding: Variable-byte compression represents an integer in a variable number of bytes, where each byte consists of one status bit, indicating whether another byte follows the current one, followed by 7 data bits.
Thus, 142 = 1   27 + 16 is represented as 10000001 0001000, while 2 is rep resented as 00000010.
Var-byte compression does not achieve a very good compression ratio, but is simple and allows for fast decoding [22] and is thus used in many systems.
Rice Coding: This method compresses a sequence of integers by  rst choosing a b such that 2b is close to the average value.
Each integer n is then encoded in two parts: a quotient q =  n/(2b)  stored in unary code using q + 1 bits, and a remainder r = n mod 2b stored in binary using b bits.
Rice coding achieves very good compression on standard un-ordered collections but is slower than var-byte, though the gap in speed can be reduced by using an optimized implementation described in [27].
S9: Simple9 coding is an algorithm proposed in [2] that combines good compression and high decompression speed.
The basic idea is to try to pack as many values as possible into a 32-bit word.
This is done by dividing each word into 4 status bits and 28 data bits, where the data bits can be partitioned in 9 di erent ways.
For example, if the next 7 values are all less than 16, then we can store them as 7 4-bit values.
Or if the next 3 values are less than 512, we can store them as 3 9-bit values (leaving one data bit unused).
Simple9 uses 9 ways to divide up the 28 data bits: 28 1-bit numbers, 14 2-bit numbers, 9 3-bit numbers (one bit unused), 7 4-bit numbers, 5 5-numbers (three bits unused), 4 7-bit numbers, 3 9-bit numbers (one bit unused), 2 14-bit numbers, or 1 28-bit numbers.
The 4 status bits store which of the 9 cases is used.
Decompression can be optimized by hardcoding each of the 9 cases using  xed bit masks, and using a switch operation on the status bits to select the case.
S16: Simple16 (S16) [27] uses the same basic idea as S9, but has 16 ways of partitioning the data bits, where each of the 16 cases uses all 28 data bits.
The result is that S16 approximately matches the speed of S9, while achieving slightly better compression.
We note here that there are other methods related to S9, such as Relate10 and Carryover12 [2], that also achieve improvements over S9 in certain cases.
PForDelta: This is a compression method recently proposed in [14, 30] that supports extremely fast decompression while also achieving a small compressed size.
PForDelta (PFD)  rst determines a value b such that most of the values to be encoded (say, 90%) are less than 2b and thus  t into a  xed bit  eld of b bits each.
The remaining values, called exceptions, are coded separately.
If we apply PFD to blocks containing some multiple of 32 values, then decompression involves extracting groups of 32 b-bit values, and  nally patching the result by decoding a smaller number of exceptions.
This process can be implemented extremely e -ciently by providing, for each value of b, an optimized method for extracting 32 b-bit values from b memory words.
PFD can be modi ed and tuned in various ways by choosing different thresholds for the number of exceptions allowed, and by encoding the exceptions in di erent ways.
We use some modi cations to PFD proposed in [27], but also add in this paper additional ones that achieves signi cantly better performance in terms of both size and speed.
Interpolative Coding: This is a coding technique proposed in [17] that is ideal for the types of clustered or bursty term occurrences that exist in real large texts (such as books).
In fact, the goal of the document reordering approach is to create more clustered, and thus more compressible, term occurrences, and Interpolative Coding (IPC) has been shown to perform well in this case [6, 7, 23, 24, 25].
IPC di ers from the other methods in an important way: It directly compresses docIDs, and not docID gaps.
Given a set some bounding values l and r known to the decoder, we  rst encode dm where m = (i + j)/2, then recursively compress the docIDs di, .
.
.
, dm 1 using l and dm as bounding values, and then recursively compress dm+1, .
.
.
, dj using dm and r as bounding values.
Thus, we compress the docID in the center, and then recursively the left and right half of the sequence.
To encode dm, observe that dm > l + m   i (since there are m   i values di, .
.
.
dm 1 between it and l) and dm < r   (j   m) (since there are j   m values dm+1, .
.
.
dj between it and r).
Thus, it su ces to encode an integer in the range [0, x] where x = r   l   j + i   2 that is then added to l + m   i + 1 during decoding; this can be done trivially in  log2(x + 1)  bits, since the decoder knows the value of x.
In areas of an inverted list where there are many documents that contain the term, the value x will be much smaller than r   l. As a special case, if we have k docIDs larger than l and less than r where k = r   l   1, then nothing needs to be stored at all as we know that all docIDs properly between l and r contain the term.
This also means that IPC can use less than one bit per value for dense term occurrences.
Evaluation: Index compression techniques are usually evaluated in terms of: (1) The compression ratio, which determines the amount of main memory needed for a memory-based index or the amount of disk tra c for a disk-based index.
State-of-the-art systems typically achieve compression ratios of about 3 to 10 versus the naive 32-bit representation, while allowing extremely fast decompression during inverted list traversals.
(2) The decompression speed, typically hundreds of millions of integers per second, which is crucial for query throughput.
In contrast, compression speed is somewhat less critical, since each inverted list is compressed only once during index building, and then decompressed many times during query processing.
We note that there are two di erent ways to evaluate the compression ratio.
We can consider the total size of the index; this models the amount of space needed on disk, and also the amount of main memory needed if the index is held entirely in main memory during query processing.
Alternatively, we can measure the compressed size of the inverted lists associated with an average query under some query load; this models the amount of data that has to be transferred from disk for each query if the index is entirely on disk (and also the amount of data that has to be moved from main memory to CPU as this can become a bottleneck in highly optimized systems).
In reality, most systems cache part of the index in memory, making a proper evaluation more complicated.
We consider both cases in our experiments, but  nd that the relative ordering of the algorithms stays the same.
Several papers have studied how to reorder documents for better compression [7, 23, 25, 24, 6].
In particular, the approaches in [7, 23, 25, 6]  rst perform some form of text clustering on the collection to  nd similar documents, and then assign docIDs by traversing the resulting graph of document similarities in a Depth-First-Search [7] or TSP-like fashion.
Subsequent work in [24] looked at a much simpler approach, assigning docIDs alphabetically according to URL, and showed that this method basically matches the performance of previous techniques based on text clustering.
Note that such an alphabetical ordering places all documents from the same site, and same subdirectory within a site, next to each other.
This results in improved compression as such documents often have the same topic and writing style.
We use alphabetical assignment of docIDs in all experiments, but our techniques work with any ordering.
Our focus is not on  nding a better assignment, but on exploiting an existing assignment using optimized compression and query processing techniques.
In contrast, previous work considered only a few standard techniques for docID compression, and did not consider frequency compression or query processing.
Another related problem is the compression of inverted indexes for archival collections, i.e., collections that contain di erent versions of documents over a period of time, with often only minor changes between versions.
This problem has recently received some attention in the research community [11, 15, 28, 5], and the basic idea is also to exploit similarity between documents (or their versions).
The techniques used are di erent, and more geared towards getting very large ben-e ts for collections with multiple very similar versions, as opposed to the reordering approach here which tries to exploit more moderate levels of similarity.
In future work, it would be very interesting to compare these di erent approaches on documents with di erent degrees of similarity.
For example, the alphabetical ordering used here could be easily extended to versioned collections (by sorting  rst by URL and then by version number), and could in fact be seen as providing an alternative e cient implementation of the approach in [5] that is based on merging consecutive postings in a list.
IR systems may assign docIDs to documents in a number of ways, e.g., at random, in the order they are crawled or indexed, or sometimes based on global measures of page quality (such as Pagerank [9]) that can enable faster query processing through early termination.
The document reordering approach in this paper and the previous work in [7, 23, 25, 6,
 optimize compression.
While this is a reasonable assumption for some systems, there are other cases where this is di cult or infeasible.
We now discuss two cases, distributed index structures, and tiering and early termination techniques.
Large-scale search engines typically partition their document collection over hundreds of nodes and then build a separate index on each node.
If the assignment of documents to nodes is done at random, then a local reordering of documents within a node might not give much bene t.
On the other hand, if pages are assigned to nodes based on a host-level assignment or alphabetical range-partitioning, then we would expect signi cant bene ts.
However, this might require changes in the architecture and could impact issues such as load balancing.
Document ordering is also complicated by the presence of tiering and other early termination mechanisms, which are widely used in current engines.
In a nutshell, these are techniques that avoid a full traversal of the inverted lists for most queries through careful index layout, which often involves some reordering of the documents.
In some approaches, such as a document-based tiering approach [21], or a partitioning of inverted lists into a small number of chunks [19, 16], reordering for better compression can be applied within each tier or chunk.
Other approaches may assign docIDs based on Pagerank [9] or other global document scores mined from the collection [20], or use a di erent ordering for each list [13]; in these cases our approach may not apply.
Query processing in state-of-the-art systems involves a number of phases such as query parsing, query rewriting, and the computation of complex, often machine-learned, ranking functions that may use hundred of features.
However, at the lower layer, all such systems rely on extremely fast access to an inverted index to achieve the required query throughput.
In particular, for each query the engine typically needs to traverse the inverted lists corresponding to the query terms in order to identify a limited set of promising documents that can then be more fully scored in a subsequent phase.
The challenge in this initial  ltering phase is that for large collections, the inverted lists for many commonly queried terms can get very long.
For example, for the TREC GOV2 collection of 25.2 million web pages used in this paper, on average each query involves lists with several million postings.
Current systems typically use a style of query processing called document-at-a-time (DAAT) query processing, where all inverted lists associated with a query are opened for reading and then traversed in an interleaved fashion.
This approach has several advantages: (a) it performs extremely well on the AND and WAND [10] style queries common in search engines, (b) it enables a very simple and e cient interface between query processing and the lower-level index decompression mechanism, and (c) it allows for additional performance gains through forward skips in the inverted lists, assuming that the postings in each list are organized into blocks of some small size that can be independently decompressed.
In our experiments, we use an optimized DAAT query processor developed in our group, and we organize each inverted list into blocks with a  xed number of postings.
We choose 128 postings as our default block size (shown to perform well, e.g., in [27]), and keep for each inverted list two separate arrays containing the last docID and size of each block in words in (almost) uncompressed form.
This allows skipping of blocks during query processing by searching in the array of last docIDs.
All decompression is performed in terms of blocks; to add another compression method to our query processor it su ces to supply a method for uncompressing the docIDs of a block, and one to uncompress the frequencies.
(A block consists of all 128 docIDs followed by all 128 frequency values.)
This design is highly useful in Section 7, where we use several compression techniques within the same index.
One interesting result of our experiments is that reordering of documents, in addition to improving compression, also speeds up index traversal in a DAAT query processor.
In particular, our query processor (with no changes in the software, and independent of compression method) performs more and larger forward skips during index access in the reordered case, and as a result decompresses less than half as many blocks per query as in the unordered case.
Note that this is related to, but di erent from, recent work in [8, 12] that shows how to choose an optimal set of forward pointers (basically, how to choose variable block boundaries) for each list based on an analysis of the query load.
Thus, we reorder documents while keeping block sizes constant, while [8, 12] modify block sizes while keeping the ordering constant; it would be interesting to see how the approaches work in combination, and whether the reordering could be improved by considering query loads.
For our experiments, we use the TREC GOV2 data set of 25.2 million web pages from the gov domain that is distributed by the US National Institute of Standards and Technology (NIST) and used in the annual TREC competitions.
This data is widely used for research in the IR community, thus allowing others to replicate our results.
It is based on a 2004 crawl of the gov domain, and is also accompanied by a set of 100000 queries (the 2006 E ciency Task Topics) that we use in our evaluation.
While the data set does not represent a complete snapshot of the gov domain at the time of the crawl, it nonetheless contains a fairly signi cant subset of it.
This is important since our techniques perform best on  dense  data sets such as GOV2 that are based on a fairly deep crawl of a subset of domains.
In contrast, a  sparse  set of 25.2 million pages crawled at random from the many billions of pages on the web would not bene t as much.
In this paper, we study the problem of optimizing compression and query processing performance given a suitable assignment of docIDs.
Previous work in [7, 23, 25, 6, 24] focused on  nding a good docID assignment, and then evaluated the assignment by compressing docIDs using standard techniques.
In contrast, we focus on how to best exploit a given assignment by optimizing compression and query processing techniques for this case.
Our compression codes are available at http://cis.poly.edu/westlab/.
Our main contributions are as follows: (1) We propose new versions of the PForDelta (PFD) approach and compare them with state-of-the-art techniques in the literature as well as new variants that are tuned for both speed and compression ratio.
Our experimental results show that our versions of PFD can achieve signi cant improvements in size and speed.
(2) We study the compression of frequency values under such assignments.
Previous work only considered do-cIDs, but we show that frequencies can also be compressed signi cantly better through suitable docID assignment.
Our main contribution here is the application of transformations inspired by move-to-front coding to improve the compressibility of frequency values.
(3) We study the impact of docID reordering on overall index size and query throughput on the TREC GOV2 data set of 25.2 million web pages.
We observe a reduction in minimum index size by about 50% over the case of a random docID ordering, resulting in a minimal size of about 3.45 GB for a full-text index of the entire collection.
We also show that the docID reordering leads to signi cant improvements in query throughput on conjunctive queries for document-at-a-time (DAAT) query processors by reducing the number of random seeks in the index, in addition to any bene ts obtained via the reduction in index size.
(4) The various compression techniques studied by us show a trade-o  between speed and compression ratio.
Thus, the techniques that achieve the smallest size are much slower than the fastest ones, which in turn result in a larger index size.
This motivates us to study hybrid index organizations that apply di erent compression schemes to di erent lists.
We set up a formal optimization problem and show that by selecting a suitable compression scheme for each list based on an analysis of a query log, we can simultaneously achieve almost optimal size and speed.
In this section, we perform a detailed study of compression techniques for docIDs.
In particular, we  rst study distributions of docIDs on TREC GOV2 data set, and then discuss state-of-the-art compression methods and propose our new algorithms, and  nally we evaluate all these methods through some preliminary experiments.
The performance of a compression method depends on the data distribution it is applied to.
For inverted index compression, compression is best when there are many small numbers.
The optimized assignment of docIDs is intended to increase the number of small numbers and thus improve compression performance.
In Figure 1, we show a histograms of d-gaps for the TREC GOV2 data set under three di erent orderings of documents: original, which we get from the o cial TREC GOV2 data set; sorted, where docIDs are reassigned by us after we sort their URLs, as in [24]; and random, where docIDs are assigned at random.
From Figure 1 we can see that the sorted ordering results in more small gaps than the other two kinds of indexes, suggesting a higher compression ratio.
In addition, the d-gaps for the original ordering have a similar histogram as those for the random ordering, suggesting that the compression methods will very likely have a similar performance.
Furthermore, we analyze individual inverted lists and  nd that such a reordering results in more clusters (not shown in the Figure 1), i.e., sequences of consecutive small d-gaps.
) s
 c o d ( s p a g f o e g a t n e c r e









 sorted original random





 Log2(gap) Figure 1: Histograms of d-gaps for inverted lists corresponding to 1000 random queries on the TREV GOV2 data set, under three di erent orderings: original, sorted and random.
The x-axis is the number of bits required to represent d-gaps in binary, and the y-axis is the percentage of such d-gaps.
(Thus, the  rst point is for 1-gaps, the second for 2-gaps, the third for 3-gaps plus 4-gaps.)
We now describe two modi cations to PFD that achieve signi cant improvements over the versions in [30, 14, 27].
Recall that the implementations of PFD in previous work encode a block of 128 value by  rst allocating 128 b-bit slots, and then for those 90% of the values less than 2b directly storing them in their corresponding slots.
For each value larger than 2b, called a exception, we store an o set value in the exception s corresponding slot indicating the distance from the current exception to the next one, and the actual value of the exception in some additional space after the 128 b-bit slots.
One disadvantage of such a code structure is that when two consecutive exceptions have a distance of more than 2b, we have to use more than one o set to represent the distance, by forcing additional exceptions in between these two exceptions.
We cannot solve this problem by simply increasing b since this would waste lots of bits on 90% of values; but if we decrease b more exceptions will be produced.
This means in particular that this version of PFD cannot pro tably use any values of b less than b = 3, but this case is very important in the reordered case.
To overcome this problem, we present a new code structure for PFD that stores the o set values and parts of the exceptions in two additional arrays.
In particular, for an exception, we store its lower b bits, instead of the o set to the next exception, in its corresponding b-bit slot, while we store the higher over ow bits and the o set in two separate arrays.
These two arrays can be further compressed by any compression method, and we  nd that S16 is particularly suitable for this.
We call this approach NewPFD.
Our second improvement is in the selection of the b value for each block.
As it turns out, selecting a constant threshold for the number of exceptions does not give the best tradeo  between size and speed.
Instead, we model the selection of the b for each block as an optimization problem similar to that in Section 7.
Thus, we initially assign the b with the smallest compressed size to each block, and then increase speed as desired by selecting a block that gives us the most time savings per increase in size, and change the b of that block.
We call this OptPFD.
We note here that for a given target speed, we can easily derive simple global rules about the choice of b, instead of running the iterative optimization above.
Thus this version can be very e ciently implemented even on very large collections.
We now present a few minor optimizations of some other methods that we used in our experimental evaluation.
GammaDi : This is a variation of Gamma coding that stores an integer x by encoding the unary part of the Gamma code (that is, 1 +  logx ) as the di erence between 1 +  logx  and the number of bits required to represent the average of all gaps in the list.
The motivation is that when docIDs are clustered, the di erences between d-gaps and their average gap may be smaller than the gaps.
S16-128: As S9 and S16 only have 9 or 16 possible cases for encoding numbers, sometimes they have to choose a wasteful case when a better one might exist.
Now suppose we have a sequence of numbers consisting mainly of small values.
In this case, a version of S16 called S16-128 can do slightly better by providing more cases for small numbers and fewer for larger numbers.
Optimized IPC: Recall that the key step of interpolative coding (IPC) is to encode a number x in the range < lo, hi >, where lo and hi are respectively the lowest and highest possible values of x.
The original IPC encodes the o set o = x lo using a b-bit number, where b =  r  and r = hi   lo + 1 is the number of possible values of the o set.
This wastes bits if r is not a power of 2.
We can do better by using a trick from Golomb coding to encode o as follows: If o < 2b   r, use b   1 bits to represent o, otherwise use b bits to represent o+2b  r.
(This technique was already described for IPC in [26].)
In addition, before we apply the above optimization, we transform the range of values in such a way that the shorter codes are applied to values in the middle of the range, since such values are more likely even in a highly clustered list.
Also, while IPC is usually considered as a list-oriented method, we apply it to blocks of a certain size.
As it turns out, this also improves compression if we choose a good block size.
In particular, block sizes of the form 2b   1 appear to work best, and thus we usually choose blocks of size 127.
We  rst describe our experimental setup, which we also use in later sections.
For the data set, we used the TREC GOV2 data set.
We then selected 1000 random queries from the supplied query logs; these queries contain 2171 unique terms.
All experiments were performed on a single core of a 2.66GHz Intel(R) Core(TM)2 Duo CPU with 8GB of memory.
sorted original random list-IPC w/o opt list-IPC block-IPC








 Table 1: Compressed size in MB/query for docIDs using a basic list-wise IPC (no optimizations), a list-wise version with optimizations enabled, and its block-wise version, under the original, sorted, and random orderings.
In Table 1, we compare the original IPC, which is list-wise, with its improved version with our various optimizations and its block-wise version with our optimizations, on the GOV2 data set under the original, sorted, and random orderings.
From Table 1, we can observe the following: First, all IPC algorithms work signi cantly better on the d-gaps under the sorted ordering than under the other two orderings; second, both list-wise and block-wise IPC with our optimizations are much better the original IPC, but block-wise IPC with our optimizations achieves the best compression.
) y r e u q

 / ( e z i
 d e s s e r p m o









 NewPFD OptPFD

 Decompression Speed (million ints/sec)


 Figure 2: Compressed size in MB/query versus decompression speed in million integers per second for docIDs, using PFD, NewPFD, and OptPFD under the sorted ordering.
The points from left to right for PFD and New-PFD correspond to the following percentages of exceptions: 5%, 8%, 10%, 20%, and 30%.
For OptPFD, the points correspond to di erent target speeds for the optimization and their corresponding sizes.
Compared to IPC, the main advantage of PFD is that decoding is very fast.
In Figure 2, we show the trade-o s between decompression speed and compressed size for PFD, NewPFD, and OptPFD as introduced above.
From Figure 2, we see that OptPFD can always achieve a much smaller compressed size for a given decoding speed than the other method.
Thus, choosing b not based on a global threshold on exceptions, but based on a global target speed, achieves a much better trade-o  than the naive global threshold used in PFD and NewPFD.
While OptPFD is still worse than IPC in terms of compressed size, decompression is much faster than for any version of IPC (as we will show later).
We also ran experiments under the original document ordering, and observed slightly smaller but still signi cant gains for OptPFD over PFD and NewPFD, while PFD and newPFD were overall similar in performance.
) y r e u q

 / ( e z i
 d e s s e r p m o






 sorted original random
 s 9 s 16 s 16-1 28 O ptP F D e ntro p y v ar-b yte m a Diff ric e V T g a m m a d elta ric e g a m Figure 3: Compressed size in MB/query for docIDs under the original, sorted, and random orderings.
In Figure 3, we compare the average compressed size per query of the docIDs for most of the state-of-the-art inverted index compression methods on the TREC GOV2 data set under the original, sorted, and random orderings.
For each data set, we show results of twelve compression methods: var-byte, S9, S16, S16-128, OptPFD, Delta coding, Gamma coding, GammaDi , Rice coding, a variant of Rice coding called RiceVT described in [26, 18] which essentially promotes the implicit probabilities of small gaps, the block-wise interpolative coding with our above optimizations, and entropy, which uses the global frequency distribution of the compressed integers.
For OptPFD, we chose a setting that minimizes the compressed size.
From Figure 3, we make the following observations: First, just as Figure 1 suggested, many compression methods can achieve a much better compression ratio on the d-gaps under the sorted ordering than under the other two orderings; second, all compression methods on d-gaps under the original ordering achieve similar performances with those under the random orderings; third, IPC achieves the best compression performance among all methods; fourth, OptPFD is quite competitive with all other methods (even with IPC, although it is slightly worse than IPC in terms of size).
One disadvantage of IPC is that its decompression is slow.
In contrast, all other methods to the left of the entropy method are fairly fast, and much faster than those further to the right.
In this section, we  rst discuss the e ect of docID reordering on frequencies, and then propose more e ective compression algorithms.
In particular, we show that reordered frequencies can be transformed in such a way that their entropy is lowered signi cantly, leading to better compression.
Frequency values by themselves are not changed at all by reordering, and thus reassigning docID by sorting URLs does not a ect the distribution of frequencies.
However, such an ordering results in more local clusters of similar values.
This can be shown by comparing the compressed size of context-sensitive and context-free methods.
The former methods, which include IPC, S9, S16, and OptPFD, encode batches of numbers, while the latter methods, such as gamma or delta coding, encode each number independently, resulting in no change in compression after reordering.
original random ) y r e u q

 / ( e z i
 d e s s e r p m o







 s9 s1 6 s1 6-1 28 O ptP F D e ntro p y v b yte m a Diff rice V T g a m m a d elta rice g a m Figure 4: Compressed size in MB/query for frequencies under the original, sorted, and random orderings.
In Figure 4, we display the compressed size of the frequency data under state-of-the-art compression methods on the TREC GOV2 data set, using original, sorted, and random orderings.
From Figure 4, we see exactly what we would expect: The context-sensitive methods (all methods to the left of entropy) get better compression results under the sorted ordering than under the other orderings, while the other methods get the same results under all three orderings.
We also notice that for the context-sensitive methods, compression under the original ordering has very similar performance with that under the random ordering.
As before, IPC achieves the best compression performance.
However, none of the existing methods takes advantage of the local clusters created by the sorted ordering to further reduce compressed size.
In the following, we show that under such an ordering, the context information of frequencies can be further exploited to reduce frequency values and thus signi cantly improve compression.
The basic idea is that we exploit the context information of frequencies to transform them into even smaller values, using one of the following two techniques: a version of Move-To-Front coding (MTF) [4], and a method we call Mostly-Likely-Next (MLN).
More precisely, we propose to perform a transformation on the frequency values before compressing them with other compressors.
Move-To-Front (MTF): The MTF [4] transform is used as an important part of Burrows-Wheeler transform-based compression [26].
Its basic idea is that, as long as a number has been seen lately, it will be represented by an index that is likely to be smaller than its own value, in a separate index array whose  rst element is always the number we just saw.
For example, given a list of numbers [5, 5, 5, 3, 2, 2], and assuming that all numbers are in the range [1,5], we keep a separate index array which is initialized as < 1, 2, 3, 4, 5 >.
We  rst encode the  rst number 5 as its index in the index array, which is the same as own value 5, and then move 5 to the front of the index array such that next time when we meet 5 again we will encode it as the index in the index array, which is 1, instead of the real value 5.
From then on, whenever we meet a value, we encode it as its index in the index array and move it to the front of the index array.
Therefore, the original list could be encoded as < 5, 1, 1, 4, 4, 1 >.
From the example we can see that MTF works well especially when there is a cluster of numbers of the same value.
We experimented with several MTF-based mechanisms for preprocessing frequency values.
While the basic MTF version achieved some bene ts, we found that other variants that do not directly teleport the last used element to the  rst slot in the array actually performed better.
In the end, methods that move the last used value from its current position i to a position such as i/2 or 2i/3 achieved overall best performance in our experiments.
We also note that MTF may slow down the speed of decompression, especially when the range of values is large, since we have to do exactly the same move-to-front operations for all numbers to be decoded.
Most-Likely-Next (MLN): An alternative called MLN is also used to transform numbers to smaller values, but can overcome some problems of MTF.
In a nutshell, MLN uses a table that stores for each value (within some limited range [0 .
.
.
Q   1]) which values are most likely to follow.
Thus, for Q = 16, MLN would rely on a 16   16 array, precomputed for each list, that lists in position (i, j) the (j + 1)th most likely value to follow a value of i. Conversely, when applying MLN, we replace each value with its rank in the array indexed by the value of its predecessor.
(For values   Q, no tranforma-tion is applied.)
Thus, MLN needs to store an array for each list.
However, in our experiments, MLN outperformed the best version of MTF in terms of both size and decompression speed.
Both MTF and MLN result in signi cant runs of 1 values in the transformed set of frequencies, since many frequency values under the ordered list are followed by more occurrences of the same value.
We start by comparing the performance of our PForDelta variants, PFD, NewPFD, and OptPFD, on frequency values under sorted document ordering.
The results are shown in Figure 5, where we see that again OptPFD signi cantly outperforms the other two versions in terms of the trade-o  between decoding speed and size.
NewPFD OptPFD







 Figure 5: Compressed size in MB/query versus decompression speed in million ints/sec for frequencies, using PFD, NewPFD, and OptPFD, under sorted ordering.
In Table 2, we compare the average compressed sizes of the frequency data per query on the GOV2 data set, under the original, sorted, and random orderings.
We use three versions each for list-oriented and block-oriented IPC: The best version from before, one with MTF, and one with MLN.
sorted


 list orig


 rand


 sorted


 block orig


 rand





 Table 2: Compressed size in MB/query for frequencies, under the original, sorted, and random orderings, using IPC, IPC with MTF, and IPC with MLN, for list and block-oriented methods.
From Table 2 we make the following observations: First, as with docIDs, IPC performs much better under sorted or-the block-wise versions always perform better than their list-wise counterparts; second, for frequencies under the sorted ordering, the versions with MTF and MLN are much better than the one without them; third, IPC with MLN slightly outperforms IPC with MTF.
block IPC s9 s16 s16-128 NewPFD OptPFD entropy var-byte rice gammaDi  riceVT gamma

 basic MTF MLN

































 Table 3: Compressed size in MB/query for frequencies under sorted document ordering.
Both MTF and MLN can also be applied to the other algorithms to get better compression ratios.
From Table 3, we observe the following: First, the entropy is greatly reduced by either MTF or MLN; second, all methods except var-byte improve over their basic versions, no matter whether they use MTF or MLN; third, MLN is usually better and never much worse than MTF.
We also tried MTF and MLN transformations of d-gaps for docIDs, but there was no bene t.
In previous sections, we studied the compression ratios of various techniques on random queries, but did not consider decompression speed, total index size, and query processing performance.
In this section, we study these issues in detail.
In the experiments, we used the optimized decompression methods from [27] for var-byte, Rice coding, S9, and S16, and S16-128, New-PFD with  xed threshold 10% for exceptions, OptPFD with minimum compressed size, and the best block-wise version of IPC.
(We did not try to implement optimized decompressors for gammaDi , riceVT, gamma, and delta coding, as these methods are known to be relatively slow.)
In Table 4 we give for each method the decoding speed in millions of integers decoded per second for three cases: Decompression of docIDs, and decompression of frequencies with and without MLN transformation.
We start out with decompression speed.
var-byte s9 s16 s16-128 NewPFD OptPFD rice
 docID freq















 freq-MLN







 Table 4: Decoding speeds in millions of integers decoded per second, for docIDs, frequencies, and frequencies with MLN transformation.
The results in Table 4 are overall not surprising.
NewPFD and OptPFD are the fastest techniques, though S9, S16, S16-
much slower.
Adding MLN slows down the faster methods signi cantly, but does not impact slow methods such as IPC much.
We note that additional increases in speed can be obtained for OptPFD by trading o  size versus speed.
Next, we look at total index size.
For this, we built block-wise compressed indexes for three methods that we believe provide the most interesting trade-o s between decompression speed and compressed size: IPC, NewPFD, and OptPFD.
We compare ordered and unordered indexes, and for ordered indexes we provide numbers both with and without MLN.
The results are shown in Table 5.
We see very signi cant improvements in index size through document reordering.
The best compression is obtained with IPC, using MLN for frequencies, which results in a total index size of around
 the smallest size under OptPFD, using sorted docID ordering and MLN for frequencies.
In fact, even without MLN (which as shown earlier slows down OPTPFD signi cantly) we can obtain an index size only slightly larger than 4 GB.
In contrast, NewPFD results in much larger index sizes, of 5.5 GB and more, showing the bene t of OptPFD over NewPFD.
We note that many other sizes between 4 GB and 5.5 GB can be obtained by trading o  size versus speed in OptPFD (though even the smallest size results in fairly fast decoding).
However, note that even NewPFD is much better than the best unordered results, and that all the ordered indexes can be completely held in main memory given realistic memory sizes of 4 to 6 GB.
sorted New Opt









 original



     New Opt





        





 docID freq total f+MLN total Table 5: Compressed index size in MB on the entire GOV2 data set, for IPC, NewPFD with 10% threshold on exceptions, and OptPFD optimized for minimal index size.
For the sorted case, we provide numbers for frequencies and total index sizes with and without MLN.
We also observe that the ratio of frequency data to do-cID data is much smaller than in our previous experiments.
The reason is that when looking at total index size, we include a large amount of data in shorter (but not very short) lists, while our query-based measurements are skewed towards longer lists.
In shorter lists, d-gaps are larger while frequency values tend to be smaller, compared to longer lists.
The bene ts of OptPFD over NewPFD for compressed size tend to be larger on these lists, particularly for frequencies.
Next, we look at query processing speed for intersection-based queries using BM25 ranking.
Table 6 shows query performance for an index compressed with OptPFD (but no MLN for frequencies) using ordered and unordered docID assignments, under the assumption that all index data is in main memory.
Somewhat surprisingly, the ordered index is about twice as fast as the unordered one!
Note that this is not due to savings in disk accesses, as all the data is in main memory, and also not due to changes in decompression speed, as the ordering has only a moderate impact on the speed of OptPFD.
Instead, as shown in Table 6, this is mainly due to the ordered index decoding much fewer blocks of data than the unordered one.
running time (ms/query) num of docIDs decoded (million/query) num of freqs decoded (million/query) sorted


 original


 Table 6: Running time and number of decoded docIDs and frequencies for OptPFD on the GOV2 data set.
ple and intuitive way.
Consider the shortest list in a query.
Under DAAT query processing, almost all of the shortest list will be decompressed, and most docIDs in this list will generate a lookup into the next longer list.
If the docIDs in the shortest list are clustered, then more of these lookups will hit the same block of the next longer list, while other blocks do not get hit at all and do not have to be decompressed.
(Informally, if we throw enough balls uniformly at random into n bins we will hit almost all bins, but if our throws are clustered in certain areas, then many more balls are needed to hit most bins.)
A formal analysis of this phenomenon is complicated dependencies between terms and queries with more than two terms, and we leave this for future work.
Finally, we also give query processing speeds for other compression methods, in particular IPC and NewPFD, with and without docID reordering.
Note that the number of decompressed blocks per query does not change, as all methods use the same block size.
As shown in Table 7, we also get significant improvements in query processing speed for the other methods by using ordered indexes.
However, the method achieving the best compression, IPC, is much slower than the others.
NewPFD is even faster than OptPFD, but as shown in Table 5, the index size is much larger.
Moreover, the same speed at lower index size could be obtained by trading size for speed within OptPFD (not shown here).
NewPFD OptPFD sorted


 original


 Table 7: Running times in ms per query for IPC (with MLN), NewPFD, and OptPFD.
In previous sections, we saw that using reordered index structures results in signi cant improvements in index size and query processing speed.
However, the best method in terms of size, IPC, which outperforms all other methods by a signi cant margin, is fairly slow and can decompress only about 50 million integers per second.
The fastest methods, PForDelta and its variants, i.e., PFD, NewPFD and OptPFD, are around 20 times faster, but produce a larger index (though the index size for PForDelta under reordered docIDs is still better than for the best method without reordering).
Thus, there is a trade-o  between size and speed.
This motivates the question of whether we can get a better trade-o  by combining di erent compression methods in the same index.
Our index setup can easily accomodate di erent compressors within the same index (or even the same list), as all compression is performed in a block-wise fashion.
Moreover, from studies on inverted index caching, we know that di erent parts of the index have very di erent access frequencies; e.g., in [27] more than 90% of all index accesses can be served from a cache of 30% of the index size.
Thus, we could exploit this highly skewed access pattern, by compressing frequently accessed inverted lists using a very fast method, and other lists using a slower method that gives better compression.
Our goal is for the resulting index to have both size and speed close to the best achieved by any method: Problem 1: Given a limit t on the average time per query, and a set of available compression methods, select for each inverted list a compression method such that the overall index size is minimized, while satisfying the time limit t.
Problem 1 : Given a limit t on the average time per query, a limit b on the amount of I/O bandwidth (in MB/s) available, a caching policy P that uses some main memory to cache index data, and a set of available compression methods, select for each inverted list a compression method such that the total amount of main memory needed for caching is minimized, while satisfying the limits on t and b.
In the  rst problem, we are looking at a main-memory resident index, and our goal is to minimize the amount of memory we need to provide, given a (feasible) time constraint.
Our hope is that by relaxing the time constraint very slightly versus the minimum, we can very substantially decrease the memory requirement.
The second problem looks at an index that is partially cached in memory (a very common setup in practice), and the goal is to minimize the amount of memory that needs to be provided for caching to assure that the available I/O-bandwidth does not become the main bottleneck of the system.
Note that the  rst problem is the special case of the second where b = 0, i.e., no disk access is allowed.
Also, there are obviously many other ways to set up these optimization problems, including duals of the above, or setups that model the search architecture in more detail.
Due to space constraints, we focus on Problem 1.
The problem is obviously NP-Complete due to its relationship to Bin Packing, but we would expect a very good approximation via simple greedy approaches in this case.
In particular, we take the following approach: (a) Select a su ciently large query trace.
For each available compression method, build an index and issue the queries against this index.
(b) For each inverted list Iw for a term w, and each compression method c, measure the following: (i) sc(w), the compressed size of list Iw under method c, and (ii) tc(w), the total amount of time spent decompressing the list using method c over the given query log.
(c) Initially, assign to each inverted list the compression method that gives the smallest size.
(d) Now repeatedly greedily select a list Iw and change its compression method to a faster but less space-e cient method, until the time constraint is satis ed.
In particular, in each step, choose the list Iw that minimizes (sc  (w)   s c(w))/(tc(w)   tc  ) over all w and all methods c  6= c where c is the compression method currently used for Iw.
In other words, choose the list and compression method that gives you the smallest increase in index size per time saved.1 We note that query processing time in our setup consists of the time for decompression and the times for other tasks such as intersection and score computation, and that the latter are independent of the compression methods used (since all methods use the same block size for decompression).
Thus, we can check if the time constraint is satis ed in (d) without reexecuting the query trace.
Also, for best results it is useful to treat the frequencies and docIDs of a list separately, as most queries decompress fewer frequency than docID data.
We implemented the above method, and ran it on 99000 of the 100000 queries on the TREC GOV2 data set, leaving the other 1000 for testing the performance of the resulting con guration.
In Figure 6, we show results for a hybrid index combining IPC and OptPFD.
As shown, while IPC requires about 29 ms per query, we can get less than 12 ms
 note the version of OptPFD that we used only minimizes compressed size, and that a better overall tradeo  than the one in the  gure could be achieved by selecting di erent settings for OptPFD.
(In fact, this hybrid index optimization problem motivated the optimization problem underlying the size/speed tradeo  for OptPFD in Figure 2.)
) s m ( d e e p
 y r e u










 Compressed Size (MB) Figure 6: Total index size in MB versus processing speed per query in milliseconds, for a hybrid index involving OptPFD and IPC.
The leftmost point is for pure IPC and the rightmost for pure OptPFD.
In this paper, we have studied compression and query processing in inverted indexes with optimized document ordering.
Previous work has focused on  nding document orderings that minimize index size under standard compression schemes.
In contrast, we focus on how to tune compression schemes and maximize query througput given a good ordering.
Our experimental results show signi cant bene ts in compressed index size and query throughput.
Our work motivates several interested open questions.
First, we showed that query processing bene ts from more e cient skipping in reordered indexes.
This was a natural side product of reordering, but additional improvements might be possible by combining reordering with the ideas in [8, 12] for selecting block boundaries in compressed indexes.
Second, there is an interesting relationship between compression of reordered indexes and e cient indexing of archival collections.
We are currently investigating how to apply the ideas in this paper to archival collections.
We are also looking at performance optimizations that allow faster decompression of interpolated codes, and at how to  nd document orderings that are better than the alphabetical ordering studied in [24] and used by us in this paper.
by NSF Grant IIS-0803605 and a grant from Google.
