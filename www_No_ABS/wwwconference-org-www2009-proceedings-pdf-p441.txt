Consider an internet advertising system tasked with showing contextual advertisements (ads) on a publisher s page when a user visits the page.
This so-called content-match system strives to choose and serve the best ads using a range of cues   the pro le of the user and the content on the publisher s page   to search the pool of available ads and retrieve one or more ads to display on the page.
The challenge is to accomplish this task as e ciently as possible, given hundreds of millions of available ads.
Since the click-through rate on contextual ads is lower than sponsored-search ads,  This work was done while the author was visiting Yahoo!
Research.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
the revenue per serving is lower, and this applies even more pressure on lowering the cost of serving contextual ads.
A natural way would be to cast the ad serving problem as nearest-neighbor search: treat the user pro le and the page content as vectors in a high-dimensional space, pre-process each ad to map it to the same space, and develop a notion of similarity between a user-page vector and an ad vector in this space.
At this juncture, it becomes important to draw a comparison between this scenario and the traditional keyword-based web search.
While both deal with a large amount of data, they di er at a fundamental level.
First, content-match has a higher volume of tra c for the simple reason that people browse more than they search.
Second, the latency requirements for content-match are stricter than for web search   the additional time to load the contextual ads should be barely noticeable when a user loads a publisher s page in her browser.
Third, web search engines manage latency by heavy use of caching, from caching the posting lists all the way to result set caching; this is crucial since the query distribution is heavy-tailed.
It is not clear if caching will particularly bene t a content-match system since the each user-page vector can be almost unique.
Fourth, inverted index, which forms the core of keyword-based web, is far more well-understood and scal-able data structure than those that exist for the more challenging nearest-neighbor search.
And  fth, the half-life of ads is much smaller than that of web pages and this adds to the complexity of building a scalable ad serving system.
On the other hand, the problem of selecting and serving contextual ads o ers the following  exibility: similar ads can be shown to similar users visiting similar pages.
In other words, the nearest-neighbor search need not be exact and it is acceptable to provide an approximate answer to the user-page vector.
There are two ways of exploiting this  exibility.
The  rst is to develop fast approximate nearest-neighbor search algorithms; however, it is not immediate if these algorithms will still be scalable in the vast world of ads.
The second way, inspired by the web search setting, is to develop caching policies that can judiciously take advantage of the freedom to provide an approximate answer; this is the focus of our work.
We study policies for similarity caching [6], where a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item.
Conventional caching (i.e., exact caching) is not very e ec-tive in such approximate nearest-neighbor search scenarios, as shown by our experiments on real data gathered from a large content match application, because: (a) user-page caching ine ective, and (b) each cache item is quite large (as explained in Section 4) limiting the size and coverage of the cache.
A moment of re ection distills the basic characteristic of the above scenario: achieving an e ciency accuracy trade-o .
Such tradeo s are quite common in many web applications.
We mention two such applications.
The  rst is personalized search.
Here, the goal is tailor the web search results to a user depending on her pro le.
While it is prohibitive to compute a per-user ranking for each query, it is yet reasonable to assume similar users can be shown similar search results.
The second application is in content-based image search, where it may su ce to show a cached image that is similar to a query image; independent of our work, Falchi et al.
[9] recently studied similarity caching in this context.
Our contributions.
In this paper we study similarity caching in content-match systems.
We study two objectives that dictate the e ciency accuracy tradeo .
In the  rst objective, a cache hit is said to occur if the similarity between two items is more than a pre-speci ed threshold.
Here, we propose generalizations of two well-known algorithms in the classical caching setting: least recently used (lru) and least frequently used (lfu).
Our generalizations go beyond just rede ning what a cache hit means.
They make full use of the wiggle room available by ensuring that the cached items cover the similarity space without much overlap.
In the second objective, there is a smooth tradeo  between the IO cost and utility of o ering an item similar (but not equal) to the requested item.
Here, we propose a simple randomized policy that generalizes both lru and lfu.
We conduct extensive experiments on data from a real content-match system.
Our experiments show that similarity caching can signi cantly improve the performance of content-match systems, without compromising much in terms of accuracy.
Inspired by the patterns observed in the data and the performance of the similarity caching algorithms, we propose a simple generative model for content-match requests.
This model captures two fundamental characteristics of content-match requests: long-range dependences (user-page visits have a heavy tail) and similarities (presence of many similar users and similar pages).
In this model, we give a theoretical upper bound on the performance of classical caching and a lower bound on the performance of similarity caching.
We compute the parameters of this model from the content-match data using the maximum-likelihood principle and show that the actual performance numbers agree with our theoretical estimates.
Organization.
The paper proceeds as follows.
Section 2 overviews the related work.
Sections 3 describes various similarity caching objectives and policies.
Section 4 contains the experimental evaluation.
Section 5 presents a simple request arrival model, whose parameters can be obtained from real data by maximum likelihood, and shows the quantitative gain of similarity caching over exact caching.
Section 6 contains concluding remarks.
Caching has long been recognized as a critical component in high performance applications.
Indeed, most applications utilize caching on many levels, and multilevel schemes have recently been studied [16].
The closest form of caching to this work is result caching.
In this framework the set of results for popular queries is fetched from the cache, rather than being recomputed every time the query is issued.
In web search applications result caching has been extensively studied, see, for example, [8, 14, 15, 17, 19]; and it is safe to say that all modern search engines use some form of result caching.
In the theoretical community, this kind of a caching problem serves as a cornerstone of competitive analysis theory.
The well known caching schemes, e.g.
LRU, LFU, have been analyzed and the limits on their performance are well understood [18].
At  rst glance, similarity caching may look like a variation on a clustering problem   the goal is to  nd a set of points that best represent the incoming queries.
What makes this problem di erent is the online component   while the clusters must remain relatively stable in order to save on the computation costs, the clusters must also evolve as the query distribution changes.
Capturing this trade-o  is at the heart of this work.
Independent of our work, Falchi et al [9] recently introduced a notion of a metric cache, and showed an increase in the hit rate when using a simple variant of LRU.
In this work, we derive adaptive caching strategies, demonstrate their e cacy on real datasets and prove a bound on the expected increase in performance due to similarity caching.
On the theoretical side, the problem can be formulated as a special case of the so-called Metrical Task Systems (MTS) problem, see [2] for a full description.
However, MTS is a much more general problem, and, as such, the known algorithms achieve very weak performance guarantees.
Recently Chierichetti et al. [6] provided the  rst theoretical analysis of similarity caching.
They show that the competitive ratio of any caching scheme depends crucially on the dimensional-ity of the space and complement their analysis with several algorithms for similarity caching when the points lie in Euclidean space.
While providing an interesting analysis, their results do not extend to the Jaccard similarity model that we consider in this work, nor do they consider the nearest-neighbor search problem that comes up in  nding the closest point in the cache.
Solving the nearest neighbor problem e ciently is a keystone of an e ective similarity caching strategy.
The problem has a rich history, see for example the survey by Indyk [12], and the references therein.
In this work we use Locality Sensitive Hashing [7, 13] to solve the nearest neighbor problem, and demonstrate its e ectiveness against exhaustive search.
We  rst brie y describe the basic principles behind caching.
To avoid ambiguity, we use exact caching to refer to the classical notion.
We then describe the similarity caching setup, including the objectives, policies, and practical considerations.
In our discussions, a request item p is always associated with a key-value pair hkey(p), val(p)i; the requested item is speci ed by key(p), and val(p) is returned as the result.
Typically, the size of the key is insigni cant compared to that of the value, i.e., |key(p)| (cid:28) |val(p)|.
In our content-match application for instance, the size of key is roughly

 An exact caching scheme works in the following way.
On cache is probed to check if it has p. If so, then this is called a hit and the cached item is used to serve the request.
If p is not found in the cache, then this is called a miss and p is brought into the cache from the disk.
If the cache is full, then an existing cached item is evicted to make space for p. The main objective in exact caching is to serve as many requests from the ( xed-size) cache as possible, i.e., maximize the number of cache hits or minimize the number of cache misses (disk accesses).
Thus, exact caching consists of two steps, namely, hit-or-miss determination and an eviction policy.
While the hit-or-miss determination is trivial in exact caching, the eviction policy can be realized in one of several ways.
We brie y recount two popular ways: least recently used (lru) and least frequently used (lfu).
The lru policy exploits the temporal locality in a request stream, i.e., recent requests are likely to be re-requested in the near future.
To implement this, lru associates with each item in the cache a reference time that denotes the most recent moment when this item was used to serve a request.
When needed, lru evicts the cache item with the least reference time.
The lfu policy keeps counts of how many times each cache item was hit in the past and evicts the least frequently used item when needed.
Several variants of lfu have been proposed to deal with its di erent shortcomings [1, 21].
For example, it may happen in lfu that certain items occur in a burst to accumulate such high frequency counts that they never get evicted from the cache.
Window-lfu deals with this by counting the frequency of each item within a recent  nite length time-window.
As discussed earlier, there are several web applications where the concept of exact caching can be relaxed, leading to similarity caching.
The goal now is to serve items with keys that are  similar  enough to the keys of the requested items with as few cache misses as possible; we assume a notion of similarity that can be de ned between the keys of the items.
There is a clear tradeo  between the similarity of o ered items to the requested items and the incurred (disk access) cost.
We revisit the caching basics in this context.
To begin with, the semantics of a hit-or-miss is not clear in similarity caching   how similar should two item keys be that we can declare a cache hit?
And operationally, how to check if a similar item indeed exists in the cache?
It is clearly expensive to perform an exhaustive search of the cache to look for the item most similar to the requested item.
Likewise, the rest of the caching steps can potentially exploit the  exibilities o ered by similarity caching.
For example, it might be bene cial to bring a requested item into the cache even for a cache hit (under similarity).
And, it might make sense to incrementally  reorganize  the cache to ensure that cached items are  well-separated  in the similarity space.
This takes full advantage of similarity and enables us to work with a smaller cache.
Objective.
More formally, we can de ne our similarity caching problem as follows: let b denote the average IO cost budget, i.e., b is the fraction of cache misses.
Let c(p) denote the (cached) item o ered by the caching policy for request p. Then for a given cache size, an IO budget B, and a list of requests (speci ed by keys), say P, the goal of the caching policy is to maximizeX p P util(sim(c(p), p)), subject to the IO cost being at most b.
Here sim(p, q)   [0, 1] denotes the similarity between keys of items p and q and util(s) denotes the utility of o ering an item of similarity s to the requested item.
Thus the objective depends on the similarity function sim( , ) and the utility function util( ).
Utility function.
The function util( ) o ers a knob to control the aforementioned tradeo  between the similarity of o ered and requested items and the IO cost.
For instance, if util(s) = 1 for s = 1 and 0 otherwise, then our problem formulation reduces to exact caching.
Another instantiation of util( ), which is more relaxed than exact caching, is when util(s) = 1 for s     and 0 otherwise, where     1 is a given threshold.
The resulting objective is called the threshold objective and we study this in detail in Section 3.3.
In general, util( ) does not have to be a threshold function; it can be any monotone function.
We call this the smooth objective and study this in Section 3.4.
As we will show later, the threshold and smooth objectives are su ciently di er-ent, and thus it makes sense to study them separately.
Let   > 0 be a given threshold.
Recall in the threshold objective we have util(s) = 1 for s     and 0 otherwise.
From this de nition, the hit-or-miss determination is straightforward: a cache hit is said to happen if and only if there is a cached item with similarity at least   to the requested item.
Note that this amounts to a nearest-neighbor search within the cache; in Section 3.5 we will discuss an e cient way to address this problem in practice.
More than one cached item can hit a requested item p; let C  (p) denote the set of such items.
Thus, a cache hit happens i  C  (p) 6=  .
Of all the items in C  (p), the one most similar to p is o ered by the policy, i.e., c(p) = arg maxq C  (p) sim(p, q).
It is clear that a cached item q can o er hits for all the requests that fall inside a ball B  (q) of radius   around q.
An e ective similarity caching policy should judiciously use these balls to  cover  the space of items.
We now generalize the eviction policies in exact caching to work in the similarity case.
We begin by generalizing lru policy as follows: on receiving a request p,  rst determine if it is a hit or miss by the above de nition.
If it is a hit, update the reference time of c(p).
If it is a miss, bring p to the cache, and evict the cached item with the oldest reference time (as in the vanilla lru).
We refer to this policy as sim-lru.
The lfu policy can also be generalized in the same way to derive sim-lfu.
Note that both sim-lru and sim-lfu have the following property: no two items in the cache are within similarity   of each other.
In other words, the cache does not contain any redundant items.
While sim-lru and sim-lfu policies ensure there is no redundant item in the cache, some redundancy can still creep in under these policies.
This can happen if the balls around each cached item signi cantly overlap.
Incremental re-clustering We give an extreme example to illustrate this.
Let the cache size be 1 and let   be a  xed similarity threshold.
Let and sim(b, c) =   .
Consider the following stream of requests: a, b, c, b, a, b, c, b, .
.
..
It is easy to see that under the sim-lru policy the cache contains either item a or c, and the cache miss rate is asymptotically 1/3.
However, if the policy were to cache b, then the miss rate is essentially zero, since b can be used to serve both a and c. This leads to the point we raised before: an astute policy would bring b into the cache even though the current cache item (either a or c) would have o ered a hit under similarity.
This is the underlying intuition behind our policies described next.
We explain our ideas using lru policy as an example, but the same ideas can be applied to lfu.
First, for each cached item p (i.e., both key(p) and val(p) are present in the cache), we store the keys of all the items served by p, i.e., keys of all items that fall in the ball B = B  (p).
Each ball has a representative, rep(B), which is initially p. Thus, the cache data structure is a set of balls and for each ball B, the key and value of a representative rep(B) and a set hst(B) of keys of past requests served by B.1 (Since |key(p)| (cid:28) |val(p)|, storing hst(B) is inexpensive from a practical point of view.)
Our goal is to tightly cluster the items inside the balls and minimize overlaps between the balls to avoid redundancy.
We will accomplish this by appropriately updating the representative in a ball.
The policy works as follows: on receiving a request p, we compute the similarity of the request item with the representative of each ball.
Say the closest representative is r = rep(B).
If sim(r, p) <   , then we fetch p from the If sim(r, p)     , then we use r disk to serve the request.
to serve the request.
We perform the following operation before serving the next request.
We  rst tentatively add p to hst(B).
We then check if any other item in hst(B) makes a better representative than the current representative r. To be a new representative, an item r0   hst(B) has to satisfy the following two properties: (1) sim(r0, p)     for each p   hst(B), and (2) it has the maximum sum of similarity to all the items
 p hst(B) q hst(B) in the ball, i.e.,
 r = arg max sim(p, q).
The  rst property ensures that r0 can be used to serve every item in the ball and the intuition behind the second property is that r0 should  cover  the items in its ball as e ciently as possible (i.e., it lies close to the center of the ball).
To update the representative of B, we  rst compute the total similarity score, as shown above, for every item in B.
Note that this computation can be performed incrementally to make its complexity linear in hst(B).
Let r0 be the item satisfying (2).
Then, one of following two scenarios can arise.
If r0 also satis es (1), then we update rep(B) = r0 and fetch val(r0) from the disk (this costs an IO operation).
If r0 does not satisfy (1), then we delete p from hst(B), form a new ball around p, with p as its representative; ball B is left unchanged.
This also costs an IO operation.
We refer to this policy by cls-lru.
In our earlier example, it is easy to check that b will be cached by cls-lru.
In Section 4 we demonstrate its performance improvement over sim-lru on a real dataset.
requests served by B for e ciency purposes.
So far we have studied similarity caching policies under the threshold objective.
In particular, we showed how sim-lru policy can allow redundancy in the cache and then proposed cls-lru to address this.
Next we focus on the smooth objective setting, i.e., util( ) is any monotone function.
First we discuss how the threshold-based policies of Section 3.3 can fall short for the smooth objective.
Observe that under the threshold objective, for a small  > 0, two cache hits with similarities   and   +  have the same utility, while under the smooth objective they do not.
Hence, when a threshold-based policy, say, sim-lru, is used for the smooth objective, if a request item p appears and has similarity   to a cached item, the policy will use this cached item to o er a hit.
In doing so the policy incurs a utility loss of 1   util(  ).
If p appears too many times, this loss can accumulate to become signi cant enough.
On the other hand, an alternative would be to bring p in the cache (incurring an IO cost), if we believe this can avoid the recurring utility loss.
Clearly, doing this is pro table only if item p is requested often enough.
Based on the above intuition, we propose our next randomized policy called rnd-lru.
(Again, we describe our ideas in terms of lru, but they apply to lfu as well.)
On receiving a request p, the rnd-lru policy  nds the cached item, say q, with the highest similarity to p. Then with probability  (1   util(sim(p, q))) the policy declares a cache miss and reads item p from the disk.
With the remaining probability it uses q to serve the request.
Here     [0, 1] is a parameter that controls the utility vs. IO tradeo .
Observe that if sim(p, q) is small, then p is likely to be declared a cache miss (as desired).
And if sim(p, q) is very high, then it is likely to be declared a hit.
But if p occurs often enough, even if sim(p, q) is very high, our randomized policy will declare it a cache miss at some point and hence p will make it to the cache.
Thus, this simple policy is e ective in trading o  recurring utility losses vs IO costs.
Note that there is little or no bookkeeping in this policy.
Our proposed caching policies (in Sections 3.3 and 3.4) assume each cache item to be of uniform cost and utility.
Extending it for nonuniform functions, as done for conventional caching in [5], is a part of our future work.
As discussed in Section 3.3, a hit-or-miss determination in similarity caching requires to solve a nearest-neighbor problem in high-dimensional space.
In other words, given an item p, we need to obtain the cached item q such that sim(p, q) is maximized, where the similarity function is de- ned on the space of the keys of the items.
To do this e ciently, we use locality sensitive hashing (LSH) [13]: we hash the keys of items using an LSH function with the property that keys are hashed to the same value if and only if they are similar according to sim( , ).
In our content-match application, we use weighted Jaccard measure for the similarity between two vector-valued keys x = (x1, .
.
.)
and y = i: (xi=yi=0) min(xi, yi)/ max(xi, yi).
It is well-known that min-wise independent hash functions can be used in this case [4].
(y1, .
.
.
): sim(x, y) =P In Section 4.4.3 we show how LSH signi cantly reduces the computation cost of hit-or-miss determination while degrading the performance negligibly.
In this section we perform an empirical evaluation of our similarity caching policies.
Then in Section 5 we analyze them theoretically.
We obtained a page request log from a content-match system.
The log contains a time-ordered sequence of more than eight million page requests.
For each page request, the system prepares a weighted feature vector taking the page content, site, and user information into account; this feature vector is used to determine the relevant ads for the page.
The set of relevant ads are the candidates for the system s  nal ad selection process, which takes various business and advertisers  constraints into account (of which some can be time-sensitive) in selecting ten or fewer ads that are eventually displayed on the page.
To select the relevant ads quickly at page request time, the system extracts features for ads and builds an inverted index to store them.
For each feature the index has a posting list containing the identi- ers of ads that have the feature.
To determine relevant ads for a page request, the system probes the index using the WAND algorithm [3], which has been designed to minimize the number of index operations required for answering long queries, as is typically the case for content-match systems.
We simulate a cache on this request log in our experiments.
Each item in the cache corresponds to a page request and contains the feature vector of the page request (this vector constitutes the key and is usually around one 1KB) and the 1000 most relevant ad candidates for the page (this set of ads constitutes the value and is usually in tens of MBs).2 In our caching policies, to compare the similarity between a new page request and a cached item, we apply the same weighted Jaccard similarity function that is used to match ads by the content-match system.
When a page is requested, we perform a lookup operation in the cache to search for it.
If the page is not found (i.e., a miss occurs), we probe the index to  nd the 1000 most relevant ad candidates for the page.
(In case of a hit, we get these candidates from the cache.)
Then, the  nal ad selection process is run on these candidates.
Hence, a cache hit saves us from probing the index, which is an expensive operation, in terms of time, because it requires accessing data from disk and seeking and examining hundreds of posting lists on average.
In this experiment we demonstrate why similarity caching is well suited for such content-match systems.
In particular, we show that the relevant ads for similar page requests have signi cant overlap.
Hence, on receiving a new page request, if we  nd a page request in the cache that is similar to it, the top 1000 relevant ads for this cached request can be used to serve the new request as well.
By doing so the content-match system can avoid probing the ad index and save a lot of computation/IO cost.
For this experiment we sampled 500K pairs of page requests from the request log.
Then for each pair, say hp, qi, we computed the similarity in page requests (sim(p, q)) and the Jaccard overlap in the top 1000 ads for them (ads(p, q)).
ads.
This causes redundancy and can be avoided by buiding an in-cache index; we leave this as part of our future work.
Figure 1: Relationship between similarity in page requests and Jaccard overlap of the sets of ads chosen for those requests.
Pairs with similar values of sim(p, q) are put together in a bucket.
In Figure 1 we show the result.
Each point in the  gure denotes a bucket where the x-coordinate is the average similarity score sim(p, q) and the y-coordinate is the average Jaccard overlap ads(p, q) of the pairs in the bucket.
As it is clear from the  gure, as the similarity score between two page requests increases, the overlap in the sets of relevant ads for them also increases.
In this experiment we show the advantage of similarity caching over exact caching in general (and not under any speci c caching policy).
We take the  rst 100K items from the request log and ask the following question: given a similarity threshold what is the minimum size of the cache such that all 100K page requests are covered by the items in the cache.
(The cache can contain items from this 100K log only.)
This is the well-known k-center problem that is NP-hard [10].
Hence, we use the farthest point heuristic, an approximation algorithm known for the k-center problem [11].
Under this heuristic, the cache is constructed one by one.
We start with a random item in the cache.
Then at each step, we select that item from the 100K items that has the minimum maximum similarity to the existing items in the cache and put it in the cache.
The algorithm stops when the minimum maximum similarity drops below the given threshold.
As shown in Figure 2, the required cache size increases as the similarity threshold   increases (  = 1 implies exact caching).
For instance, the required cache size is 80K when   = 1, but it drops by almost 50% to 40K when   = 0.8.
This shows that similarity caching allows a cache to o er a signi cant more number of hits in comparison to exact caching, independent of the caching policy.
Next we perform this comparison under speci c caching policies.
First we study threshold objective from Section 3.3.
Figure 3 shows the performance of sim-lru and sim-lfu
 size of the cache required to  cover  all 100K pages.
Figure 4: Performance of sim-lru policy with and without re-clustering on a cache of 1K size.
Figure 3: Performance of sim-lru and sim-lfu policies on a cache of 1K size.
policies for a 1K cache.
The x-axis denotes the similarity threshold   while the y-axis denotes the hit ratio.
Note that the hit ratio for sim-lru is roughly 7% for   = 1 (when sim-lru reduces to exact LRU policy), but it gets more than doubled when   is set to 0.88.
This shows how a little sacri- ce in similarity can dramatically increase the performance of the caching policies.
Both sim-lru and sim-lfu perform similarly in this experiment, so for brevity we explain the rest of the experiments in the context of LRU only.
Note that the focus of this paper is on studying similarity caching and not on  nding the best exact caching policy.
In Figure 4 we plot the sim-lru and cls-lru policies.
The size of the cache is kept to 1K.
Recall that cls-lru policy keeps the cached items tightly clustered together and avoids redundancy in the cache.
Hence, it is not surprising to see that it performs better than sim-lru.
As threshold   gets closer to 1, both policies start performing similar and approach the exact LRU policy.
Figure 5: Performance of sim-lru policy while using LSH for probing the cache, where s denotes the number of hash functions used in LSH.
(The size of the cache is kept to 1K.)
Next we measure how LSH a ects the performance and computation cost of our caching policies when it is used for hit-or-miss determination (as described in 3.5).
Figure 5 shows the performance of the sim-lru policy under LSH for a cache of 1K size.
Here, s denotes the number of hashes used in LSH (s = 0 implies no LSH).
It is clear that the performance degradation due to LSH is negligible.
In Figure 6 we show the computation cost of hit-or-miss determination.
In particular, we plot the average number of similarity score computations performed per request in hit-or-miss determination under sim-lru policy.
When LSH is not the used (s = 0), the policy does an exhaustive search of the cache and thus, incurs a 1K computation cost per request.
Observe how the computation cost goes down by a order of magnitude when LSH is used, for a negligible degradation in performance.
Now that we have con rmed that LSH does not signi -
performed per page request, on average, in hit-or-miss determination in sim-lru policy (for a 1K cache).
s denotes the number of hash functions used in LSH.
When LSH is not used (s=0), due to an exhaustive search of the cache the number of score computations per request is 1000.
cantly a ect the performance, we use LSH to test our policies on larger cache sizes.
In Figure 7 we show the performance of sim-lru and cls-lru policies for di erent cache sizes.
The number of hashes used in LSH, s, is set to 4 and similarity threshold   is set to 0.9.
Similar to Figure 4 cls-lru continues to perform better than sim-lru.
It is tempting to think that for large cache sizes, the cache can allow some redundancy and thus re-clustering may not be needed.
However, from Figure 7 we note that re-clustering is considerably bene cial even for large cache sizes.
To illustrate how exact caching is not e ective in our scenario, we performed experiments to  gure out that exact caching requires a cache size of more than 100K items to achieve the hit ratio of 30% (achieved by our cls-lru policy using a 20K cache).
Assuming a cache item to occupy 10MB of storage, a 100K-item cache requires 1TB of memory and is beyond the budget of most content-match systems.
In this experiment we compare the performance of our policies with an upper bound.
This allows us to see how good our policies are on the absolute scale.
We obtain the upper bound in the following way: for a request we check if the request has more than   similarity to any earlier request in the log.
If it has, then we call this request a hit, otherwise it is called a miss.
Clearly, no caching policy can do better than this (without using any extraneous information).
Figure 8 shows the upper-bound and the performance of sim-lru policy on a 1K and 10K size cache.
As it is clear from the  gure that our policies come quite close to this upper bound for a fairly modest cache size.
Next we study the smooth objective (of Section 3.4).
Recall from Section 3.2 that smooth objective involves a function util( ) that gives the utility of a hit for a similarity value.
For our content-match application, we can use Fig-Figure 7: Performance of sim-lru policy with and without re-clustering for di erent cache sizes for similarity threshold   = 0.9.
LSH is used to control the computation cost (s = 4).
Figure 8: Performance comparison of our policies with the upper bound.
ure 1 to derive this function.
In the  gure we showed the relationship between the similarity of two page requests and the overlap in their sets of relevant ads.
If we count utility in terms of this overlap, then a linear or a quadratic form of util( ) function appears meaningful.
Figure 9 shows the performance of sim-lru and rnd-lru policies for linear (i.e., util(s) = s) and quadratic (i.e., util(s) = s2) utility functions.
The x-axis denotes cost in terms of the fraction of page requests which were not answered from the cache (and required probing the ad index).
The y-axis is the average utility per request.
sim-lru has the threshold   parameter while rnd-lru has the   parameter to control the tradeo  between cost and utility.
Observe that both policies perform fairly similar under high cost budgets, but when the cost is set low, rnd-lru performs signi cantly better than sim-lru.
The reason behind it is that when cost is small, sim-lru policy operates under a loose similarity threshold   and o ers hit using the cached items that are quite far from the requested items.
In rnd-lru some of these requests get declared as cache
 cies for 1K cache for di erent cost budgets under linear and quadratic utility functions.
misses when they appear often enough, and hence they are brought from disk to the cache.
This leads to a better covering of the high-dimensional space of items, which results in a higher utility in turn.
In this experiment we compare the performance of our caching policies with static caching wherein the cache is not updated over time.
Typically, a static cache is constructed by mining previous request logs and keeping the most frequent items in the cache to hit as many future requests as possible.
Previous studies have shown that for certain Web applications static caching works well [2, 20], thereby motivating us to compare it with dynamic caching.
The comparison is done for di erent similarity thresholds.
For this experiment we took the  rst 2 million requests from the log.
We put the  rst 250K of these requests in the training set, while the rest were put into the test set.
For constructing a static cache given a similarity threshold, we employ the k-center algorithm on the training set and put the most populated k-centers in the cache.
This cache is then used to serve the test set (and not updated at any time).
Figure 10 compares the performance of static caching with dynamic caching (sim-lru to be speci c) on the test set.
We perform the comparison for similarity threshold   equal to 1 (which refers to exact caching) and   = 0.9.
As expected, both static and dynamic caching perform better when the similarity threshold   is lowered.
More importantly, we see that dynamic caching performs much better than static caching.
This shows that page requests for our content-match application have strong temporal patterns and dynamic caching is able to exploit that, unlike its counterpart which remains static, by de nition.
To explain the performance boost added by similarity caching, we propose a simple generative model for the data and then prove a lower bound on the performance of similarity caching vis-a-vis exact caching.
For simplicity, we deal with distances instead of similarities, where d(p, q) = 1   sim(p, q); we assume d( , ) is a metric.
Figure 10: Static vs. dynamic policies for exact and similarity caching.
Intuitively, the model can be described as noisy copying: with some probability each new arriving item is a fuzzy copy of one of the previous items.
Formally, the model is parametrized by four values: p,  ,  , and k. When a new item q arrives, we look at the past k items.
With probability p, the item q represents a perturbed copy of one of the previous k cached items.
More speci cally, with probability p , there is no perturbation, and q is an exact copy.
And with probability p(1    ), q is selected so that it is at distance d from a previous item, where d is distributed as N (0,  2)   (0, 1];3 this is the perturbed copy.
Finally, with probability 1   p, q is generated uniformly at random in space, i.e., the distance between q and any of the past k items is uniform in [0, 1].
The added advantage of similarity caching can be traced exactly to the fuzzy copy step.
For exact caching to perform well, we need   to be relatively high, otherwise the chance for a new item to be a cache hit would be prohibitively low.
Similarity caching, on the other hand, takes advantage of the perturbed copies   if the value of   is positive, then some of the incoming items would fall within the threshold a orded by similarity caching.
In fact, we can show formally the improvement on the hit rate due to similarity caching.
To validate the model, and test the degree of perturbed copies in our data, we ran a maximum likelihood test to obtain the best values for p,  , and   in the dataset.
For k = 1000, the best  t occurred at: p = 0.18,   = 0.2, and   = 0.4.
The exact copying parameter,   is relatively high, so it is not surprising that exact caching does yield a noticeable improvement in performance over no caching at all.
However, observe that the   parameter shows that there is room for similarity caching.
That is, of the items that are perturbed copies, more than 65% (one standard deviation) lie within a distance of 0.2 (similarity   0.8).
And it is these items that provide the extra performance boost over exact caching.
q 2
     interval (0, 1]; the pdf f (x) of this variable is f (x) = , for x   (0, 1] and f (x) = 0       erf` 1   exp`   x2 2 2 otherwise.
 /  
  
 Given the model above, we proceed to provide a bound on the extra performance a orded by similarity caching.
While the algorithms we analyze are not exactly those we experimented with, in some sense they represent the best possible scenario for exact caching and one of the worst scenarios for similarity caching.
The added bene t provided by real, as opposed to stylized (yet analyzable) algorithms only serves to increase the gap between exact and similarity caching.
An equivalent model.
It is easier to consider the following equivalent model, represented by a weighted tree, with items positioned at its nodes.
The (transitive closure of the) weighted tree will be our metric.
The model is as follows.
At time t = 1, we have a tree with a single node and an item on the node.
At time t > 1, an item x is chosen arbitrarily among those generated in the last k steps.
Let N be the node where x was placed.
Then, with probability p , the new item y is placed on N ; otherwise a new node N0 is added to tree as a child of N and the new item y is placed on N0.
The weight d of the edge (N, N0) is chosen uniformly at random in [0, 1] with probability 1 p and from N (0,  )   (0, 1], with probability p(1    ).
We  rst obtain a simple upper bound on the performance of exact caching.
Theorem 1.
The probability that exact caching incurs a cache hit is   p .
Proof.
Suppose that the new item y is about to be generated, using item x.
Each of the items in the cache c1, .
.
.
, ct will be at some distance to x, say d(x, ci) = di with d1   d2       dt.
Let j be the number of di s equal to 0, j = |{i | di = 0}|.
The probability that ci can be used to answer y, if d(ci, x) > 0, is 0 under exact caching.
On the other hand, note that the ci s having di = 0 are all equal to each other.
The probability that those can be used to answer x is p .
Let us now consider the following threshold objective similarity caching policy, parametrized by distance threshold     (0, 1] (i.e.,   = 1    where   is the similarity threshold).
For analysis purposes, we will assume the eviction policy to be a slight modi cation of fifo (instead of lru or lfu).
(1) The cache will store keys of the last k items, together with the items used to return the answer.
That is, for i = 1, .
.
.
, k, the i-th entry of the cache will contain the pair (xi, x0 i) where xi is the key of the requested item and x0 i is the answer we returned for this request; (we will always have i)    ).
We apply a fifo eviction policy: when a new d(xi, x0 item y comes in, we evict the oldest pair and add a new pair (y, y0) to the cache, where y0 is either x0 i for some i (when we have a cache hit) or a newly computed answer y0 (when we have a miss).
(2) To determine hit-or-miss when a new item y arrives, we search the cache for the closest cache item to y : xi  = arg minxi d(xi, y).
Then, a. if (d(y, xi ) > 0 and d(xi , x0 2 ) or (d(y, x0 answer for y) and add (y, y) to the cache, 2 ) or (d(y, xi ) > i ) >  ), we fault (computing the exact i ) >     b. else we add (y, x0 i ) to the cache, i) to (xi, y0) when d(xi, x0 (3) Finally, if we faulted and computed the exact answer for y, we try to  improve  each cache pair using y; that is, we i)   d(xi, y0).
update a pair (xi, x0 Note that in this algorithm we may choose to fault even when we did not have to, for example when the cached item and its answer are at a distance of more than  /2 away.
Even with these extra faults, we show that the hit ratio of this algorithm is much better than that of exact caching.
Theorem 2.
Suppose the cache has the same size k as the model.
Then, the hit ratio of sim-fifo with distance threshold     (0, 1], is 0@p  + (1   p)       1  23/2  

    21/2 + p(1    )  
 erf 2   erf   (1   o(1)) d(x, x0 otherwise.
Proof.
Recall that for each cache item (xi, x0 i)     i)    .
We call an item good if d(xi, x0 i) we have 2 and bad Let  e (exact) be the event that the distance generated by our generative model is 0,  c (close) be the event that the distance generated by our model is in`0,   the event that the distance is in`    , and  f (far) be

 the probability with which these events occur.
Then, qe = p , qf = 1   qc   qe, and Z  /2 qc = ((1   p) + p(1    )f (x)) dx
 = (1   p)  
 + p(1    ) erf erf     .
      1  23/2  21/2 Suppose the adversary chose an item xi (from the last k items) to generate the new item y, as dictated by our generative model.
Note that due to the FIFO eviction policy, (xi, x0 i) must be present in the cache.
Since d is a tree metric, the closest item to y will be xi.
If xi is a good element (i.e., i)     d(xi, x0 2 ), then: can answer y = x0 item, (y, x0   with probability qe item y is an exact copy of xi.
We i without faulting, and add a good i)     with probability qc we have a cache hit because d(y, x0 i) may be i)    .
The new pair (y, x0 i), to the cache.
d(y, xi) + d(xi, x0 either good or bad.
  with probability qf we will fault, and the new pair (y, y) must be good.
On the other hand, suppose that the adversary chose an item xi, that is a bad element to generate the new item y.
Then,   with probability qe, we have a cache hit.
And we add a bad item (y, x0 i) to the cache.
i)       with probability qc we fault, bringing the good element (y, y) into the cache.
In the update phase since the dis-2 we update (xi, x0 tance (xi, x0 i) to (xi, y), thereby decreasing the number of bad elements.
(Pair (xi, y) may get evicted afterwards by the FIFO policy if xi is the least recently used element).
  with probability qf we fault and add a new good ele-2 and d(xi, y)     ment (y, y) to the cache.
