Web Search engines are nowadays an intrinsic part of daily life of hundreds of million of people; they are also part of a major industry of this century.
Despite the massive user adoption of these services, the pro tability of the companies that power them and the maturity of the search sciences that underpin the whole, there is considerable room left for improvement.
Better understanding of user queries, and especially providing cheap, accurate topical categorization systems are necessary objectives for several reasons.
Firstly, end-users bene t from it, via the e cient use of the federated search paradigm, which will integrate and blend both traditional Web search results with so-called Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
vertical search results, whether coming from open, structured databases like Wikipedia or from a federation of closed sources (e.g.
partner content like specialized news feeds).
This approach enables the display of rich content into the search result page.
For example, this could mean for music related queries displaying artist news and videos, o ering track samples or videos.
From user surveys and analysis of click-through rates, we can deduce heightened user interest on such specialized query-speci c content.
Secondly, search engine businesses and search advertisers also bene t from it because topical categorization can help displaying more relevant sponsored search adverts.
With adequate categorization quality, the advertisers can then buy placements not directly attached to individual keywords, but rather to categories, thus enabling the leveraging of the long tail of rare queries [4].
Finally, general purpose query categorization can be seen as an enabler or facilitator for the design and implementation of additional advanced services, like blending or redirection between services and advanced query analysis.
The origin of this research is basically the need for an ef- cient way to implement a federated search paradigm into Web search, for a broad set of di erent markets.
To pursue this goal, we assumed very limited editorial resources were available and potentially a large number of di erent languages and cultures were to be covered, eliminating from the start any solution whose success would depend too much on editorial analysis or costly linguistic data.
As a consequence, we explored the following major research questions:   Can we design an unsupervised, yet e cient query categorization system ?
  Can we realistically implement such a system on a very large scale (all the Web being the target) ?
  Can we propose an innovative approach using only unstructured Web data to build the categorizer model ?
Traditional approaches include synonym based classi ers and supervised learning on (query, categories) pairs.
The  rst type of systems was simply unacceptable due to the cost constraint of such approaches.
The second type raised questions on the availability and cost of preparing training corpora for markets generating limited revenue, as is the case in many small countries.
This lead us to consider the unusual and di cult option of designing a novel, unsupervised approach.
However, a key constraint is that the categorization power must remain high, as errors in the selection of on the relevancy and user experience1.
The major contribution of this paper is reporting that an unsupervised learning paradigm can match the categorization power of supervised, costly approaches.
To our knowledge, it is the  rst reported system to work without training instances, be it already known queries or documents.
It must be further emphasized that we also prove in the evaluation that the proposed approach performs well on di erent taxonomies which di er by the granularity of their categories and without tuning any parameters, thus providing an o -the-shelf categorizer for di erent problems.
Subsequently, from a computational linguistics and machine learning point of view, we discovered that cross-reference concept graphs can summarize a massive amount of knowledge derived from unstructured data (query logs and Web documents) and, when analyzed with proper algorithms, provide a proper environment for query analysis.
In this paper we will describe our categorizer model (Section 2), evaluate it for query categorization (Section 3) and compare it against other published methods (Section 4).
The approach we present here is implemented as a part of what we call the Knowledge Based Search (KBS) platform.
KBS has several other usages including query sense disambiguation, query rewriting and search suggestions, all of which utilize the concept graph.
We will concentrate here on the aspects that play a role in query categorization.
Firstly, we will focus on the concept graph model that drives the system.
Then, we will fully describe the algorithms that implement query categorization in this setting.
In our implementation, there is only one type of node, which corresponds to concepts.
Concepts are represented as words or phrases.
Edges bear a weight corresponding to the degree of cross-reference (xref ) between the concepts.
xref is computed thanks to a search engine.
Using standard Information Retrieval notation, we can de ne the raw xref between two terms: xrefraw(t, t
 ) := T F (d, t
 ) (1)
 d Dt where T F (d, t0) is the term frequency of term t0 in document d and Dt is the set of top results from a search engine.
Basically, xrefraw is the sum of the term frequencies of term t0 in the result set of term t. T F is usually normalized to document length and associated with an IDF term that scales down the importance of frequent terms [15].
As our concepts may be represented by single words or phrases, it is thus not straightforward to normalize (1).
Using a non-normalized xref would be inconvenient as (1) wouldn t be 1think, for example, of a database of songs.
It is simple to  nd song titles that are also common words, so that if a query is misguided to the database, it is likely that it will return a hit.
In the music feeds we worked on, titles like  Love , play , etc appeared several dozen of times; additionally passing on unnecessary queries incurs an additional cost in terms of vertical search indexes  load [6] comparable between queries.
Moreover, computing an inverse document frequency on phrases can be costly, depending on the search engine implementation.
That s why we prefered to use a slightly di erent and simpler calculation which roughly estimates (1) while having the nice property of being normalized: xref (t, t
 )   1 |Dt| 1[t0 d] (2)
 d Dt which is the ratio of documents that contain term t0 in the result set of query t. This quantity has the advantage of being normalized by de nition and straightforward to compute given access to a subset of the result set on any search engine, while capturing the intuitive sense of (1).
Indeed, it can be viewed as a boolean model of (1) where each document confers only a binary weight to xref .
In fact, we omitted an important parameter in (2) : N , the number of results to take into account in Dt.
This parameter has a serious impact on the values of xref .
If we stick to N = 10 for example2, we will only capture cross-references that are obvious.
Conversely, setting N = 40 or more has the e ect of capturing more relations between terms, but with the drawback of adding noise.
Finally, Equation 3 fully de nes the xref that we use in our model : xrefN (t, t
 ) =

 1[t0 d] (3)
 d DN t We will assume from now on that N is set to a suitably low value that takes advantage of the ranking function of the search engine (the optimal value is likely to be implementation dependent, and thus less interesting in the context of this paper).
Using cross-reference as the primary metric to build a concept graph seems not to have been investigated before.
Previous work on automatically integrating unstructured data to build concept graphs traditionally use various formulations of co-occurrence rather than cross-reference to link concepts together.
The entity containment graph extracted from Wikipedia in [24] uses co-occurrence at the paragraph level to compute relationships between entities.
Another example of co-occurrence graph can be found in [17] where the authors conduct a detailed analysis of the statistical mechanics of a graph extracted from the Reuters-21578 news corpus.
Co-occurrence can be interpreted as the semantic proximity between concepts, whereas xref encode a di er-ent relationship.
We further investigate the properties of this relationship and its bene ts for categorization.
A  rst advantage of xref is that it allows to use Web query logs as a corpus to build the concept graph.
In fact, the use of xref is scalable with respect to the amount of data that we can process, since it requires only query level processing, a task for which search engines are typically optimized (e.g.
cache infrastructure).
Computing co-occurrence on a collection of the size of the web (or signi cant portions of it) would be prohibitive as it requires document level processing (for example counting the number of phrases in which two words occur together) for which search engines databases are not designed for.
the best performance and relevancy on the  rst result page, that s to say on the 10  rst documents bene t from the  nely tuned relevance model implemented in the ranking function, as well as the di erent spam and undesirable content  ltering.
We also put two additional constraints on the graph model.
First, it must be a Directed Acyclic Graph (DAG).
This is achieved by running a batch mode cycle remover process that removes only the weakest edge of a given cycle.
The acyclicity property is important in terms of computational e ciency as it simpli es the implementation of the graph storage and retrieval primitives.
It allows us not to bother implementing cycle detection in the graph traversal algorithms that we use at runtime.
Next, we only allow one edge between two given nodes.
This means that when building the graph we retain only one xref value between two nodes : even if xref (t, t0)   0 and xref (t0, t)   0 we will only create one edge between t and t0 and give it an orientation corresponding to the heaviest xref .
Namely, if xref (t, t0) > xref (t0, t) then the edge will be oriented like this: t   t0.
This edge orientation constraint has a very important e ect on the model, as we found it in uences greatly the graph topology.
At the statistical level, it modi es the out degree distribution by skewing the otherwise familiar power law distribution as can be seen in Figure 1.
As a consequence, we experienced on a 350000 nodes graph a maximum in-degree of   212 whereas maximum out-degree was no more than   28.
We believe that these constraints enforce a graph topology that tends to favor paths leading to well connected nodes and limits the paths going out of these well connected nodes.
We will now focus on the interpretation of the xref measure.
Based on our experience with the concept graph we will describe three main properties of xref .
like Movies, Cinema etc, whereas the reverse xref will be probably very low3.
When designing the whole system we made the assumption that this property holds in a vast majority of cases and will allow the system to provide good generalization power.
Indeed, we can deduce that applying property 1 to all concepts will result in the ability to link any speci c concept (e.g.
concepts representing a given query) to more generic concepts (e.g.
concepts representing categories).
This is of the uttermost interest since it allows us to rede ne the categorization task as generalizing the sense of a given query until it matches the sense of a category.
This idea has been implemented in the algorithms described later in Section 2.3.
The scope of this property is limited by two sister properties: Property 2.
The conceptual relation of genericity is web bound.
Indeed, when looking at Table 1, one easily notices that the resulting edge between Football and Sports would be Sports   F ootball, which con icts with the orientation most humans would choose consciously when presented with both alternatives.
Actually, it may be seen as an oddity, but we tend to think it is an important feature of the system because it re ects the reality of the web corpus.
In that sense it is a very di erent resource than editorially based taxonomies and ontologies.
Another example is the query  rose  : an ontology based system or a concept graph extracted from a non-web corpus will probably answer that the two main senses of the query are referring to a color and a  ower.
On a web search engine, you tend to have di erent meanings : the  ower and the rock band  Guns n Roses , due to the popularity of these two concepts on the web.
Property 1. xref encodes a Conceptual Genericity relation Property 3.
The conceptual relation of genericity is weakly transitive.
As genericity is a conceptual rather than computational notion [22], we assume there is no formal proof of this proposition.
However, we believe anyone used to searching the Web has an intuitive comprehension of this property.
For example, when querying for  www 2009 , it is likely several top ranked documents will contain concepts such as  world wide web conference  and  Madrid , whereas the reverse xref will be much lower, as  Madrid  is about so much other concepts than just  www 2009  (see Table 1).
To Madrid world wide web conference www2009 From www 2009 www 2009 Madrid Indiana Jones Movies Movies Football Sports Indiana Jones Sports Football xref






 Table 1: Examples of xref values Another empirical evidence of this property can be exhibited on movie names for example.
It is likely that most movie names will have a strong xref to some generic nodes At the graph level, when following a path, the weight of each edge plays an important role in making the genericity relation hold or not to the next node.
Of course, a weak edge in a given path downgrades the genericity relation between the start and end nodes of the path.
But if the path is too long, the transitivity won t hold either.
A funny evidence is that we found a path starting from  Madonna  and leading to both  A.
Einstein  and  Nuclear Physics  in no more than
 some were weak, but none of them was lower than 0.1.
This property had a deep in uence on the way we designed the algorithms that use the concept graph for categorization, as we will see later.
In the previous sections, we described our model as a concept graph but we didn t properly de ne what type of concepts we refer to, nor did we describe the way they are extracted.
Basically, we de ned xref on terms, without putting any constraint on the nature of the terms.
It would thus seem reasonable to compute xref on whole phrases as well as on single words.
As other studies [8][21] suggested that noun-phrases are easy to extract and that they are good
 pared to the size of the collection (in our case : the Web!)
(b) out-degree Figure 1: log2   log2 plot of degree distribution of a 350k nodes xref Concept Graph at capturing the sense of queries in the context of information retrieval, we use such concepts in our implementation.
Namely, we use prisma [1], a text mining technology that detects noun-phrases in text corpora.
Indeed, all nodes in our graph are  prisma concepts .
However, we do believe that any other concept or entity extraction technique could be used without any other loss in quality than the one induced by the precision of the concept extraction.
In our study, we set the system to extract the top-20 most salient concepts for each document.
Graph To leverage the concept graph for query categorization, we propose to focus on relevant portions of the graph at query-time.
These will then be mined to discover the nodes that best describe the query.
Finally, multiple methods could then be used to categorize the query using the most relevant nodes detected.
In the KBS system, we chose to directly integrate the categories as special nodes in the graph.
This approach has the advantage of providing very simple and dynamic means of maintaining the categorizer.
Adding, deleting or changing the scope of a category means then only adding, deleting or changing the linkage of the category node.
We use the Hook-Category (Algorithm 1) to integrate a given category into the graph.
The algorithm basically detects descriptors : nodes that best describe the target category.
These will be used in the query categorization algorithm.
The input category can be used alone, but it usually helps if some keywords (what we call seeds) describing the category are available.
For the federated search use case, we took at most 10 terms from the keywords that appear on the summary of the search result page when typing the label of the vertical.
For instance, for the  Health  vertical, we used {diet,  tness, longevity, disease, symptoms, treatments} as input because they were Algorithm 1 Hook-Category INPUT: a graph G(E, V ) + a list of seeds + a threshold  
 category list descriptors   {} for all keyword in seeds(category) do descriptors of a for the node   match(keyword) if node 6= null then descriptors   descriptors   node for all neighbor in succ(node)   pred(node) do if xref (node, neighbor)     and xref (neighbor, node)     then descriptors   descriptors   neighbor end if end for end if end for part of the snippet of the  rst result page when running the query  health .
For each seed, the algorithm seeks a corresponding node in the graph using a basic string matching primitive.
Each of these are promoted to descriptors.
Then, the algorithm tries to  nd other strongly related nodes in the neighborhood of each descriptor.
Those matching the condition will be promoted to descriptors too.
The threshold   was determined after manual reviews (we found that   = 0.5 produces a very small topical drift) and is mainly implementation dependent as it explicitly controls xref which in turn can be di erent from one search engine to another.
Table 2 shows a typical run of the Hook-Category Algorithm.
Having identi ed the descriptors, the category is then plugged into the graph by creating full strength links from all the descriptors.
To distinguish from other nodes, the type is set to  domain .
This may be seen in Figure 2, where the  Movies  domain is boxed and labeled  domain , receiv-labeled with a type/value/weight triplet, the weight being produced by the Categorize algorithm.
Edges are labeled with the xref value.
movies, cinema Input (seeds) Output (descriptors) movies, cinema,  lms, actor, dvd, showtimes, trailers, imdb Algorithm 2 Categorize INPUT: a graph G(E, V ) + a list of (terms,weight) pairs + a free parameter Niter OUTPUT: a weighted category list Table 2: Sample results of the Hook-Category Algorithm ing two incoming edges from  Films  and  Movies , whereas other nodes are labeled  prisma .
This is the very core of the KBS query categorizer.
Given a query at runtime, we take the most salient concepts of the N  rst documents in the result page and weight them with their frequency of occurrence in these documents according to Equation 4:
 d DN q for all v in V do weights[node]   0.0 end for for all (term, weight) in input do node   match(term) if node 6= null then weights[node]   weight end if end for for i = 0 to Niter do for all v in V do for all p in pred(v) do weights[v]   weights[v] + weights[p]   xref (p, v) end for end for weightinitial(concept) :=

 1[concept d] (4) end for return {domain   G} weighted by weights[domain] The number of documents to consider (N ) might not be the same as in Equation 3.
In our implementation we set N = 10 for performance reasons.
In an industrial implementation, we can reasonably expect the documents to be indexed with their top concepts.
The (concept, weightinitial) pairs are then used as input for the Categorize algorithm (Algorithm 2) to  nd the best category related to the query, if any.
This algorithm is similar to many graph mining algorithms that use random walks to detect salient nodes, except that it is limited to a fairly reduced number of steps.
We found reasonable values for Niter to be around 4.
Above this value we experienced a topical drift due to the weak transitivity of xref as explained in the previous sections.
Even if the weight propagation is limited in essence by xref being a normalized value in the [0, 1] range, we believe it is not reasonable to go further.
An example of the result of the Categorize algorithm can be viewed in Figure 3.
In this case, the query  spurs  has two possible categories:  Football  and  Basketball 4.
Ac-
team or to the  San Antonio Spurs  basketball team cording to KBS, the best is  Basketball  with a weight of
 interpreted as a degree of relation between the query and each domain.
We can also choose to keep only the heaviest in case we only want boolean categorization.
The score is also important to derive the con dence with which the system did the prediction.
In this section we evaluate our unsupervised query categorization system.
First we will give some details regarding the way we built the model.
Then we will report on relevancy evaluations.
Finally, we will assess the coverage on the KBS categorizer on a real world query stream.
For the purpose of this study, we built a concept graph using 2 million queries randomly sampled from Yahoo!
Search as a raw input.
These queries were processed using the prisma technology to extract the concepts in them.
After this step, we expanded our concept list by querying the search engine, once per concept, in order to retrieve addi-Football domains.
Some nodes are typed as  person  or  team  as a byproduct of the concept extraction, which can be ignored in this discussion.
tional ones.
This step multiplied by   3 the number of unique concepts in our list.
After these steps, we computed xref for all the concept pairs.
The whole process produced a graph of 2.30 million nodes and 6.74 million edges.
For relevancy evaluation, we report results on two di erent datasets.
The  rst one aims at testing KBS for the federated search use case.
The second one is the KDD Cup 2005 query classi cation task against which many proposed approaches have already been evaluated.
We give standard metrics for both evaluations : precision, recall and F1 for comparison with other published studies.
KBS was used as a binary categorizer in both experiments, retaining only the heaviest category output for each query.
This dataset was extracted from random queries sampled from Yahoo!
Search US query logs in February 2007.
Human evaluators removed navigational (mainly URLs or website names) and transactional (e.g.
shopping) queries, resulting in a reduced set of 3151 queries.
Professional editors were provided with a taxonomy that consisted of 9 categories corresponding to vertical, production search systems : Music, Travel, Finance, Movies, Jobs, Health, Games, Sports, Autos.
They manually classi ed each query into at most one category.
This process showed that a variety of queries representing 26% of the total couldn t be mapped to the given taxonomy.
The intended usage is that once a prediction is made for a given query, one of these vertical search systems is then queried for fresher, more focused and richer content than what can be found in the general index.
Thus, precision is of great importance for this application, as false positives might put undue stress on these systems, as well as display irrelevant elements on the search result page.
We compare our results to a baseline classi er that uses only word features.
It is based on Support Vector Machines (SVM) [10] and trained on half of the dataset using a radial basis function as kernel.
The training phase included optimization of the   kernel parameter and of the error penalty.
Testing was done on the other half of the corpus using a one-against-all voting strategy.
As SVMs are among the best text classi ers to date [5][15], the baseline outlines what can be achieved using only word features from the query.
Results are as follows : Metrics micro averaged Recall micro averaged Precision micro averaged F1 Baseline KBS Di .
(%)





 +24.6 +66.8 +42.8 Table 3: Global Relevancy Metrics on 3k random queries from Yahoo!
Search US Our system shows an improvement in F1 of 43% compared to state of the art methods for text categorization.
KBS  overall precision is above 0.80, which is an excellent score when compared to usually reported  gures (see for instance the KDD Cup evaluation where the most precise system yield around 0.75 precision), especially given the good level of recall : 0.63.
Please note again that these results were produced by an unsupervised learning scheme (thus not involving any learning on the queries themselves).
ticed that the worst confusion between categories was 3.9% between Sports and Autos.
As the Sports category includes all motor sports, this confusion seems reasonable.
Generic false negative patterns included (in order of importance) :   queries returning no results (including misspellings not corrected by the search engine.
E.g.
 chrisbenoi wick-kepida ).
  noise introduced during the extraction of concepts : sometimes the concepts returned are irrelevant or poorly weighted, one anecdotal concept being over-weighted.
  ambiguous queries for which KBS didn t choose the same best category as the editors.
E.g.
 radiology jobs  for which KBS ranked Health  rst, then Jobs.
Category Recall Precision Music Travel Finance Autos Movies Jobs Health Games Sports

















 Table 4: Detailed Relevancy Metrics Table 4 presents category-wise metrics.
When inspecting these metrics, please note that the lowest precision is above 0.68, an important fact for the federated search application.
Our second evaluation uses the KDD Cup 2005 dataset [13].
This dataset consists of 800 evaluated queries extracted from MSN Search in 2005 and manually classi ed into 67 categories by 3 human evaluators.
Several other papers between 2005 and today used it as a comparison point [20] [2].
We didn t tune the system in any way for the KDD Cup categories and thus evaluated KBS as an o -the-shelf cate-gorizer.
Indeed, we fed the Hook-Categories algorithm with only the category names split on ampersands and slashes (e.g.
Shopping/Bargains & Discounts 7  shopping, bargains, discounts), thus avoiding any third party keyword generation method and associated additional bias.
Best reported  gures during the competition and afterward are reported in Table 5, along with KBS performance for this evaluation.
System KDD Cup Winner [18] Best today [20]
 Precision






 Recall


 Table 5: Micro-averaged Relevancy Metrics on KDD Cup 2005.
Again, we report an outstanding F1 score, better than all systems that were engaged in the competition and matching the quality of the best reported approach as of today [20].
Please note again that KBS is subject to constraints unknown to all other reported approaches.
Namely, the system doesn t make use of the training corpus furnished with the dataset (1200 queries) and wasn t trained in whatsoever manner on the test queries.
Compared to the best reported approach, which uses third-party taxonomies, we emphasize again that our model doesn t make use of any external resource other than the search engine and its query logs.
In this evaluation, the number of categories makes the task harder for an unsupervised learning algorithm like KBS than for a supervised learning algorithm since the last one can often bene t from very precise information on category separation.
Failure patterns we discovered are as follows:   failure to  nd any matching nodes during Hook-Categories (20% of the queries)   failure to extract relevant concepts from the search engine results (8.7% of the queries) The  rst failure pattern is, of course, mainly due to the fact that we only used the category names as seeds and could be easily overcome by using a third party keyword expander.
One possible approach might be to leverage an existing document level taxonomy to extract additional concepts describing the categories.
But this option does not  t in the scope of this experiment as we wanted to evaluate KBS in the purest setting possible.
The second one is mostly due to the di culty of the task of  nding relevant concepts from documents in the result set.
Some categories were severely penalized by this pattern: Information/Local & Regional, Online Community/Homepages, Shopping/Bargains & Discounts, Online Community/People Search to cite a few.
The root cause is that these categories embody implicit rather than explicit characteristics.
For instance, very few (if any) web documents belonging to the Local & Regional categories have concepts in it saying it is local.
The concepts found were rather the names of the town or county and even if it would be possible, the Hook-Categories algorithm is a fairly bad solution to link all possible localities concepts to the category node.
To further assess the capability of KBS to provide query categorization on large portions of real world query streams, we evaluated its coverage (i.e. the ratio of queries on which KBS predicted that they belong to at least one category) using the KDD Cup categories on 500000 queries randomly sampled from Yahoo!
Search UK in September 2008.
81.01% of the queries in this data appeared only once, thus assessing the presence of long tail queries.
Table 6 show the categories that show up on more than 2% of the queries along with their frequencies.
We found out that 68.2% of the queries did trigger a category prediction.
This means that a fairly large portion of the query stream (including many rare queries) could potentially bene t from content not present in the main index.
We also expect the coverage to raise if we augment the size of the concept graph, for example by including concepts found in documents retrieved for uncovered queries.
Actually, the production version of KBS has this learning feedback enabled by default.
Living/Career & Jobs Entertainment/Movies Entertainment/Games & Toys Information/Law & Politics Living/Fashion & Apparel Information/Arts & Humanities Living/Real Estate Entertainment/Celebrities Information/Education Living/Travel & Vacation Living/Pets & Animals Living/Health & Fitness Entertainment/TV Shopping/Bargains & Discounts ...
Total Frequency (%)













 ...
Table 6: Frequency of KDD Cup Categories on 500k random queries from Yahoo!
Search UK (excerpt)

 Query categorization is one of the major branches of classi- cation on the trunk of modern information retrieval.
While sharing a common background with document classi cation (see for example the work in [7] where the authors explore the classi cation of Web documents into taxonomies), it has to deal with very speci c constraints unknown to other domains.
The major constraint is of course the length of the queries, which is 2-3 words on average, thus reducing the number of word features available to classi ers.
This is seen as one of the main reasons that explain the di culty of the task.
Another aspect of this task is that the categories used should help understanding the user intent when typing the query.
Actually, the topical classi cation, as treated in this paper, is just one dimension of the task.
Indeed, if we take the transactional, navigational, informational classes proposed by [3], these are orthogonal to the topical classes.
This is also the case for the  localness  in [9] or the task classes in [11].
Focusing on recent work on topical classi cation, we can identify the following trends :   few available datasets: to the best of our knowledge, there is only one publicly available, real world dataset for the Web queries categorization task : the KDD Cup 2005 described in [13].
It consists roughly of a
 only regret that the community doesn t have larger, more varied and recent benchmarks (4 years is a very long time period for rapidly evolving environments like the Web).
  using external knowledge: the winners and runners up [23][12][18][19] of the competition all agree on the importance of using external knowledge, an aspect also assessed by more recent works in their particular context [4][2].
We can only con rm this point of view as the concept graph built with query logs is the key part of our system.
  predominance of supervised learning: another major point of agreement is the use of supervised learning, be it as a sole learning mechanism [4][23] or combined with synonyms classi ers and exact lookup on dictionaries [2][18].
As stated in the introduction, dictionaries are costly resources that one can t necessarily a ord for speci c languages and cultures or markets.
In a sense, one might consider the Hook-Category algorithm as a far cousin of a synonym based classi er.
The major advantage of KBS compared to synonym based classi ers is that categories hooked in the graph can receive weight from a large variety of more spe-ci c concepts which results in a better generalization power.
  using document level classi ers as an intermediary step: Some of the KDD Cup participants, as well as more recent work [23][20] also rely on the availability of document level taxonomies that can be mapped to the target classes.
We can only stress the originality and advantage of our system : its independence from any linguistic or editorial resource, like existing training sets at the query or document level.
  search engine as a mandatory building block : in the setting of a commercial search engine, the only resource you can a ord  for free  is the search engine itself.
But even without considering resource constraints, quite all the reported systems use a search engine at one step or another.
As our own system relies heavily on Yahoo!
Search Technology, we believe that search engines are nowadays mandatory building blocks of a vast majority of advanced services and applications.
The best system engaged in the KDD Cup challenge [18] is based on the fusion of synonym-based and statistical, supervised classi ers plugged on 3 di erent search engines.
The statistical classi ers rely on the existence of document-level training data.
No evidence is given in the paper that the system could run with more limited resources, nor that the ensemble classi ers approach could be run at query time (that s to say with very tight time constraints).
The advantage of our approach is that we have proposed an e cient, scalable way of computing the model and a simple, e cient categorization algorithm that can be implemented in the runtime part of a search engine.
A recent study [4] reports the most impressive  gures so far on a non-disclosable dataset of rare queries.
The proposed approach is based on a document-level, supervised classi er that is run on the top results returned by a search engine.
While being similar in its use of the search engine, KBS has the advantage of being unsupervised and consequently to be independent of any training data.
In this paper we presented a novel, unsupervised, yet ef- cient approach for query categorization based on an automatically built concept graph.
We explored the properties of cross-reference as a powerful conceptual generalization method.
We evaluated the approach against both in-house and publicly available datasets, reporting metrics that show that KBS matches or outperforms other reported approaches on the given datasets.
We noted the quality of these results unsupervised, low cost and language agnostic system.
As a byproduct, we can also report that a version of KBS has been successfully deployed in production on Yahoo!
Search UK as a Federated Search enabler.
Indeed, each query made on the UK site triggers the KBS query categorizer, producing load peaks of 200+ queries per second with an average response time under 20 milliseconds.
This is maybe the clearest demonstration of the e ciency of our approach.
Practically speaking, it means that the query execution plan is modi ed according to the results of the categorizer to display di erent page layouts and rich content to the user.
Click-through rates analysis showed that the user engagement is good on these new features.
From a network sciences point of view, providing additional insights into the mechanics of the concept graph is part of our future investigations.
For instance, we would like to better understand the structure of cross-reference graphs, to which extent their topology can be constrained by the  one-edge  rule and which properties can be deduced from it.
Also, beyond the utility of the conceptual genericity property,  nding a way of reasoning on the concept graph as was proposed in the semantic networks world [16][14] is a challenging research direction.
We think it would be a much valued advance in future research if an approach could be proposed that combines ontologies or other semantic sources with a system like KBS that gives conceptual insights.
For example, consider the problem of using ontologies for information retrieval.
We might be interested, for instance, in the disambiguation of queries.
When an ontology detects that a given word is ambiguous, it is usually unable to rank the di erent meanings.
In this setting, we believe KBS could be of great help (see the  rose  case in section 2.1 for example), as it sums up knowledge found on the Web in the concept graph.
From an application point of view, we believe the concept graph is a powerful resource to be used for higher tasks in the  eld of Web information retrieval, for example in advanced analysis of queries, or in the ranking of query intents.
Indeed, it is likely that the future of Web search will be centered around the detection of user intent and a better comprehension of the task she is involved in.
An enlightening example is the query  blue suede shoes 5.
Without a proper query categorization solution, many intent detection systems would be eventually classifying it as a transac-tional/shopping intent, whereas KBS is already capable of adding a level of conceptual context by tagging it as belonging to the  Music  category.
If properly integrated into such a system, KBS would provide useful insight for the query intent detection task at a low cost and for all markets.
Finally, we conjecture that the concept graph we have described is an instance of a larger class of dimensional reduction techniques.
We lack space to fully develop our argument, but the fact that the concept graph organizes the concepts from most speci c to most generic can be seen as relatively similar to the principle of Principal Components Analysis, a technique traditionally used in statistical learning to reduce the number of dimensions in the dataset.
Indeed, the concept graph can be seen as the weighted result of a form of hierarchical clustering.
Each node in the graph
 can then be viewed as a summary of its predecessors.
In the  eld of Web information retrieval, where the traditional vector space model [15] often counts several millions of dimensions, we believe such a dimensional reduction technique could be very useful to reduce the complexity of a number of tasks like document clustering and classi cation for example.
Giving a formal proof of this conjecture and exploring the theoretical questions around it will open perspectives of future research.
