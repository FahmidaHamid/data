Many times a user s information need has some kind of geographic boundary associated with it.
For example, when the user issues the query  manhattan co ee , he probably wants information only about co ee shops in the Manhattan region of New York.
Previous research has shown that a signi cant portion (more than 13%) of web queries contain geographic (henceforth referred to as geo) information [9,
 in user queries for various retrieval tasks: we can person-Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
alize retrieval results based on the geo information in the query and improve a user s search experience; we can also provide better advertisement matching and deliver more information about local goods and services that users may be interested in.
Many researchers have demonstrated how to improve retrieval performance for a query by incorporating related geo information [2, 20] when this information explicitly appears in the query or is known beforehand.
However, recent research has found that only about 50% of queries with geo intent, i.e., queries where the users expected the results to be contained within some geographic radius, had explicit location names [19].
For example, many users search for  pizza  expecting the search engine to detect their location and correspondingly present results in their neighborhood automatically.
Therefore, identifying implicit geo intent and accurately discovering missing location information is important and necessary for using any retrieval model that leverages geo information.
We expect that in handheld devices like cellphones, the percentage of queries with implicit geo intent will be much higher.
In our work, we develop techniques to discover geo intention even when explicit geo information is missing, and further explore di erences between geo intent queries.
Towards this goal, we  rst address the challenging task of discovering a user s implicit geo intention at a  ne grained, i.e., city/location level.
Previous research has shown that a large portion (83.77%) of explicit geo queries contain city level in-formation[9], which implies that users often have a city level granularity in mind when issuing geo queries.
We therefore believe that  nding implicit city/location level information can greatly help satisfy users  speci c geo information needs, e.g.
a user who searches for  macy s parade hotel rooms  can receive a variety of information about hotels in New York City.
We then investigate di erent localization capabilities between geo intent queries.
For example, some queries may imply users  local geo information need, e.g.
the queries  pizza  or  dentist  typically imply that the user is looking for information in some limited radius around their current location, while other queries like  map  or  hotel  [9] may imply that the user is looking for information in a far o  location from their current one, often say while planning a trip.
If we can automatically detect a geo query where the location associated with the user intent is near that of the physical location of the user, the IP location (or GPS information if the user is using a mobile phone) of the user issuing this query can be used for searching and  ltering so that more locally relevant information is delivered.
In localization capability.
By the same measure, the queries  map  and  hotel , have geo intent, but no localization capability.
We also examine the fact that some queries that have localization capabilities may have a geographic region of relevance that is of smaller radius than others.
For example, users may be willing to drive up to only 10 miles for  pizza  but be willing to drive say up to 30 miles for a good  dentist  and up to 100 miles for a bargain on a  2008 honda civic .
For the convenience of description, we consider that an explicit geo intent query consists of (a) a location part: that explicitly helps identify the location and (b) a non-location part, e.g., in the query  pizza in 95054 , the term  95054  is the location part and the remaining terms, the non-location part.
Welch and Cho [19] have recently found that features derived from non-location parts of explicit geo queries in web search logs can help identify queries that have implicit geo intent.
Nevertheless, their work only considers di erentiat-ing geo queries (explicit and implicit) from non-geo intent queries and does not further investigate di erent levels of geo information and di erent localization capabilities.
Our techniques stem from ideas in language modeling [6, 11] which have been widely utilized for natural language pro cessing, speech recognition and information retrieval (IR).
Basically, we build geo language models at a  ne grained i.e., city level and extract n-gram language model features for discovering users  speci c implicit geo intent.
We also combine many other appropriate language and non-language geo features from  ned grained geo queries with n-gram features for better geo-intent discovery.
In order to be able to accurately train di erent language models for thousands of di erent cities and robustly extract geo features at  ne levels of granularity, we utilize a sample from a months worth of web search logs from a major search engine (Yahoo!)
which contains more than 2.8 billion search instances.
We  nd that our city language models are good at predicting the city pertinent to the query with very high accuracy.
Our chief contributions are (1) a method for identifying users  implicit city-level geo intent (2) a method for discriminating di erent localization capabilities of geo queries.
(3) a method for predicting the city corresponding to the geo-intent in a location-speci c query.
(4) Our models are learned from large amounts of click-through data and involve little supervision.
This allows us to quickly retrain models on fresh data, and adapt to seasonal and other variations, since the query logs are constantly evolving.
For example, we can quickly relearn the location for the  next red sox game .
Studying geo intent queries with the aim of  nding localization capabilities (city/location or a larger regional level) can help better understand users  underlying geo intent, thus allowing us to better customize search results for di erent users.
We begin by reviewing related work in  2, and then describe our geo intention analysis system in detail in  3 and  4.
We describe the experimental setup and the results of evaluating di erent components of our system in  5 and conclude in  6.
Although considerable work has been done on how to utilize geographic information in meta data for IR [1, 12], research on automatically detecting and understanding users  di erent geo intents in web search has just started.
In 2007, the GeoCLEF community began a geo query parsing and classi cation track [1], which required participants to not only extract location and non-location topic information of explicit geo queries but also required them to classify the topics into three prede ned subcategories: informational (e.g.
news, blogs), yellow pages (e.g.
restaurants, hospitals) and maps (e.g.
rivers, mountains).
Di erent from this track, our work aims at detecting users  implicit geo intent and classifying geo intent queries on the basis of di erent localization capabilities.
Welch and Cho s pilot study [19] shows that features extracted from non-location parts of explicit geo queries can help discriminate queries that have geo intent from those that don t.
Di erent from their work, we utilize more complex language modeling features for not only detecting users  implicit geo intent but also discovering the exact missing location information and understanding the localization capability of the geo information need.
Jones et al. [9] studied the relationship between the non-location part of an explicit geo query and the distance of the query s location part from the issuer s IP location and found that geo queries have varied distance distribution and therefore di erent localization capabilities.
We further use this distance between the IP and the city of intent in a geo-query to label geo queries into several subcategories and study the utility of language modeling features for discriminating between these categories.
Other research [22] considers using statistics from the IP locations of users who clicked a given query to study the query s localization capability.
Raghavan et al. [14] built language models from the contextual language around di erent name entities (e.g.
person, location, organization, etc) in a TREC corpus, and utilized these entity language models for linking, clustering and classifying di erent entities.
Pasca [10] utilized di erent contextual language patterns in the search logs to extract di erent types of name entities.
These works demonstrated the e ec-tiveness of using contextual features for categorizing entities.
In our work, we build language models for geo location entities from large scale web search logs, and investigate whether more complex contextual features can help discover users  speci c geo intent.
Besides using web search logs, some research [13] considers mining the returned web snippets from a commercial search engine to discover missing local information.
Other research [18] considers mining both top web search results and web search logs to disambiguate whether a query that contains a geo location name implies geo intent, e.g.
determining whether the query  New York Style cheesecake  is a geo query, and discovering locations related to implicit geo queries, e.g.
 nding  Seattle, WA  is related to the query  space needle .
These works complement our approach to better understand users  implicit speci c geo intent.
In this paper we focus on building models using city level geo information for detecting and discovering users  speci c geo intent.
The architecture of our geo intent analysis system is depicted in Figure 1.
Given a query Q = w1       wn, the system  rst determines whether explicit geo information exists: if yes, the system goes to the fourth step, which divides the query into city and non-city parts Q = (Qc, Qnc) and sends the non-city part Qnc back to the third component of the system for analyzing users  speci c geo intent; otherwise, the system goes to the second step to detect whether Use Classifier I to if Q has determine implicit city level geo intent.
 Find no explicit city  Analyze the specific city level geo intent in Q :
 determine if it has: a) Local geo intent b) Neighbor region geo intent c) None of the above
 entities in Q Input query: ( 1 wQ = nw ) city level geo info.
Use geo info.
to customize search results for users    Identify possible explicit  Analyze city/location info.
Figure 1: System Architecture for Discovering the User s Speci c Geo Intent the query has implicit city level geo intent by using the  rst level classi er.
If detected, the implicit city level geo intent is further analyzed in the third step.
The third component discriminates users  speci c geo intents within implicit geo queries.
Here we intuitively de ne three geo sub categories according to their di erent localization capabilities: (1) local geo queries, which consist of geo queries that imply a user s intention to  nd locally relevant information, e.g.
 pizza  or  dentist ; (2) neighbor region geo queries, which consist of geo queries that imply a user s intention to  nd related information from nearby regions, e.g.
 car dealer  or  real estate ; and (3) remaining geo queries that do not fall into the above three categories and are not easily localized, e.g.
 state maps  or  hotels .
By classifying geo queries into these sub categories, users  speci c geo information needs can be better satis ed: e.g., if the query is labeled as a  local geo query , related local information from or close to the user s IP location can be delivered.
We further use our city language models to predict cities in location-speci c queries.
These queries usually contain an entity (university, school, local media channel, doctor name etc) through which one can pinpoint a location (city/town level) corresponding to the geo-intent.
We  nd that if the query contains such an entity, the city language model is able to detect the city with very high accuracy.
If the query is labeled as a  location-speci c query , information from the city where the entity occurs can be retrieved.
The results from the third component are combined with other available information, e.g.
the user s IP location or an explicit city name in the query, to customize results for different users and improve information retrieval performance.
The geo location analysis tool used in the second step as a black-box for automatically identifying di erent levels of explicit geo information in queries has been used in several past papers [9, 15].
This tool utilizes both context-dependent (e.g.
 in , at ) and context-independent features to  nd possible location parts in a query, and maps these location parts to a large global location databases containing zip-codes, cities, counties, states, countries etc.
This tool calculates a con dence score in the range (0,1) for each location candidate identi ed in the query based on the con -dence of whether the candidate is indeed a geo location, and outputs all the possible locations and con dence scores.
We only consider location candidates whose con dence scores exceed 0.5.
In addition, so as to limit our scope, we only consider city location candidates that are in the United States.
Our major contribution is to design and evaluate the two components that analyze users  speci c geo intent, (enclosed in dashed lines in Figure 1).
In the next section, we describe how in each of these components language modeling techniques are employed to build city-level geo language models, and how rich geo language features at the city level are extracted for training the two classi ers.
We emphasize that studying  ne grained and complex geo language model features is necessary for this task that is more challenging than the task of identifying broad sense geo intent queries [19].
In this section we describe the set of features that were extracted from the search logs.
These features were used in the construction of the two classi ers in Figure 1 and are described in greater detail in  5.2 and  5.3.
First, for each query Q in the web search log, we correct possible spelling errors, remove any stopwords present in the INQUERY [4] stopword list1, and then utilize the geo location analysis tool [15] to identify every possible explicit city level geo query.
That is, we decompose Qcg as (Qc, Qnc), where Qc and Qnc denote the location/city and non-location part respectively.
This preprocessing step is similar to that employed by Welch and Cho [19] except for two main di erences: one is that we utilize the geo location analysis tool instead of a dictionary to identify the location part (Qc) in a query.
Since the tool uses contextual clues, it helps disambiguate whether a word like  reading  refers to the location or to the verb sense of  read .
This tool also covers zip-codes and many colloquial geo location names, e.g.
 nyc  for  New York City , that may appear in web queries.
Therefore using this tool has some advantages compared to the dictionary based approach of Welch and Cho [19].
The other main di erence is that instead of generating a group of base queries from each query by removing di erent levels of possible location names, we only generate one base query (from Qnc) by removing the location part (Qc) for further feature extraction.
We also do not apply stemming because research show removing stop-words has signi cant positive impact for geo intent analysis while stemming has little additional impact [19].
We then consider two di erent ways of extracting city level geo features for our modeling: the  rst is by building city language models by using all the identi ed non-location portions (Qcg) in the training data; the second is by viewing each unigram, bigram and trigram in the non-city part (Qnc) as a Geo Information Unit (GIU) that can help discover users  speci c geo intent.
We also collect various statistics of these GIUs.
We describe these two methods in the following two subsections.
City names often have strong co-occurrence statistics with terms or phrases like  map ,  hotel ,  hospital  and so on in
 version and use the remaining 414 stopwords.
the non-city parts (Qnc) that co-occur with a certain city name in the location part (Qc) can possibly help discover missing city information in an implicit geo intent query.
To build language models for each city, we go beyond the  bag of words  approach used in entity language models built by Raghavan et al [14] and instead follow a bigram language model approach.
The reason is that bigram information can be very important to infer implicit geo intent from phrases, for e.g., the words  time  and  square  individually may not imply geo intent, but the phrase  time square  has a high possibility of being related to New York City.
We do not build trigram language models because trigrams in web queries are much sparser than bigrams, making trigram language models not as robust as bigram language models.
In the typical bigram language modeling approach, the probability of a string is expressed as the product of the probabilities of the words that compose the string, where the probability of each word is conditioned on the identity of the previous word [6]; therefore, given a query Q = w1       wn, we have:
 n
 i=1 P (wi|wi 1
 )   n
 i=1 P (wi|wi 1), (1) where wj i denotes the string wi       wj .
Then, for each city Ck, we build bigram language models from the non-location portions (Qnc) of all the explicit geo intent queries (Qcg) that have the location portion (Qc) identi ed as the city Ck.
In this way, we can calculate the probability P (Q|Ck) of a query Q generated from a city Ck s language model by: P (Q|Ck) = n
 i=1 P (wi|wi 1
 , Ck)   n
 i=1 P (wi|wi 1, Ck).
(2) Researchers have proposed a broad range of smoothing techniques that adjust the maximum likelihood estimation (MLE) of parameters to solve the zero frequency problem in language modeling, and thereby produce more accurate estimations and predictions.
Many good comparison studies of di erent smoothing techniques can be found in the literature [6, 21].
Di erent smoothing techniques can have signi cantly di erent results.
In this study, for the estimation of bigram probability, we employ a state-of-the-art smoothing technique (method B in Chen and Goodman[6]), which combines two intuitions from the Dirichlet smoothing and Good-Turing smoothing: P (wi|wi 1, Ck) = #(wi i 1, Ck) +  P (wi|Ck) #(wi 1, Ck) +   ,   =     |VCk |, (3) where #(wj i , Ck) denotes the frequency counts of the string wj i in the non-city parts (Qnc) related to the city Ck, |VCk | denotes the vocabulary size of the words that appear in the city Ck s language model,   acts as the e ect of Dirichlet smoothing,   is a constant to control the degree of smoothing for di erent cities that have di erent vocabulary sizes.
For the unigram probability P (wi|Ck) in equation 3, we employ the standard Dirichlet smoothing: P (wi|Ck) = #(wi,Ck)+ P (wi|C ) = #(wi,Ck)+ #(wi,C )/#(w ,C ) , #(w ,Ck)+  #(w ,Ck )+  (4) where w  denotes all the words and C  denotes all the cities, e.g.
#(w , Ck) denotes the counts of all the words appearing Orlando q = Disney world ticket  P (Ci|Q) City Name




 New Castle San Antonio Kissimmee Anaheim q = Harvard University  City Name Cambridge Princeton Longwood P (Ci|Q)




 Boston Tuskegee Table 1: Top-5 cities and the city generation posteriors for two sample queries.
in the non-location parts of geo-intent queries (Qnc) related to the city Ck and #(w , C ) denotes the counts of all the words co-occurring with all the cities.
  is the Dirichlet smoothing parameter.
For the task of detecting the cities relevant to a location speci c query, we calculate the posterior probability of each query Q generated from a city Ci by: P (Ci|Q)   P (Ci)P (Q|Ci), (5) where we set the prior P (Ci) to be a uniform distribution, i.e.
the posterior calculation will be only a ected by the city generation probability P (Q|Ci), and not be biased towards those cities that appear most frequently in the query logs.
After calculating all the posteriors, we can sort them to discover the most probable cities that each implicit geo query Q may be generated from.
Table 1 shows the top-5 cities and the corresponding posteriors calculated by our city level language models, trained in experiments, for two sample queries:  Disney world ticket  and  Harvard University .
 New Castle  appears in the top cities related to the  rst query because of its ambiguous meaning   the geo analysis tool we used fails to determine whether it means a new palace in Disney or the city named  New Castle .
We evaluate city language models for this task later in the paper (refer  5.4).
These posteriors are useful as features to detect implicit city level geo intent; therefore, we use them as geo language model features for classi cation as well as for discovering the missing locations in the third component of Figure 1.
Intuitively, the unigrams, bigrams and trigrams in the non-city parts (Qnc) of explicit geo queries (Qcg) can help detect users  implicit geo intent, e.g.
the queries  golden gate bridge  or  shermen s wharf  may imply that users are interested in information about San Francisco.
Thus, we view each unigram, bigram and trigram in the non-location portions (Qnc) of all the geo-intent queries (Qcg) as a Geo Information Unit (GIU) that can help discover users  spe-ci c geo intent, and extract statistics in the training data for each information unit.
Then given any new input query Q, we  nd all the geo information units in this query and utilize them to generate a wide range of features for various classi cation tasks.
For each n-gram GIU wi+n 1 = wi       wi+n 1 appearing in the non-location part (Qnc)s of all geo-intent queries (Qcg), we calculate the following GIU features: i i   The frequency count of wi+n 1 in the set of queries, Qnc, , C ), and the appearing in ) = , C )/#g(ngrams), where #g(ngrams) denotes from all cities C , denoted as #(wi+n 1 MLE probability (Pg(wi+n 1 )) of wi+n 1 the n-grams of all the queries, Qnc #(wi+n 1 the number of n-grams in the set of all Qnc.
: Pg(wi+n 1 i i i i i   The frequency of wi+n 1 in all queries (including both geo and non-geo intent), denoted as #(wi+n 1 ), and the MLE probability of wi+n 1 appearing in the n-grams of all the queries: P (wi+n 1 ) = #(wi+n 1 )/#(ngrams), where #(ngrams) denotes the number of n-grams in all the queries.
i i i i   The pairwise mutual information (PMI) score [7] between and all city locations C : i wi+n 1 P M I(wi+n 1 i i i i i ) =


 P (wi+n 1 P (wi+n 1 Pg(wi+n 1 P (wi+n 1 )   The number of cities that co-occur with wi+n 1   The MLE probability P (wi+n 1 .
appearing in the n-grams of Qncs that co-occur with city Ck, calculated by: P (wi+n 1 (ngrams) , where #Ck (ngrams) denotes the number of n-grams in the Qncs that co-occur with city Ck.
|Ck) = #(wi+n 1 |Ck) of wi+n 1 i #Ck ,Ck ) i i i i   Given the MLE probability P (wi+n 1 |Ck) we calculate i i )   P (Ck)P (wi+n 1 the posterior: P (Ck|wi+n 1 we assume P (Ck) is a uniform distribution.
Then we  nd the city Cm that has the maximum posterior to generate wi+n 1 ) and the frequency counts #(wi+n 1 , Cm) as two more GIU features.
, and use P (Cm|wi+n 1 i i i |Ck), where i i )}, where N (wi+n 1   To measure the skewness of the posteriors {P (Ck|wi+n 1 ), k = 1,       , N (wi+n 1 ) denotes the number of cities that co-occur with the GIU, wi+n 1 , we calculate the KL divergence between the posteriors and a uniform distribution U (wi+n 1 ) and is computed by the following formula: N(wi+n 1 ) = 1/N (wi+n 1 ) i i i i i i
 k=1 P (Ck|wi+n 1 i ) log P (Ck|wi+n 1
 ) i ) i After calculating the above features for each GIU, given a new query Q, we can extract all the GIUs in it, and then either directly utilize the features of these GIUs to form a high dimensional sparse feature vector for representing this query, or aggregate some features to form a low dimensional feature vector in order to reduce the training cost.
For the high dimensional representation, each GIU feature from each textually di erent GIU occupies a di er-ent dimension in the feature vector.
For the low dimensional representation, we  rst aggregate features from the unigram GIUs, that is, for each of the GIU features we calculate the typical statistics like minimum, maximum, and average of the feature values from all the unigram GIUs and then keep each statistic in a di erent dimension in the feature vector.
We aggregate bigram and trigram GIUs in the same way and also keep calculated statistics in di erent feature dimensions.
We test both approaches in experiments.
We designed three experiments to evaluate the major parts of our system (enclosed in the dashed line in Figure 1) for discovering users  implicit speci c geo intent: (1) The  rst experiment is to evaluate how the  rst level classi er   Clas-si er I, in the second component in Figure 1, performs to detect users  implicit city level geo intent when no explicit city information is found in the query.
(2) The second experiment is to test how the well the second level classi er   Classi er II, in the third component in Figure 1, categorizes implicit geo queries into di erent localization capabilities.
(3) The third experiment is to investigate how well City Name Frequency in Frequency in geo sub training set geo sub testing set New York Los Angeles Chicago Houston Las Vegas









 Table 2: Statistics of top-5 most frequent cities in two geo query subsets.
our city language models detect location-speci c queries and discover missing city information.
In the next section we describe our data-set creation and feature extraction methodology before we move on to describe the evaluation of the di erent classi ers.
We utilize a large industrial-scale real-world web search log from Yahoo!
for this study.
The training set is a subset of the Yahoo!
web search log during May, 2008.
It contains about 2.13 billion rows of search instance records covering about 1.44 billion queries and related information, e.g.
users  IP and the clicked URLs.
The testing set is randomly sampled from the Yahoo!
web search log during June, 2008 and contains about 2.10 billion rows of search instance records covering about 1.42 billion queries and related information.
We applied the explicit geo information analysis tool described in  3 on both the training and the testing sets to identify each explicit geo query that contains a U.S. city location candidate with the con dence score larger than 0.5.
In this way, about 96.2M U.S. city level geo queries are iden-ti ed in the training set and extracted to form a geo sub training set, and about 96.7M U.S. city level geo queries are identi ed in the testing set and extracted to form a geo sub testing set.
We  nd 1614 distinct cities in the two geo query subsets.
Table 2 shows 5 most frequent cities in the geo sub training/testing set respectively.
We build city language models for each city as described in  4.1 by using all the explicit geo queries Qcg = (Qc, Qnc) in the geo sub training set.
Then given any implicit geo query Q, we can calculate a set of city generation posteriors P (Ci|Q) from the trained city language models, and use the posteriors as the geo language model features for classi cation.
In experiments we use the 10 largest posteriors of each query as features for simplicity and noise reduction.
We then utilize all of the original training set and the geo sub training set to extract GIU features for all the unigram, bigram and trigram GIUs that appear in the queries (Qnc) in the geo sub training set as described in  4.2.
In experiments, to reduce noise, we  lter any n-gram GIU (wi+n 1 ) that sat-is es the condition   min(Pg(wi+n 1 ), P (wi+n 1 ))   1  
 GIUs and 191802 trigram GIUs.
These GIUs are used for calculating both a low and a high dimensional representation for each query in later classi cation tasks.
i i i Next, we describe each experiment in detail, including how we generate positive and negative samples for each task, the classi ers used, and the evaluation results.
In this section we describe the details of how we build models and evaluate the classi er for city level geo-intent detection (refer Figure 1).
www.local.com travel.yahoo.com www.tripadvisor.com www.yellowbook.com www.city-data.com search-desc.ebay.com www.youtube.com www.amazon.com www.myspace.com www.nextag.com Table 3: Some DNs in DN+ or DN 
 Our automatic labeling method for this task utilizes URLs that have been frequently clicked for a query to automatically generate geo/non-geo intent labels for queries, instead of hiring human editors to make judgments.
For example, if many users repeatedly clicked the URL local.yahoo.com for a query, it has a high probability of having geo intent.
To  nd URLs that reliably imply users  geo intents, we consider only the domain name (DN) of the URL.
We collect
 geo sub training set to form the set DN1.
We also collect 100 DNs that are most frequently clicked from the other queries that are not in the geo sub training set but in the whole training set, into another set DN2.
Then we obtain the DN sets DN+ and DN  for labeling queries that may/may not have geo intent by:
 Some DNs that are intuitively useful for labeling users  geo intent and appear in both DN1 and DN2 end up being excluded from both DN+ and DN .
On analysis we found a few possible reasons for this.
For example, in the above process, the clicked URLs of possible implicit geo queries or larger regional level (state/country) geo queries are counted in DN2.
Similarly, the clicked URLs of some ambiguous queries where the black-box tool [15] falsely identi es city names are counted in DN1.
Therefore we introduce weak supervision into this domain name selection process by putting three useful DNs back to DN+ and two back to DN : DN+ = DN+   {www.citysearch.com, www.yellowpages.com, DN  = DN    {en.wikipedia.org, answers.yahoo.com} local.yahoo.com} In this way, we obtain 67 DNs in DN+ and 64 DNs in DN  respectively.
Some example DNs from the two sets are shown in Table 3.
For any query in the geo sub training set, if it has a clicked DN in DN+, we label the query as a positive sample.
For any query that is in the training set but not the geo sub training set, if it has a clicked DN in DN , we label the query as a negative sample or non-geo intent query.
We remove duplicates that have the same query terms and domain names.
After that, we obtain 7.5M positive and 57.8M negative samples.
We then use the location portion (Qc) of the positive samples as the labels and the non-location portion (Qnc) as the implicit geo intent queries.
Next, we randomly sample 20,000 implicit geo queries and 20,000 non-geo queries to obtain 40,000 queries in the training subset I.
For evaluation, we generate two testing subsets: testing subset I-1 and testing subset I-2 from the original testing set in two ways.
The  rst method is to follow the same above procedure: labeling positive samples only from queries in the geo sub testing set that have clicked DNs in DN+ and extracting Qncs as the implicit geo intent queries; labeling negative samples only from queries not in the geo sub testing that have clicked DNs in DN .
In this way, we obtain 8.0M implicit geo queries and 58.1M non-geo queries.
Then we randomly sample 80,000 queries ( half positive, half negative) as the testing subset I-1.
The second method di ers from the  rst in how it  nds the positive samples and creates the implicit geo queries.
In the second method, we directly label both positive and negative samples from the original testing set by only checking whether they have clicked DNs in DN+ or DN .
We use the black-box tool [15] to  nd and remove all the possible location portions (place names, zip-codes etc) in the positive and negative samples.
Then we remove the duplicates.
In this way, we obtain 31.3M positive samples and 53.2M negative samples.
Then we randomly sample 80,000 queries ( half positive, half negative) as the testing subset I-2.
Note that classifying testing subset I-2 is more representative of the true query log, and possibly harder, because positive samples are directly obtained from the original testing set instead of only from the geo sub testing set.
Testing subset I-2 may contain some real implicit geo queries instead of only the queries (Qnc) from explicit geo queries as in testing subset I-1.
We evaluate three state-of-the-art classi cation techniques: Support Vector Machines (SVM) [5], gradient boosted decision tree [8] and multinomial logistic regression (MLGR) [3] for building the  rst level classi er.
For the SVM, we employed linear kernel (SVM-Linear) as well as nonlinear RBF gaussian kernel (SVM-RBF).
Training SVM-linear typically costs much less time than training SVM-RBF, while SVM-RBF usually performs better when the original input feature space is low dimensional.
Decision trees have the advantage that they can learn conjunctions of features.
For the gradient boosted decision trees, we used the TreeNet tool by Salford Systems2.
For the MLGR, we utilized the open source R Project and its nnet library3.
For each labeled query sample, we calculate the geo language model features   top-10 city generation posteriors, and the GIU features (low/high dimensional feature vectors), then combine them for classi cation in two ways: 1) a low training cost way, which only uses the posteriors and the low dimensional GIU features, and 2) a high training cost way, which uses all the features that include the high dimensional GIU features in addition.
Then we separately scale each feature dimension to be in the range [0,1] for all the samples, and train the classi er based on di erent models with the data in the training subset I.
We employ 5-fold cross validation to select the model parameters that achieve the highest average accuracy.
Then we test the optimized classi er on both the testing subset I-1 and I-2.
Performance is evaluated by using the typical precision, recall and accuracy metrics: precision measures the percentage of true positive samples (true geo intent queries) in the queries labeled by the classi er to be positive (have geo intent); recall measures the fraction of the true positive samples detected by the classi er in all the true positive samples; accuracy measures the percentage of the correct labels, including both positive and negative ones, in the test set.
In this task, low precision will hurt users  search experience more than low recall or low accuracy.
Thus a classi er for 2http://salford-systems.com/ 3http://www.r-project.org/ Testing subset I-2 low dimensional features Acc

 all features low dimensional features all features

 Acc

 Acc

 Acc SVM-linear
 Treenet






 \ \ \ \ \ \ \ \ \ \ \ \ Table 4: Performances of discovering users  implicit city level geo intent on the testing subset I-1 and I-2 by using di erent classi cation techniques and two sets of features.
Precision, Recall and Accuracy are denoted by P, R and Acc, respectively.
this task in a practical system should have high precision and reasonably good accuracy and recall.
The evaluation results are shown in Table 4.
We did not test the performances of training MLGR and Treenet with all features due to the high training cost.
Results on the testing subset I-1 show that : (1) all the classi ers perform well by only using the low dimensional features (posteriors + low dimensional GIU features) with precision, recall and accuracy values above 89%, 82% and 87% respectively.
(2) using all features can further improve precision while recall drops about 14% and accuracy drops about 5%.
(3) SVM-linear achieves the highest precision on both feature sets; Treenet achieves the highest recall by only using low dimensional features; SVM-RBF achieves the highest accuracy by only using low dimensional features.
This result is expected since linear classi ers do better with an increase in the number of features in the presence of su cient training data.
Results on the testing subset I-2, the harder task, show that : (1) all the classi ers still perform reasonably well and achieve precision values higher than 80% when only using low dimensional features except Treenet which has a precision of 78%.
(2) using all features can improve all the metrics and achieve high precision and reasonably good accuracy.
On both testing subsets, we achieve both high precision, which is important for users  satisfaction and good accuracy although recall drops for the hard task.
Thus, the geo city language model features and GIU features can be used for e ectively discovering users  implicit city level geo intent.
As we know, the same web query can be issued by di er-ent users at di erent time.
Thus the web log samples from two di erent months may have considerable amount of the identical queries.
We do an overlap analysis in order to better understand our evaluation results.
We  nd that in the
 67% of the queries have appeared in the geo sub training set (from the May s sample).
There are 28.9M and 29.2M distinct queries (Qnc) in the geo sub training and testing sets, respectively.
We  nd about 48.06% of these distinct queries (Qnc) of the geo sub testing set have appeared in the geo sub training set.
The overlap also reveals that many geo language patterns found in old web query logs can be reused because many geo queries appear repeatedly.
This process of splitting the training and test sets by time is a common procedure in domains where the data occurs as a time series
 that our models can generalize well for new queries as well.
As shown in Figure 1, when Classi er I has detected an implicit city level geo intent query, the query will be passed
 to the second level classi er   Classi er II for analyzing the query s capability of being localized to the issuer s IP location.
In this section, we describe the details of how we build and evaluate Classi er II.
We consider three prede ned categories: Local Geo queries or LG, Neighbor Region geo queries or NRG, and remaining geo queries or RG, in  3 for this analysis.
Our low cost training set generation technique again utilizes the geo sub training/testing sets where the non-city part (Qnc) in the original data is used to create an implicit geo intent query and the location part (Qc) is the city level label corresponding to the geo intent.
We then use the information of distance L between the city level label (Qc) and the issuer s IP location to generate one of the above three subcategory labels for each query.
To better understand the distribution of the distance L in the geo queries, we divide L into 12 intervals and calculate the number of the geo queries in the geo sub training set (before and after we remove the duplicates) with distance values in each interval.
The results are shown in Figure 2.
It can be seen that a signi cant portion of geo queries can be localized to less than 50 miles from their issuers  IP locations.
A relatively small portion of geo queries can be localized to a 50-100 miles radius from the issuers  IP locations.
Many geo queries can hardly be localized.
For representing this di erence, we generate a geo sub category label for each query Q by  rst collecting the distances, L = {L1...Ln}, (note that the same query may be issued by users with different IPs) and then calculate the median of these distances (Lm = median(L)) and assigning Q to LG, NRG or RG if Lm < 50, 50   Lm < 100, or Lm   100 (unit:miles), respectively.
We generate a geo sub category label for each implicit geo query (Qnc) in the geo sub training set, remove duplicates and then randomly sample 15K implicit geo queries from each of the three geo sub categories to form training subset II.
We then process the geo sub testing set in the same way to obtain the testing subset II.
To investigate the utility of our geo features for discriminating between di erent geo sub categories, we design four classi cation tasks : task A   to discriminate between queries in LG and RG; task B   to discriminate between queries in LG and NRG; task C   to discriminate between queries in NRG and RG; task D   to simultaneously discriminate between queries in all three categories.
In this experiment, we again use SVM [5], Treenet and MLGR [3], described in  5.2.2, for building Classi er II.
The classi er uses the same geo features, including the top-10 city generation posteriors from the city language model and the GIU features, for the four classi cation tasks.
We
 ( y c n e u q e r







 <

 -


-



-



-



-




-

    	 


 -





-


k
 -








k
 -k
 e r o

 <

 -


-



-



-



-




-

     	 


 -





-


k
 -


k
 -k
 e r o
 Figure 2: Distributions of the distance L between the city Qc in the query and the issuer  IP location.
X axis denotes the distance intervals used (less than 5 miles, 5-10 miles, etc), Y axis denotes the number of geo queries (unit: million) in each interval.
Left/Right graph shows L s distribution in the geo sub training set before/after we remove the duplicates respectively.
Task B Task A
 low dimensional features Task C Task D All-3 SVM-linear
 Treenet
















 SVM-linear








 all features Table 5: Accuracies of discriminating implicit geo queries  di erent localization capabilities to issuers  IP locations by using di erent classi cation techniques and two sets of features for each of four clas-si cation tasks on the testing subset II.
also test the low and high training cost methods of using geo features as described in  5.2.2.
We separately scale each feature dimension to be in the range [0,1] for all the samples, and train the classi er based on di erent models using data in training subset II for each of the four tasks.
We employ 5-fold cross validation to select model parameters that achieve the highest average accuracy for each task and test the optimized classi er on the testing subset II.
Performance is evaluated by using accuracy as a metric.
Note that tasks A, B and C are binary classi cation tasks involving two labels while task D is a three-category classi cation task with three labels.
The results are shown in Table 5.
Again when using all features, we only test the performances of training SVM-linear and SVM-RBF.
We make the following observations: (1) Using low dimensional features (top-10 city generation posteriors + aggregate GIU features), the model cannot easily discriminate the subtle di erences between LG (local geo queries) and NRG (neighbor region geo queries), but can di erentiate between LG and RG (not local or neighbor region geo queries), and between NRG and RG, with reasonable accuracy (62.8% by Treenet and 61.8% by SVM-RBF) (2) Using high dimensional features greatly improves the accuracy to more than
 categories simultaneously (task D).
This means that di er-ent geo sub categories indeed have di erent GIUs and GIU features.
(3) Using all features in a nonlinear model like SVM-RBF performs better than a linear model, especially for the three-category classi cation task (task D).
Therefore, by using SVM-RBF and all geo features, Classi er II Location-speci c query airport check metro airport wood eld mall jobs utah herald journal classi ed ads wkrn news 2 motel near knotts berry farm california Buena Park location Detroit schaumburg Logan Nashville Table 6: Example of correct predictions of the city name for a location speci c query can e ectively discriminate di erent localization capabilities of implicit geo queries  to issuers  IP locations.
In this way we can determine users  speci c geo intents.
Note that although training SVM-RBF with high dimensional data is computationally expensive, the prediction cost is very low.
In addition, SVM-linear which has low training cost but reasonably high accuracy (87%) is a good choice when o line training cost is a big issue.
In this task we aim to  nd queries with mentions of an entity that is in some way speci c to a particular geographic location (in our case cities).
Such  localized entities  may be hotels, local tv and radio channels, local newspapers, universities, schools, people names like doctors, sports teams and so on.
Basically if a location (city/town level) can be pinpointed to some item mentioned in the query, then the query is a location-speci c query, by our de nition.
Examples of a location speci c query and corresponding locations are shown in Table 6.
We evaluate our city language models for retrieving cities in location-speci c queries in this experiment.
One important property of location-speci c queries is that although explicit geo information is missing one may still accurately discover the exact location (city/town level) in the user s mind, e.g.
 Liberty Statue  or  Disney   can be viewed as location-speci c queries, which are highly likely to be related to New York or Orlando respectively.
Our low-cost training method again utilizes the non-city part (Qnc) of explicit geo queries as implicit geo intent queries, and tries to discover possible location-speci c queries from them.
This approach has another advantage that the city part (Qc) can be used as the ground truth city label for automatic evaluation.
Since it is extremely expensive to hire human editors to examine over hundred million implicit geo-queries (Qnc) with their city labels (Qc) and identify all the possible location-speci c the following weakly supervised approach combined with the city language models for this discovery task, and then sample outputs of the city language models on the testing data for human evaluation.
i i i i i Our weakly supervised approach  nvolves designing a few ad hoc rules to  nd the GIUs that may come from location-speci c queries.
For example, we require that the maximum city generation posterior  P (Cm|wi+n 1 )   be larger than a threshold, t1, and the corresponding maximum frequency count, #(wi+n 1 , Cm), be larger than a threshold t2; as another example of our rules, we either require that wi+n 1 appear in less than a threshold, t3, number of cities or its overall counts in the geo queries divided by the number of city: #(wi+n 1 , C )/#(|C |) is larger than a threshold t4.
These rules are constructed by considering the characteristics of the GIU features that location-speci c queries may have, and the thresholds are set by looking through the GIUs (wi+n 1 ) and their GIU feature values in the training data.
We leave the question of how to automatically generate these rules for future work.
In this way, from the geo sub training set we obtain 1022 unigram GIUs, 4374 bigram GIUs and 3765 trigram GIUs that may come from location-speci c queries.
We then select queries, which contain any of these GIUs, in the geo sub training/testing sets.
In this way we form training subset III/testing subset III, each of which contains about 1.06M and 1.05M possible distinct location-speci c queries (distinct Qncs) respectively.
We use these automatically generated training and testing subsets to automatically tune parameters for our task.
We now describe how to utilize city language models to further discover cities for location-speci c queries from these two subsets.
date locations Discovering missing related cities for location-speci c queries can be viewed as a challenging multi-category classi cation task, in which there are 1614 di erent categories (city labels).
Given a query (Q) which has implicit geo-intent and is location-speci c, we calculate the city generation posterior P (Ck|Q) of each city Ck by using city language models (CLM) and equation 5.
Then we sort these posteriors and get the corresponding ranked list of cities.
We check whether the maximum posterior P (Cm|Q) is larger than a threshold ta: if yes, Cm is suggested as a candidate location for the location speci c query Q.
Next, we discuss how to tune ta with the training subset III.
We utilize the city part (Qc) as the ground truth city label for each query (remember that the implicit geo-query, Q, is the non-city part, Qnc, of a query in the logs), and calculate precision and recall metrics to roughly evaluate the CLM s performance and tune ta.
Speci cally, given a query Q, we retrieve a set of cities {Ck|P (Ck|Q) > ta}.
When the ground truth city label (Qcm ) is the same as the city (Cm) that has the largest value of P (Cm|Q) > ta, we count that as a right decision made by the CLM in the counter N1; but if Cm is di erent from its ground truth city label Qcm , we count that as a wrong decision by the CLM, using the counter N2.
We then calculate the precision P , and recall R by
 N , where N denotes the number of queries in the training subset III.
Intuitively, P measures the percentage of exactly right location suggestions for the suggested good location-speci c queries, and R measures the and R = N1+N2
 i i n o s c e r









 at =
 at =
 at =
 at =
 at =








 Recall Figure 3: Precision/Recall curve on training subset III for location-speci c query discovery.
percentage of suggested good location-speci c queries in all the possible location-speci c queries.
Figure 3 shows the precision/recall curve with di erent ta values on the training subset III.
It can be observed that by choosing ta = 0.7 we can maintain reasonably high precision (P = 92%) while the recall (R = 84.4%) does not drop too much.
We follow the same procedure to apply CLM on testing subset III where it achieves precision of 88%, and recall of 74% at the threshold of ta = 0.7.
To further evaluate the quality of the ranked list of cities sorted by P (Cm|Q) , for each query (Qnc) that is a location-speci c query, we also compute an IR style measure called Mean Reciprocal Rank (MRR), which is the average of the reciprocal of the ranks of the correct answers to the queries in the testing data: M RR = P r(Q) , where r(Q) denotes

 the rank position of the ground truth city label (Qc) of the location speci c query, Q.
The higher the MRR, the closer the correct answer s rank position is to the top.
When the correct label (Qc) is at rank 1 for all location speci c queries (Q), the M RR = 1.
By setting ta = 0.7, we have an M RR of 0.951 on training subset III and M RR of 0.929 on testing subset III.
These high M RRs imply that for location-speci c queries, the true city labels appear nearly at the top of the suggested city rank list.
In this way we use the training and testing subsets to tune the threshold ta.
The above promising results, especially the high precision and M RR, show that city language models can effectively suggest good location-speci c queries and discover missing city labels.
Nevertheless, our rules to discover possible location-speci c queries may be noisy and the automatic evaluation using (Qc) as the ground truth city label is still rough.
Therefore, we design human evaluation experiments to investigate the CLM s performance by asking human editors to examine the quality of some sampled location-speci c queries and their city labels.
We sampled a random set of queries from testing subset III, such that for each of these queries there existed at least one city, C, that was predicted such that P (C|Q) > ta, to obtain a set of 669 queries and 679 city predictions (10 queries have 2 predictions, the remaining have one).
After giving a detailed explanation of the task, we asked our annotators two questions: (1) if the selected query was a location speci c query and (2) if the predicted location was correct.
Judges were asked to mark  Yes  or  No  in response to these questions.
Eleven judges judged at least 80 predictions each and 240 predictions were judged by 2 annotators.
Annotators were allowed to mark a  ?  for either of the two questions.
They were also allowed to use a search engine of their choice to better understand the meaning of area of information retrieval.
The annotators were a mix of native and nonnative speakers of English.
The inter-annotator agreement on our task was very high (84.5 % on question (1) and 73% on question (2)).
The disagreement on question (2) was often for ambiguous queries like  insider tv show cbs , where one annotator considered our prediction of  hollywood  as a location to be correct, since that is the location of the CBS studios.
Similarly the query  city of angels tv.com  was a source of confusion, since the location in the show is Los Angeles, but the show itself is a national television show.
Of the queries that were marked location speci c the accuracy of predicting a location was 84.5% 5, providing further con dence to support the rough evaluation of the previous section.
However, only half of the queries of the sampled
 be attributed to the explicit geo queries, obtained by using the black-box tool [15], but the remaining was due to the ad-hoc rules used for generating the data-sets used for parameter tuning.
A cleaner data-set or better rules may help improve the accuracy of prediction signi cantly.
Nevertheless even this noisy data set can be used to train parameters with pretty high accuracy as we have seen.
We addressed the challenging task of automatically discovering user s speci c geo intent in the web search at the  ne-grained geo level   city/location level, even when the explicit geo information is missing.
We employ geo features at  ne levels of granularity extracted from large scale web search logs for this task.
We propose two di erent ways for extracting geo features: one is through building city level geo language models and calculating a query s city generation posteriors, the other one is through analyzing geo information units and extracting rich GIU features at the city level.
These geo features are used for the construction of classi ers in our geo intent analysis system, which detect and discover users  implicit geo intent at the city level, di erentiate between di erent localization capabilities of geo-intent queries, and predict cities in location-speci c queries.
For each individual step, we design a learning task for evaluating the performance; and in each task, we use minimum human labeling e ort to supervise the data and label generation to automatically obtain large-scale learning samples for training and testing.
We leverage click-through data as a surrogate for human labels.
Experimental results demonstrated the e ectiveness of using city language model features and GIU features for all three learning tasks.
We can explore active learning approaches [17] to select a relatively small number of samples for human judgment and automatically learn better rules to get clean location-speci c query candidates, to generate more accurate CLMs.
We can also exploit other information in web search logs that may help for this task, e.g.
user clicks on the local modules of the returned web pages given a query.
We can also try to build our models at a zip-code level to disambiguate between locations that have the same name in the future.
We can also consider locations beyond the US for future work.
We can incorporate our city language models into retrieval models.
We are also interested in using the geo intent analysis results for helping to provide better query suggestions.
The modeling and experimental work for this paper was done when Xing Yi was an intern at Yahoo!
Inc. Xing Yi was also supported in part by the Center for Intelligent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DAR-PA) under contract number HR0011-06-C-0023, and in part by UpToDate.
Any opinions,  ndings and conclusions or recommendations expressed in this material are the authors  and do not necessarily re ect those of the sponsor.
We also thank Rosie Jones for her valuable discussions on this work.
