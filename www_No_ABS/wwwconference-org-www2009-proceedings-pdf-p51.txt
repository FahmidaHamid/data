Many information needs are better served by explicit, self-contained answers than by lists of results or documents.
Recently, Community Question Answering (CQA) portals emerged that allow users to ask natural language questions that are answered by other users of the system.
For millions of users, and tens of millions of questions posted on popular CQA sites such as Yahoo!
Answers1, and Naver2, this method of information seeking has proven to be more successful than general-purpose Web search.
For example, Yahoo!
Answers already has tens of millions of users, and stores hundreds of millions of answers to previously asked questions, and serves millions of visits each day.
These databases of past questions and respective answers are proving to be a valuable resource for speci c information needs not well served by general-purpose Web search engines.
Unfortunately, the quality, accuracy, and comprehensiveness of the content in the CQA archives varies drastically, and a large portion of the content is not useful for answering user queries.
Not surprisingly, the reputation and expertise of the contributors can provide crucial indicators into the quality and the reliability of the content.
The reputation of the contributor could also be a valuable factor for ranking search results from CQA repositories, as well as for improving the system interface and incentive mechanisms.
In a CQA environment, schematically shown in Figure 1, there are three sets of connected entities: users, answers and questions.
In addition to the intuitive connection between questions and answers, users are also connected with two other sets of entities by both expressing speci c information needs via posting questions, and by responding to existing question via posting their answers to questions.
Unfortunately, existing methods for estimating content quality in CQA either require large amounts of supervision (e.g., [2]) or focus on the network properties of the CQA without considering the actual content of the information exchanged (e.g., [17]).
We observe that user reputation and the quality of the content they produce are often tied together in a mutually reinforcing relationship.
Building on this observation, we propose a general framework for simultaneously calculating the answer and question quality and 1http://answers.yahoo.com/ 2http://www.naver.com/ Bian et al. [3] developed a ranking system to retrieve relevant and high-quality answers, but they did not explicitly integrate content quality and user reputation information into the ranking process.
While these models have shown to be quite e ective for  nding high quality [2] and relevant [3] content, they do not explicitly model user reputation, and require substantial amounts of manual supervision.
At the same time, there has been a longstanding interest in modeling authority, reputation and expertise in social networks and communities.
Link-based ranking algorithms have been shown to be successful in the context of evaluating quality of Web pages.
Two of the most prominent link-analysis algorithms are PageRank [12] and HITS [11].
Variations of PageRank and HITS have already been applied in many contexts, especially for propagating reputation and  nding experts in the mutual reinforcement process.
Guha et al. [7] and Ziegler [20] study the problem of propagating trust and distrust among users in social media, while considering trust as a transitive property in network relationships.
Expert  nding is also an active area of research, where researchers also take advantage of mutual reinforcement principle.
Zhang et al. [17] analyze data from an online forum, seeking to identify users with high expertise.
They apply both ExpertiseRank and HITS to identify users with high expertise.
Jurczyk and Agichtein [10] show an application of the HITS algorithm to a CQA portal, especially the user interactions graph, and show a positive correlation between authority calculated with the HITS algorithm and answer quality.
Campbell et al. [5] compute the score of HITS over the user-user graph in a network of email exchanges, showing that it is more correlated to quality than other metrics.
Zhou et al. [19] propose a method for co-ranking authors and their publications using their networks.
Dom et al. [6] also study the performance of several link-based algorithms to rank people by expertise on a network of email exchanges.
Although link-based and probabilistic approaches have been shown to be successful in ranking entities on the graph, most of them focus on ranking only one type of entity, and few of them utilize other properties of the entities except with link structure.
Building on the previous work, our framework is based on the model of network relationships in social media [13] and exploits the mutual reinforcement principle [16].
We propose a mutual reinforcement framework for ranking sets of entities, speci cally applied to the CQA network that connects users, questions, and answers.
Our approach take advantage of mutually reinforcing relationship to rank various sets of entities simultaneously, and in this approach, many other features are used besides link structure.
Answer and question quality are crucial to information retrieval in community question answering.
It has been noted in previous work that user reputation is expected to correlate with answer and question quality but the relationship between user reputation and content quality is not straightforward.
 Authoritative  users may provide poor answers, and  poor  users may occasionally provide excellent answers [2,
 to calculate answer and question quality as well as user rep-Figure 1: Network of interactions in CQA connecting users, questions and answers user reputation based on their network relationships, coupled with the individual quality/reputation-related features.
In our framework, the CQA interactions are viewed as composite bipartite graphs where each pair of entity types (e.g., users and the answers they generate) can form one bipartite graph.
We develop a machine learning approach that starts with a set of known labels for users and answers, and exploits the mutual reinforcement between the connected entities in each bipartite graph to compute the respective quality and reputation scores simultaneously, iteratively re ning the labels for the unlabeled entities.
Our speci c contributions include:   A mutual reinforcement framework to calculate the quality and reputation scores of multiple sets of entities in a network relationship, simultaneously.
  A semi-supervised learning method to identify high-quality content and users in CQA that dramatically reduces the required amount of manually labeled data for training, while outperforming state-of-the-art supervised methods.
  An important practical application of our framework to enhance search over CQA archives, by incorporating the predicted content quality into the ranking.
Next we review related work, which sets the context for our new method that we introduce in Section 3.
Our work builds on the research in traditional automatic question answering (QA) [15], and on the more recent research area of QA over the Web [4].
However, question answering over Community QA archives is di erent from traditional QA.
The most signi cant di erence is that we are attempting to retrieve answers from a social media archive with a large amount of associated user information [2].
This kind of information (such as user reputation in a QA community) can be crucial to the answer quality and retrieval accuracy.
Due to the explosive rise in popularity of Yahoo!
Answers and similar sites, CQA has become an active area of research.
Jeon et al. [8] presented a machine translation model to  nd similar questions from a community QA service, but did not take quality of answers into consideration.
Su et al. [14] analyzed the quality of answers in QA portals.
Jeon et al. [9] built a model for answer quality based on features derived from the speci c answer being analyzed.
Agichtein et al. [2] presented a supervised approach to mining user interaction and content-based lexical ci c characteristics of Yahoo!
Answers and discuss how to employ coupled mutual reinforcement principle to learn answer and question quality and user reputation.
We will start with a more precise de nition of the problem of calculating answer and question quality and user reputation, and then describe the mutual reinforcement principle between these three types of entities in CQA.
Then we present a coupled mutual reinforcement framework to model answer and question quality and user reputation.
Based on mutual reinforcement in CQA network, we apply a semi-supervised regression-based approach to the problem of learning answer and question quality and user reputation.
In a CQA system, there are three distinct types of entities: users U, answers A, and questions Q.
Questions and answers are posted by a diverse community of users.
And one question can solicit several answers from a number of di erent users.
We can further categorize users into two subsets: askers Uq and answerers Ua.
Note that there can be an overlap between askers and answerers - that is, a user may post both questions and answers.
Before proceeding, we de ne question and answer quality and user reputation more precisely: Definition 1.
Question Quality: a score between 0 and 1 indicating a question s e ectiveness at attracting high-quality answers.
Definition 2.
Answer Quality: a score between 0 and 1 indicating the responsiveness, accuracy, and comprehen- siveness of the answer to a question.
In previous work, question and answer quality were de ned in terms of content, form, and style, as manually labeled by paid editors [2].
In contrast, our de nitions focus on question e ectiveness, and the answer accuracy   both quantities that can be measured automatically and do not necessarily require human judgments.
Definition 3.
Answer-reputation: a score between 0 and 1, indicating the expected quality of the answers posted by a user.
Definition 4.
Question-reputation: a score between 0 and 1, indicating the expected quality of the questions posted by a user.
Clearly, the de nitions above are somewhat  circular  in that the reputation of the user depends on the quality of the questions or answers they post where quality, in turn, can be in uenced by the user reputation.
In fact, we will soon show how to exploit this relationship in our mutual reinforcement framework.
We now state our problem more formally: Problem: Predicting Content and Contributor Quality Given a CQA archive, determine the quality of each question and answer and the answer-reputation and question-reputation of each user, simultaneously, with minimal manual labeling.
In the rest of this section we will  rst introduce the  coupled mutual reinforcement principle  for content quality and user reputation in community question answering.
We will then present our novel semi-supervised, regression-based approach, based on the mutual reinforcement idea.
Recall that our goal is to identify high-quality questions and answers, and high-reputation users, simultaneously.
We now state the mutual reinforcement principle that underlies our approach to solving this problem with the minimum of manual labeling: An answer is likely to be of high quality if the content is responsive and well-formed, the question has high quality, and the answerer is of high answer-reputation.
At the same time, a user will have high answer-reputation if she posts high-quality answers, and high question-reputation if she tends to post high-quality questions.
Finally, a question is likely to be of high quality if it is well stated, is posted by a user with high question reputation, and attracts high-quality answers.
Before we can turn this idea into an algorithm, we need to represent our setting more precisely.
Recall that CQA systems are centered around three entities and their relationships: Users (U), questions (Q), and answers (A).
The relationships between these entities are illustrated in Figure 2.
In particular, to represent the relationships between answers and their authors, we can use a bipartite graph with an edge between each user and the answers that they post.
Note that it is convenient to partition these graphs according to the question thread   that is, to consider subgraphs that involve answers to a particular question.
Similarly, we can represent the relationship between askers and the questions they post by a bipartite graph with an edge connecting an asker to their posted question(s).
We consider the sets of bipartite graphs that share the same question to be coupled.
We denote the whole graph in Figure 2 as
 where MUA = [mij] is the |U|-by-|A| matrix containing all the pairwise edges, i.e., mij = 1 if there is an edge between user ui and answer aj.
Similarly, MUQ and MQA are the matrices containing pairwise edges representing the association between users and questions they post, and question and the posted answers, respectively.
Note that users may appear in both the asker and the answerer sets; however, we purposefully remove this additional coupling by separating the  asker  from the  answerer  personas of each user and modeling the reputation of each persona separately.
Now we can state the mutual reinforcement principle introduced earlier more precisely, as the set of four simultaneous equations governing the answer-reputation ya u and question-reputation yq u of a user u, and the corresponding answer quality ya and question quality yq respectively: muaya u   ya ya     muaya u + (1    )yq( a) u a (cid:88) (cid:88) (cid:88) (cid:88) u q a u q a (1) (2) (3) (4) u   yq muqyq yq     maqya + (1    )yq u( q) where u   a or u   q represents an edge between a user and her answers, or user and her questions; yq( a) denotes the tion 3.3.2).
Finally, we summarize a CQA-MR algorithm which can both  t the model and learn on answer and question quality and users reputation (Section 3.3.3).
In a CQA system, there are several complementary feature sets for answers, questions and users, respectively.
Table 1 shows a list of features for answers, questions and users, which form the feature space of answers, X(A), questions X(Q) and users, X(U).
We denote one answer as xa in answer feature space X(A), one question as xq in X(Q) and one user as xu in X(U ).
Reputation Using Coupled Mutual Reinforcement Given an answer a, a question q and a user u described by feature vectors xa, xq and xu, let the probability of them being a good answer, good question, good asker or good an-swerer be P (xa), P (xq), Pqst(xu) and Pans(xu), respectively.
In the following, we will describe a generic approach to learning all these probabilities following the same way.
We use P to denote any of P (xa), P (xq), Pqst(xu) or Pans(xu) and use x to represent the corresponding feature vector.
Using logistic regression, we model the log-odds of P (x) by the following linear models: log P (x)
 =  T x (9) Figure 2: with user-question bipartite graph.
|Q| coupled bipartite graphs connecting quality of answer a s question; yq u( q) denotes the question-reputation of the user who ask question q; the symbol   stands for  proportional to ; and   and   are proportionality constants.
To simplify the notation, we collect answer-reputation and question-reputation scores of all users into vectors ya u and yq u respectively, and collect all answer and question quality scores into vector ya and yq, resulting in the simpli ed form of the same principle: (cid:48) UAya ya u = M ya =  M TUAya (cid:48) yq UQyq u = M yq =  M TUQyq u + (1    )M TQAyq u + (1    )MQAya (5) (6) (7) (8) where M T stands for the matrix transpose of M ; M(cid:48) M(cid:48) UQ is derived from MUA and MUQ as for each m(cid:48) mij(cid:80)|A| (mij   MUA) and for each m(cid:48) m(cid:48) ij = ij = mij(cid:80)|Q| (mij   MUQ).
m(cid:48) j=1 mij ij   M(cid:48) ij   M(cid:48) UA and

 j=1 mij We can now turn the mutual reinforcement principle into a semi-supervised algorithm to estimates content quality and user reputation, as we describe next.
tual Reinforcement Due to the tight correlation and connection between those three sets of entities in CQA (questions, answers and users), we apply a mutually reinforcing approach to learn the question-reputation and answer-reputation of users as well as the quality of questions and answers, simultaneously.
In the following, we  rst describe the features for learning question and answer quality and user reputation (Section 3.3.1).
Then, we present a logistic regression approach for learning question and answer quality and user reputation (Section 3.3.2).
However, we are given very few labels on answers and questions quality and users reputation in CQA.
Thus, we apply the discussed coupled mutual reinforcement relationship for semi-supervised learning on answers and questions quality and users reputation, and such relationship where   are coe cients of the linear models.
When given su cient labeled instances, one can compute those coe -cients by maximizing the corresponding log-likelihoods, say LL(X ) for Equation 9:
 y T x   log(1 + e T x) (10) (cid:88) x X where y   {0, 1} are the label of instance vector x; X denotes the any of U, Q or A, which corresponds to the type of instance x.
We can see that the above learning model depends exclusively on the corresponding feature space of the speci c type of instances, i.e.,answers, questions or users.
Thus the quality of questions and answers are learned only based on answer-related or question-related features while the reputation of users is estimated based on user-related features.
After adapting the coupled mutual reinforcement principle between the answer and question quality and the user reputation, showed in Equation 5, 6, 7 and 8, we are able to measure the conditional log-likelihood of observing one label set given some others belonging to di erent kinds of but associated entities.
We use y to denote the current labels for x and use y(cid:48) to denote new expected labels given the other kinds of old labels are known.
We represent Yd as the set of di erent types of entity associated with y.
For instance, based on Equation 6, the set Yd of answer entity u, yq}, and in Equation 7, the set Yd of users for ya is {ya questions yq u is {yq}.
We use KL-divergence to measure the conditional log-Words shared between question and answer Number of words of question subject Number of words of question detail Date and time when the question was posted Number of stars received earned for this question Number of answers received for this question Question Feature Space X(Q) Q: subject length Q: detail length Q: posting time Q: question stars Q: number of answers Answer Feature Space X(A) A: overlap A: number of comments Number of comments added by other participants A: total thumbs up A: total thumbs down User Feature SPace X(U ) U: total points U: questions asked U: questions resolved U: total answers U: best answer U: stars U: thumbs up ratio U: thumbs down ratio U: indegree U: outdegree U: hub score U: authority score Total points earned over lifetime community Number of questions asked Number of questions resolved Number of posted answers Number of answers that were selected as  best answer  Number of stars the user receive The ratio of thumbs up votes the user posted before The ratio of thumbs down votes the user posted before number of other users whose questions are answered by the user number of other users who answer the questions posted by the user the hub score of the user computed by HITS the authority score of the user computed by HITS Total number of thumb up votes for the answers Total number of negative votes for the answers as  (cid:48)a u ) for the logistic regression model: (cid:181) u  (cid:48)a u =  a  
  a u a u
 (cid:182) 1  L(U)  a u likelihood of y given associated Yd: |X|(cid:88) LL(y|Yd) =   1   y(i) 1   y(cid:48)(i) (11) And now we can extend the objective function from the   (1   y(i)) log y(i) y(cid:48)(i) y(i) log i=1 original log-likelihood in Eq 10 to the following: L(X ) = LL(X ) +  LL(y|Yd) (12) where   is a prescribed weight parameter.
This equation represents the combined log-likelihood for learning the probability of each type of entity.
Note that the KL-divergence can be combined with the original log-likelihood naturally because both are log-likelihood measured on probability distributions so are of the same units.
Next, we show how to  t the above models and how to solve the parameter estimation problem.
The idea is to start with uniform distributions for P (xa), P (xq), Pqst(xu) and Pans(xu), and then iteratively update them to increase the likelihood based on their coupled mutually reinforcing relationship.
In the following, we will  rst describe a generic approach to  tting any of the four logistic regression models for P (xa), P (xq), Pqst(xu) or Pans(xu) in the mutual reinforcement framework.
Then, we will describe an algorithm to learn answer and question quality and question-reputation and answer-reputation of user simultaneously.
We now describe how to  t the logistic regression model in more detail.
As an example, consider  tting the model for the answer-reputation of users.
From the current answer quality ya, we use Equation 5 to calculate ya u.
Note that we will keep the given labeled scores to the corresponding users.
Then, given user-related features X(U) and ya u, we use the Newton-Raphson update to compute the new  a u (denoted (13) (14) (15) Let Xu denote the matrix of xu values, pa  tted probabilities of users and Wa u(xi)(1   pa element (i, i) equal to pa Raphson step is thus: u the vector of the u a diagonal matrix with u(xi)), then the Newton-(cid:48)a u =  a u + (XT u Wa uXu)   We rewrite this equation as:
 u (ya u   pa u) (cid:48)a u = (XT  
 u Wa uza u u Wa  1(ya uXu) u   pa u + Wa u) is the residual.
Using u u, we are able to calculate the new answer-where za u = Xu a the new value of  a reputation of users y(cid:48)a u .
Then we can apply the same approach to  t the logistic regression model for the answer and question quality and the question-reputation of users (denoted as  a,  q and  q u, respectively).
Based on the proposed method of  tting logistic regression models, we present the following algorithm CQA-MR (Alg.
1) for simultaneously learning answer and question quality and user ask and answer reputation, where the Forward phase carries out the mutual reinforcement from the left to the right while the Backward phase from the right to the left as shown in Figure 2.
Since we generate combined log-likelihood for learning the probability of each type of entity, the mutually reinforcing approach of our algorithm should cause the successive estimates of content quality and user reputation to converge.
We will empirically demonstrate convergence in Section 5.
In this section, we have de ned the problem of calculating content quality and user reputation in CQA.
Then, We present coupled mutual reinforcement framework and a semi-supervised regression-based approach to solve the input : questions, answers and users and their connection from CQA-network.
output: answer quality ya; answer-reputation of user ya u; question quality yq; question-reputation of user yq u Start with an initial guess, e.g.
uniform values, for ya, ya u, yq and yq u; begin while ya, ya u, yq, yq u not converge do Forward  t the logistic regression models and calculate new values for ya, yq and yq sequence ; Backward  t the logistic regression models and calculate new values for yq, yq and yq sequence u in u in end problem.
In the following sections, we will setup and carry on a large scale evaluation on the framework and our new method.
This section presents our evaluation setup.
First, we describe our dataset including corpus of questions, answers and the corresponding users.
Then, we describe our evaluation metrics and some methods for computing answer quality and user reputation used for comparison in the experimental results reported in Section 5.
We also describe several ranking methods to illustrate the e ects of user reputation and answer and question quality on general QA-retrieval.
Yahoo!
Answers Web service supplies an API to allow Web users to crawl existing question answering archives and the corresponding user information from the website.
In order to ensure that our experiments are representative and repeatable, we use the TREC QA benchmarks to crawl QA archives and related user information.
This is done by submitting TREC QA queries into the CQA site and retrieving the returned questions, answers and related users.
(All the data in this paper can be found at http://ir.mathcs.emory.edu/shared/) Factoid questions from the TREC QA benchmarks We use factoid questions from seven years of the TREC QA track evaluations (years 1999 2006)3 for our experiments.
It is worth noting that TREC questions from the years 1999 to 2003 are independent of each other: each question is self-contained and is submitted directly as a query.
Starting from 2004, however, the questions are organized in groups with a  target .
For those questions, we submit their  target  as well as the questions themselves.
In total, approximately
 set of queries.
Since we need some candidate answers from Yahoo!
Answers to estimate how well di erent ranking functions perform, we select the 1250 TREC factoid questions that have at least one similar question in the Yahoo!
Answers archive.
The CQA dataset Our dataset was collected in order to mimic a user s experience with a CQA site.
We submit each TREC query to the Yahoo!
Answers Web service4 and retrieve up to 10 top-ranked related questions according to the Yahoo!
Answers ranking.
For each of these Yahoo!
questions, we retrieve as many answers as there are available for each question thread.
There are, in total, 107293 users, 27354 questions and 224617 answers.
And after automatically judging
 pattern, 19409 tuples are labeled as  relevant  while 182308 are labeled as  non-relevant , and there is no answer pattern provided from TREC for the other 22900 tuples.
Note that, although the proportion of factoid questions in Yahoo!
Answers may not be large, we use them in order to have objective metric of correctness, and extrapolate performance to whole QA archives.
Relevance Judgments In our experiment, the data are labeled for evaluating QA general retrieval in the following two ways: by using the TREC factoid answer patterns, and, independently, manually in order to validate the pattern-based automatic labels.
For automatic relevance labels we use the available regular expression answer patterns for the TREC factoid questions.
We check every answer s text body, and if the text matches one of the answer patterns, we consider the answer text to be relevant, and non-relevant otherwise.
We manually validated the accuracy of our automatically-assigned relevance labels in our previous work [3], and we found 90% agreement of automatic labels with manual relevance judgments.
Data Labeling We use a set of labels for good users and good answers from Yahoo!
Answers directly.
For some question threads in Yahoo!
Answers, there is one  best answer   which is selected by the asker.
These  best answers  can be viewed as high-quality answers.
In addition, Yahoo!
Answers selects some users as  top contributors  based on those users  answering history.
These  top contributors  can also be viewed as users with high answer-reputation.
In our data, there are 4000  top contributors  and 18000  best answers .
In order to evaluate the e ectiveness of our algorithm for calculating answer quality and answer-reputation of users, we utilize a portion of these labels for users and answers.
The other labels are used for testing by comparing with corresponding results of CQA-MR.
In our experiments, we will keep 3600 top contributors  labels and 16000 best answers  labels for training our model, and then use the rest 400 top contributors and 2000 best answers to test the per formance of our algorithm for learning answer quality and user answer-reputation.
More importantly, we will evaluate the improvements to search, as described next.
In order to evaluate the e ectiveness of our algorithm for computing question quality, we manually label a portion of the data.
We randomly chose 250 resolved questions from Yahoo!
Answers website which have received at least 5 answers.
Two annotators were given 150 questions with 50 in common, and asked to label the quality of those 250 questions independently.
The instructions for this labeling task were to consider both question subject and detail when examining question quality, and to consider answers when there is di culty to understand the question.
Questions 3http://trec.nist.gov/data/qa.html 4http://developer.yahoo.com/answers/ guidelines share by annotators.
Table 2 reports the agreement between the two raters on the 50 common questions.
Since sometimes it is very hard to distinguish between  good  and  fair  questions, we also combined  good  with  fair  to form a binary labeling.
Both agreements are reported in Table 2.
As we can see that we can get moderate agreement for both methods.
As we can see, the binary labeling results in higher agreement; hence, we will use the binary  Good / Bad  labels to evaluate question quality.
Table 2: Inter-annotator agreement and Kappa for question quality 3 categories 2 categories Agreement Kappa coe cient




 We adapt the following information retrieval metrics to evaluate the performance of the our algorithm for learning answer quality as well as the performance of general QA retrieval.
  Mean Reciprocal Rank(MRR): The MRR of each individual query is the reciprocal of the rank at which the  rst relevant answer was returned, or 0 if none of the top N results contained a relevant answer.The score for a sequence of queries is the mean of the individual query s reciprocal ranks.
Thus, MRR is calculated as (cid:88) q Qr
 rq

 |Qr| where Qr is a set of test queries, rq is the rank of the  rst relevant document for q.
  Precision at K: for a given query, P (K) reports the fraction of answers ranked in the top K results that are labeled as relevant.
In our setting, we require a relevant answer to be labeled  matched  for TREC pattern.
For this metric, the position of relevant answers within the top K is irrelevant, while it measures overall user potential satisfaction with the top K results.
  Mean Average of Precision(MAP): Average precision for each query is de ned as the mean of the precision at K values calculated after each relevant answer was retrieved.
The  nal MAP value is de ned as the mean of average precisions of all queries in the test set.
This metric is the most commonly used single-value summary of a run over a set of queries.
Thus, MAP is calculated as (cid:80)N r=1(P (r)   rel(r))

 |Qr| |Rq| (cid:88) q Qr where Qr is a set of test queries, Rq is the set of relevant document for q, r is the rank, N is the number retrieved, rel() is a binary function on the relevance of a given rank, and P () is precision at a given cut-o  rank.
We now describe the methods used to compute user reputation, which we use for our main task of improving CQA retrieval.
Speci cally, we compare the following methods:   Baseline: users are ranked by  indegree  (number of answers posted), an e ective baseline estimate of user authority in CQA according to reference [17].
  HITS: we calculate the user reputation based on HITS algorithm.
Users are ranked based on their authority scores.
  CQA-Supervised: we classify users into those with  high  and  low  reputation using a supervised clas-si er, namely SVM (SMO implementation) , trained over the features in Table 1.
Then user are ranked based on their reputation scores.
  CQA-MR: predict user reputation based on our mutual-reinforcement algorithm (Section 3.3).
Unfortunately, a direct experimental comparison with reference [2], which is most closely related to our work, is impossible as neither the dataset or the truth labels used for the experiments in [2] are available.
However, CQA-Supervised is a similar approach and uses similar features to those described in [2], thereby providing a realistic state-of-the-art content quality classi er comparable to reference [2].
Our main task is to improve CQA retrieval by incorporating content quality and user reputation.
We compare the following ranking methods:   Baseline: In this method, the answers are ranked by the score computed as the di erence of thumbs-up votes and thumbs-down votes received for each answer.
This ranking closely approximates the ranking obtained when a user clicks  order by votes  option on the Yahoo!
Answers site.
The detail of this method and how to compute MRR and MAP under this setting is discussed in [3].
  GBrank: In this method, we apply the ranking method proposed in our previous work [3], which did not include answer and question quality and user reputation into ranking function.
This method has been showed in [18] to have better performance than many state-of-the-art supervised ranking methods, such as RankSVM.
  GBrank-HITS: In this method, we optimize GBrank by adding user reputation calculated by HITS algorithm as extra features for learning the ranking function.
  GBrank-Supervised: In this method, we  rst apply a supervised method (SVM) to learn the answer and question quality and user reputation based on their individual feature set independently.
Then, we optimize GBrank by adding obtained quality and reputation as extra features for learning the ranking function.
  GBrank-MR: In this method, we optimize GBrank by adding answer and question quality and user reputation calculated by CQA-MR as extra features for learning the ranking function.
Note that, GBrank-MR and GBrank-Supervised, we use the same set of labels in learning.
In this section, we will present several large-scale experiments.
These experiments are used to demonstrate that (1) the CQA-MR algorithm exhibits good convergence behavior; (2) CQA-MR is e ective for computing the question quality; (3) the performance of general QA retrieval can be improved by incorporating predicted quality features calculated by CQA-MR; (4) user reputation from CQA-MR tends to be better than those computed by other state-of-the-art methods; (5) the amount of supervision in CQA-MR a ects the quality of predicted quality features.
utation
 We  rst perform training using Algorithm CQA-MR introduced above.
We examine the convergence behavior of CQA-MR by calculating the log-likelihood function (Eq.
10) over the iterations.
We  nd that the log-likelihood values increase and converge at around 40 iterations, when computing content quality and user reputation.
u, yq) and LL(ya We also calculate the log-likelihood for di erent values of  .
We are able to  nd that the log-likelihood has much smaller values when   is bigger, especially in the initial few iterations, which means that the conditional log-likelihood u|ya)) is very small initially.
There-(LL(ya|ya fore, the di erence in the labels between successive iterations is big, which implies labels  inconsistency, in the early stage of the algorithm.
However, we can also  nd that when we take more than ten iterations, the log-likelihood is almost the same regardless of the   values.
Thus, at later stage of our algorithm, the log-likelihood values are more sensitive to the objective function of the logistic regression while the labels remain consistent across iterations, stabilizing at around 30 iterations.
We now compare the e ectiveness of CQA-MR with other methods for predicting answer-reputation of users.
For this task, we use the holdout set of 3600 users, with 678 of them labeled as  top contributors .
Figure 3 reports the fraction of  top contributor  users included in the top K users ranked by answer-reputation, for varying K. As we can see, CQA-MR exhibits signi cantly higher precision than CQA-Supervised, which, in turn outperforms HITS and the simple  in-degree  count baseline.
We now compare the e ectiveness of CQA-MR with other methods for predicting question quality.
For this task, we use the set of 250 questions with manually labeled quality, which is described in section 4.1.
We train CQA-MR as described above (that is, no additional question quality labels provided for training) and randomly select 100 labeled questions for evaluating the performance of predicting question quality.
In order to compare with existing methods for predicting question quality, we also apply a supervised classi er, namely SVM (SMO implementation), trained over the features in Table 1.
And the testing set is the same 100 labeled questions used above while the other 150 labeled questions are used for training SVM.
Figure 3: Precision at K for the status of top contributors in testing data Figure 4: Precision-Recall curves for predicting question quality of CQA-MR and Supervised method.
while that value is 0.890 for the supervised method (SVM).
Figure 4 shows the precision-recall curves for both methods, it is clear that CQA-MR gives good performance on predicting question quality and exhibits signi cantly higher precision than supervised method.
In addition, we also try to add 150 labeled question as seeds in training CQA-MR.
Interestingly, adding the question labels as additional seeds for training CQA-MR does not signi cantly improve performance.
The answer and question quality and user reputation computed by CQA-MR, CQA-Supervised and HITS can be viewed as prior  static  features for QA retrieval since they are independent of queries.
This complements  classical  information retrieval and QA retrieval, which primarily focused on query-dependent relevance features.
In this experiment, we seek to enhance the performance of general QA retrieval by incorporating predicted quality features (answer and question quality and user reputation).
We use GBrank [18] as the ranking function and apply the same framework in our previous work [3].
For the training data we use 800 TREC queries and the associated community QA pairs, and we use another 450 queries and the associated community QA pairs for testing data.
The set of features used to train the ranking function is described in detail in [3].
The mean average precision (MAP) for CQA-MR is 0.940 We train four ranking functions, GBrank-MR, GBrank-
reputation, i.e.,HITS and CQA-Supervised.
Figure 5 demonstrates the Precision at K of GBrank-MR compared with methods of GBrank-HITS and GBrank-Supervised.
The GBrank-Supervised method replaces QA quality and user reputation calculated by a supervised learning method.
Note that we use the same set of labels in learning for GBrank-MR and GBrank-Supervised.
In order to compare the two methods GBrank-MR and GBrank-Supervised, we apply t-test based on their precision and the p-value of signi cance test is 0.02.
The GBrank-HITS method replaces user reputation by those calculated by the HITS algorithm.
Figure 5 and Table 3 indicate that GBrank-MR achieves much better performance than GBrank-HITS, which implies that the user reputation calculated by CQA-MR gives more contribution than user s authority scores computed by HITS.
However, GBrank-HITS outperforms GBrank which does not contain QA quality and user reputation features.
It shows that user s authority scores from HITS are still useful to enhance the performance of QA retrieval.
Our conjecture is that for user s answer-reputation, it is much more important because CQA-MR not only utilizes network relationship but also individual reputation-related features while the authority scores in HITS only relies on the graph structure of CQA systems.
From Figure 5 and Table 3, we can also  nd that GBrank-MR performs signi cantly better than GBrank-Supervised (p < 0.03).
After analyzing information gain of features, we  nd that GBrank-MR assigns higher weights on QA quality and user reputation features.
All of these imply that the QA quality and user reputation calculated by CQA-MR gives more contribution than those calculated by supervised method with limited amount of training data.
GBrank-Supervised also outperforms GBrank which shows that QA quality and user reputation obtained by supervised method are still useful to enhance the performance of QA retrieval.
As mentioned before, we utilize a set of training labels for users and answers in the algorithms CQA-MR and CQA-Supervised to learn predicted quality features.
In this experiment, we show the in uence of the amount of training labels (i.e.,degree of supervision) on the performance of CQA-MR and CQA-Supervised.
The labeled set contains 3600 good users and 16000 good answers.
We vary the size of training labeled set used in the learning process by randomly selecting a certain number of labels.
For example, choosing 40% labels means to use 2400 good users and 10800 good answers in the learning process.
Figures 6 and 7 report the MRR and MAP scores for the holdout validation data against varying amount of labeled training data for high quality question and answer retrieval.
We can see that MRR and MAP scores increase when there are more labels in CQA-MR. CQA-MR can achieve same accuracy as CQA-Supervised with about half of the required training data.
Therefore, CQA-MR can improve QA retrieval much more with less supervised learning compared to CQA-Supervised.
We also  nd that GBrank-HITS have higher accuracy than GBrank-MR when the amount of supervision is less than 1200 examples, suggesting that HITS indeed identi es high quality content/users, but can be improved on by our method.
Figure 5: Precision at K for Baseline, GBrank, GBrank-HITS, GBrank-Supervised and GBrank-MR for various K Supervised, GBrank-HITS and GBrank, on training data (i.e.,the 850 TREC queries) with predicted quality features added in the  rst three methods and training data without these features in the last one, respectively.
Then, we test on the remainder holdout testing data of 450 TREC queries and the associated community QA pairs.
Figure 5 illustrates the Precision at K of GBrank-MR, GBrank-Supervised and GBrank-HITS compared with GBrank and the baseline method.
The  gure shows that all of the four ranking functions outperform the baseline method.
Furthermore, after incorporating predicted quality features, i.e.,answer and question quality and user reputation, GBrank-MR, GBrank-Supervised and GBrank-HITS give better precision than GBrank without these features.
In particular, the Precision at 1 of GBrank-MR is about 79% compared to 76% Precision at 1 exhibited by GBrank.
Table 3: Accuracy of GBRank-MR, GBRank-Supervised, GBRank-HITS, GBRank, and Baseline (TREC 1999-2006 questions) Baseline GBrank GBrank-HITS GBrank-Supervised GBrank-MR Gain


 +0.045(6%)

 +0.051(7%)
 ----Gain





 Table 3 reports the MAP and MRR scores for GBrank-MR, GBrank-Supervised, GBrank-HITS, GBrank as well as the baseline method.
Table 3 indicates that GBrank-MR, GBrank-Supervised and GBrank-HITS achieve much better performance than GBrank and the baseline method.
In particular, for MRR scores, GBrank-MR achieves a gain of about 11% relative to GBrank; and GBRank-MR obtains double the gains of GBrank-HITS for both MRR and MAP scores.
The above experiments illustrate the usefulness of the extracted static features in improving answer relevance.
tation Features We now explore the e ects of the QA quality and user reputation, which is calculated by CQA-MR, on learning the ranking function.
To this end, we perform a study on its in uence on QA retrieval compared with existing graph-based
 In the future, we will explore the performance of our approach across di erent question domains, and take into account varying expertise (authority) of users for di erent domains.
In addition, we also plan to focus on exploring probabilistic models of content quality and user authority, and on applying these techniques to other online communities.
