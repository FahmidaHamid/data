Social networks (SNs, for short), including Friendster.com, Tagged.com, Xanga.com, LiveJournal, MySpace, Facebook, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
and LinkedIn have developed on the Internet over the past several years.
SNs have been successful in attracting users.
According to ComScore Media Metrix, more users visit MySpace than Yahoo, MSN or Electronic Arts gaming site [24].
SNs provide a form of self expression and help users to socialize and interact with other users.
Users can de ne a personal pro le and customize it as they wish.
Through SNs, users may engage with each other for various purposes, including business, entertainment, and knowledge sharing.
The commercial success of SNs depends on the number of users it attracts, and encouraging users to add more users to their networks and to share data with other users in the SN.
End users are however often not aware of the size or nature of the audience accessing their data and the sense of intimacy created by being among digital friends often leads to disclosures that may not be appropriate in a public forum.
Such open availability of data exposes SN users to a number of security and privacy risks [22, 34, 19].
A signi cant privacy threat is raised by an increasing amount of media content posted by users  in their pro le.
User-provided digital images are an integral and exceedingly popular part of pro les on SNs.
For example, Facebook hosts 10 billion user photos (as of 14 October 2008), serving over 15 million photo images per day [2].
Pictures are tied to individual pro les and often either explicitly (through tagged labeled boxes on images) or implicitly (through recurrence) identify the pro le holder [1].
Such pictures are made available for other SN users, who can view, add comments and, by using content annotation techniques, can add hyperlinks to indicate the users who appear in the pictures.
In current SNs, when uploading a picture a user is not required to ask for permissions of other users appearing in the photo, even if they are explicitly identi ed through tags or other metadata.
Although most social networking and photo sharing websites provide mechanisms and default con gura-tions for data sharing control, they are usually simplistic and coarse-grained.
Pictures, or in the more general case, data, are usually controlled and managed by single users who are not the actual or sole stakeholders, raising serious privacy concerns.
Data stakeholders may be unaware of the fact that their data (or data that is related to them) is being managed by others.
Even when the stakeholders are aware of the fact that their data is posted and controlled by other users, they have limited control over it and cannot in uence the privacy settings applied to this data.
The privacy breach due to poor or no access control of shared data in Web 2.0 is well documented in the public news media [34].
Letting one user taking full responsibility over another s know each other, their social relationship often does not imply that they have the same privacy preferences.
The average number of friends of Myspace users is 115 friends, which indicates that the friend relationship is being stretched to cover a wide range of intimacy level [21].
Consequently, users who share content may have di erent privacy preferences, and as a consequence their privacy preferences on some data content they share, may be con icting.
Based on such considerations, in this paper we focus on how to enable collaborative privacy management of users  shared content.
We believe this is an important contribution in the realm of Web 2.0, since to date, current SNs support privacy decisions as individual processes, even though collaboration and sharing represent the main building blocks of Web 2.0.
Designing a suitable approach to address this problem raises a number of important issues.
First, co-ownership in SN platforms should be supported.
Second, the approach should promote fairness among users and be lightweight.
Moreover, the approach should be practical and promote co-ownership, since users knowingly do not enjoy spending time in protecting their privacy [35].
We analyze these requirements from a game theoretical per-spective[26], and model the process of collaborative privacy management of shared data as a mechanism design problem.
We map the user collaborative policy speci cation to an auction based on the Clarke-Tax [7, 8] mechanism which selects the privacy policy that will maximize the social utility by encouraging truthfulness among the co-owners.
The Clarke-Tax mechanism is appealing for several reasons.
First, it is well suited to our domain, in that it proposes a simple voting scheme, where users express their opinions about a common good (i.e., the shared data item).
Second, the Clarke-Tax has proven to have important desirable properties: it is not manipulable by individuals, it promotes truthfulness among users [11], and  nally it is simple.
Under the Clarke-Tax, users are required to indicate their privacy preference, along with their perceived importance of the expressed preference.
Simplicity is a fundamental requirement in the design of solutions for this type of problems, where users most likely have limited knowledge on how to protect their privacy through more sophisticated approaches.
We integrate our design with inference techniques that exploit folksonomies, automating collective decisions, thus freeing the users from the burden of manually selecting privacy preferences for each picture.
As part of our assessment, we implement a proof of concept application, in the context of Facebook, one of today s most popular social networks and show that supporting these type of solutions is not also feasible, but can be implemented through a minimal increase in overhead to end-users.
The rest of the paper is organized as follows.
In the next section we provide an abstract representation of SNs.
Then in Section 3, we discuss data co-ownership in SNs.
In Section 4, we highlight the requirements for the design of an e ective solution supporting collaborative privacy management.
In Section 5, we describe our proposed framework which is based on the Clarke Tax mechanism.
We present our applied approach, detailed system implementation and experimental results in the context of Facebook in Section
 the paper in Section 8.
In this section we provide an abstract representation of a SN.
Our intent is not to represent any concrete system, but to identify the key elements of a SN, upon which to build our solution.
A SN is characterized by the following core components:   U.
The set of users.
The community composing a SN is represented as a collection of users.
Each i   U is uniquely identi ed.
  RT.
The set of relationship types supported by the SN.
Users in a SN are possibly connected among each other by relationships of di erent types.
   .
It denotes the functional assignment of a relationship among a couple of users.
Speci cally:   : Rt   U   U    .
Given a pair of users i, j we denote their relationship as i Rt j, where Rt is a relationship name of one of the supported RT .
The same pair of users can be related by di erent type of Rt.
We assume all the relationships in general to be binary, non-transitive and not-hierarchically structured.
Unary relationships are also enabled, such as for example i is f an of U 2, although not relevant for us.
  P rof ilei.
The pro le of a user i.
We represent it as a tuple P rof ilei = (GRelT ype1, .
.
.
, GRelT ypek, Set) where GRelT ypel represents the list of users having a relationship Rtl such that i  Rtl i where Rtl   RT .
S represents the data set posted on i s pro le.
We denote the pro le components of a user i by means of the dot notation.
For example, i s friends are represented as P rof ilei.F riends while the data set S as P rof ilei.Set.
  D. The set of data types supported.
Supported content types are multimedia video and music  les - images, documents and hypertext.
Users in SNs are connected among each other by means of direct or indirect relationships.
Direct relationships hold when two users hi Rt ji are tied with each other according to a relationship Rt supported by the SN.
Two users 1, k are indirectly related if there exists a path connecting them of the form: (h1 Rt1 2i, h2 Rt2 3i, .
.
.
, hk   1 Rtk 1 ki), where each tuple hi Rtl ji denotes an existing relationship of type Rtl between users i and j.
Provided that there may be multiple paths connecting two given users, the users  distance between i and j is the path with the minimal number of users between them.
In the rest of the paper we always refer to the minimal path, unless stated otherwise.
Example 1.
Consider users Alice, Bob and John who are part of a social network.
Alice and Bob are friends while Bob and John are colleagues.
The distance between Alice and John with respect to the relationships F riend Of and Colleague Of is 2 because their minimal connecting path is the social path (hAlice F riend Of Bobi, hBob Colleague Of Johni).
In our reference model, each user i   U enforces locally speci ed privacy policies over their data posted in P rof ilei.
Such privacy policies are simple statements specifying for each locally owned data item who has access to it, and, in the data.
In current SN sites, users have little  exibility when specifying such privacy policies (also referred to as access rules or privacy settings), and can choose among a limited set of prede ned options, such as: friends, friends of friends etc.
Additionally, access rights in a SN are limited to few basic privileges, such as read, write and play for media content.
Here, in order to provide a model that is as general as possible, we assume that users are able to specify Distance Based access conditions in their privacy policies.
That is, the users allowed to access the data, are identi ed by means of the notion of users  distance, discussed in previous section.
We omit specifying the type of access privilege, as it is not signi cant in our case, and assume generic viewer rights for users who can access another s pro le.
A privacy policy is summarized by the predicate P rP (i, n)RtSet, which indicates all the users who are connected with i with a minimum path of length n, by relationships in RtSet1.
In case i leaves the data public to the whole SN, the predicate will be of the form P rP (i,  ), while in case accessibility is restricted to owner(s) only, the predicate will be set as P rP (i, 0).
We say that a user j satis es a distance based condition P rP (i, n)RtSet if the minimal length of the path between i and j is within n hops according to the relationships listed in RtSet.
Example 2.
Suppose Alice wants her friends of friends to be able to view her pictures.
She will enforce a policy of the type P rP (i, 2)F riend Of .
Bob, in Example 1, satis es the policy, while John does not, since John and Alice are indirectly connected by means of a Colleague Of relationship.
In this section we introduce the notion of collaborative data sharing in SNs.
We present the notion of co-ownership in SNs and discuss how to detect co-ownership of data in a semi-automated manner2.
In SNs, users post data on their pro les, this data is usually considered owned by the pro le owner.
The pro le owner is also expected to take the responsibility of managing the access of the posted data content.
However, data posted on a user s pro le often conveys content not belonging only to the pro le s owner.
For example, documents can be coauthored and belong to multiple individuals.
Several users may appear in a same picture, and the same applies to other media content, such as videos.
However, if Alice posts a document in her pro le which belongs also to Bob, she is in charge of setting the privacy policy for the document, regardless of whether Bob is happy with her policy or not.
These simple observations naturally lead to the idea of supporting co-ownership (or stakeholders) in SN, to indicate the set of users who are owners of a piece of data, regardless of where (i.e., in which users  pro le) this data has been originally posted.
real-world SN, where for example, one can indicate the visibility of friends of friends, and in recent access control models proposed for SN sites [5].
terms of legislation, but in terms of the information and its relationship with users.
In order to identify co-owners of a given piece of data s, we provide a general classi cation of users based on their relationship with s. From now on, we focus our presentation on photo images or pictures, although the main idea behind our solution is general enough to be applied for other data types.
Users can be classi ed as viewers, originators and owners.
Users who are authorized to access the data s are de ned as viewers.
The originator is the user who originally posted data s on a given pro le.
Finally, the owners are the users who share ownership privileges with the originator within the social network and maintain control over s.
The potential owners of a data item posted on a pro le are identi ed using tagging features supported by current SNs.
In general, tagging consists of annotating social content by means of set of freely chosen words [38], associated with the data denoted as T Set.
Their semantic can be analyzed by means of similarity tools [28].
In the case of pictures, we employ a speci c type of tags widely used in Facebook [13].
These tags, known as id-tags, give the ability for users to add labels over pictures to indicate which users appear in them.
Therefore, each id-tag essentially corresponds to the unique user id.
By leveraging id-tags, one can easily identify the potential owners in a given picture.
We formally de ne potential owners as follows.
Definition 1 (Potential Owners).
Let s be a shared data item posted on user s i pro le P rof ilei.
Let T Set be the set of tags associated with s. The set of the potential owners of s, P ot Owni s is de ned as the set of users whose id-tags are in T Set.
For data types other than pictures, the set of potential owners can be identi ed by using the meta-data associated with the content, or by the originator s initiative.
A user j belonging to the set of potential owners is quali ed as an owner if the originator i agrees to grant ownership for a piece of data s to user j.
Ownership privileges are exclusively granted by the originator to ensure that ownership is managed with users who in fact are not complete strangers, but related only by a number of relationships that the originator believes acceptable.
This network of admitted owners can be automatically speci ed by the originator using distance based policy conditions, which indicate the type of relationships and the distance among the users.
That is, the originator i can decide to grant the ownership of s to some user j only if j has a certain distance P rP (i, j)RtSet within k hops with respect to a certain set of social relationships RtSet.
In order to mitigate the risk of originators not sharing ownership with entitled users, in Section 5 we propose an incentive-based mechanism to motivate sharing of ownership rights.
The de nition of data owners is very intuitive and we thus omit its formalization.
In our context a set of owners, denoted as Owns U Set, U Set   U , do not only decide whether to post/edit/delete s, but more importantly they share the responsibility of managing access of s, by specifying the data privacy settings (or privacy policies).
Example 3.
Consider Alice, Bob and John who are part of F actBook social network.
Alice and Bob are friends while Bob and John are colleagues.
Alice has participated to a Christmas party organized for the employees of the company where Bob and John are employed.
Alice has taken pictures with Bob in which also John appears and posts them on her F actBook pro le.
John requests to Alice to become an owner of the pictures in which he appears.
Alice has the album of the Christmas party to all the users x such that P rP{F riends Of,Colleague Of }(Alice, 2).
Since Alice and John have a degree of separation equal at most to two, John is granted the ownership.
In the next section we investigate how collaborative management of data with multiple owners can be achieved.
In case of single-user ownership, enforcing of privacy policy for a piece of data s is straightforward.
The user sets his/her privacy policy according to his/her privacy preference.
The privacy policy states who can view the user s data, by indicating the distance and the type of relationships viewers  should have with the owner.
On the other hand, a shared data object s has multiple owners where each owner might have a di erent and possibly contrasting privacy preference.
Designing an approach which combines di erent owners  privacy preferences into a unique privacy policy is a challenging task.
In particular, it is unclear how to compose the overall privacy preferences for s without violating individuals  preferences.
Furthermore, if multiple owners share more than a single data item, the decisions made in past interactions may be factored.
Several intuitive approaches are not suitable, due to the speci c constraints of the SN domain, and the data for which the privacy policy is to be speci ed.
For example, selective disclosure is not desirable, and often not feasible.
If the data in question is a picture, cropping or blurring it would result in an altered picture, likely decreasing its intrinsic value to users and owners.
Similarly, if a document is coauthored, it is not always possible to separate the di erent contributions of the authors and disclose portions of it without making it unintelligible.
Note that, cryptographic techniques may theoretically solve the problem of selective data disclosure to entitled viewers.
However, these approaches will not compose a unique privacy policy that incorporates the preferences to the di erent co-owners, and will result in a very unpractical approach, with a very large number of encryption keys for users to manage.
A database-like approach, where di erent owners could enforce their local  views  would not work either, as this approach may result in privacy violations.
For example, Alice may require only friends to view a party picture, while Bob may not care and leave the picture public to any SN member.
Clearly, as John who is not a friend of Alice logs into the social network and accesses the picture through Bob s pro le he violates Alice s privacy preference, although the picture is itself not available for Bob to view from Alice s pro le.
Based on these considerations, we identi ed the following core requirements for collaborative privacy management:   Content Integrity: The data s should not be altered, or selectively disclosed.
In other words, we cannot assume to blur a picture or crop it to remove certain subjects appearing in it.
Nor can we alter a document text or data to satisfy con icting individuals  preferences.
  Semi-automated: The access policy construction process should not solely rely on user s manual input for each data, but should leverage users  past decisions and draw from the existing context.
  Adaptive: When a new owner is added for s, his/her input should be taken into account, even if the access policy for s has been already set up.
  Group-Preference: The algorithm must leverage the individuals  information to develop a collective policy.
In the next section, we propose an approach that satis es the above requirements.
Building upon mechanism design literature, we suggest a mechanism that collects users  privacy preferences and assigns a unique privacy policy that aggregates the users  individuals  input.
The most intuitive approach to aggregate users  decisions is to let co-owners iteratively disclose their preferred settings and explicitly agree on the set of possible viewers  each owner proposes to include.
Owners could update their preferences as they view other owners  preferred settings, and try to reach a common decision on a single policy after a few rounds of revision of their internal settings.
This approach however is hardly applicable in that it requires all the owners to agree on a single set of privacy policies, which may sometimes be an endless task.
Since SN users typically access the network independently it is also hard to force synchronization, without introducing unacceptably long decision processes.
A more conservative solution is to construct a privacy policy that allows viewers  rights only to the set of users who satisfy each of the owners  preferences, avoiding the need of the owners explicit consent on the  nal set of viewers .
However, even this approach is pretty simplistic and fails to leverage the individuals  preferences within the co-owners  group.
In addition to the identi ed drawbacks, in general majority and ranking-based approaches such as the ones described above, have proved to be unfair, in that astute individuals may manipulate outcomes at their advantage.
We suggest an approach that is characterized by two main parts:
 desirable behaviors, such as granting ownership when conditions for co-ownership hold and truthfulness of co-owners when expressing their privacy preferences.
Speci cally, we suggest an application of the Clarke-Tax [8] mechanism to enforce collective privacy decisions.
vacy settings multiple times for similar data, we suggest a simple inference technique to leverage users  previous privacy decisions, when certain similarity conditions hold true.
We now describe the basic notions for our incentive-based mechanism for users to share data in the SN and make thoughtful decisions about their privacy.
We introduce a credit-based system where the user earns credits proportional to the amount of data (e.g., pictures, documents etc.)
of times he/she grants co-ownership to potential owners.
A user i is assigned an initial virtual numeraire ki   R to track the credits upon joining the SN.
There are well de ned mechanisms to credit and debit the numeraire.
For each posted data item s, shared with n co-owners, the originator i gains: c = mi + (    mi)   n where, mi   R are the credits assigned as he/she loads a data item, while     mi corresponds to the numeraire assigned for each user accepted as a co-owner,     [0, 1].
Each user accepted as a co-owner for s gains     mi, where     [0, 1].
As shown, the more the user shares ownership, the more he/she gets rewarded.
The user s numeraire is credited (taxed) based on how pivotal the user s preferences were in making the group decision.
Example 4.
Assume that in FactBook, each uploaded picture is worth 100 while   and   are set to 0.7 and 0.5, respectively.
When Alice posts her picture, she grants ownership to Bob and John, who are id-tagged.
Her bid score, initially set to 1000 is raised of m=100 for posting her picture, and of 70   2 for both Bob and John.
That is, Alice totalizes
 The owners make a collective decision on whether posting a data item and they also agree on the exposure preferences (i.e., distance based conditions) to be imposed to potential viewers.
Users associate a value with each data preference, represented by vi(g), this value represents the perceived ben-e t of the user by exposing a data item with preferences g.
For example, a user who is interested in maximizing disclosure of his photos would assign a high value to data settings g that do not limit disclosure and allow more users to view this photo.
When multiple users are involved for a single decision, they may select di erent optimal choices.
Therefore, we need to design a collective function F (.)
(also known as social welfare function) which outputs a unique outcome, in light of the individuals privacy preference inputs.
F (.
), known as the social function is a function over the individuals  value functions, and outputs a certain collective output
 F (v1(g), .
.
.
, vn(g)) = X (1) A fundamental requirement of any decision function is that it should have an  optimal  in some sense.
Di erent kinds of desirable attributes of decision functions that characterize optimality have been suggested in Game Theory, Economics, and Voting Theory.
Typically, the attributes are concerned with the in uence of an individual user on the outcome, and the impact of the outcome on the individual.
Some common criteria include Pareto Optimality, Symmetry, Fairness, and Individual Rationality.
In contexts such as ours, it is not obvious how to measure global utility.
Considerations other than pure utility values (such as income and fairness) might need to be taken into account.
One simple approach, common in game-theory, (due to Nash [26]) is to choose the outcome that maximizes the collective values (utilities).
We take this approach, since, as we will see, it satis es three important properties [11]: 1) it guarantees a relatively fair distribution of the mutually earned utility, 2) it is simple, and 3) is non-manipulable.
Our goal is to formulate a mechanism that  aggregates  all the individuals preferences into single representative group preference, which builds upon how each user values the different data exposure preferences.
Our approach requires each owner i to associate a value vi(g) to preference g proportional to how important this preference is for him.
The value function vi(g) corresponds to the estimated numeraire value that the user would bene t from adopting setting g.
Given n co-owners of a data item s for which privacy preferences g   G need to be setup, each co-owner i can essentially opt for the di erent possible privacy preferences by assigning their value vi(g) for each g   G. In this paper, we consider the additive social utility, which for a given preference g is the sum of value vi(g) for all the co-owners, i=1 vi(g).
In our case, since we cannot assume synchronization, we let the users express their net values privately (that is, each user does not know the numeraire exposed by others).
The outcome that maximizes the social value is the outcome to be selected and represented by: where F (v1(g), .
.
.
, vn(g)) = Pn g  = arg max g G vi(g) n Xi=1 (2) In essence, we wish to maximize the sum of the value for each user s bid over the picture s privacy, where the outcome g  is the privacy setting that maximizes the social utility.
If an outcome g is adopted then each user i is required to pay tax  i, the utility of the choice c = (g,  1, .
.
.
,  n) is the value of the g minus the tax numeraire, given by: ui(c) = vi(g)    i.
We utilize the Clarke Tax mechanism that maximizes the social utility function by encouraging truthfulness among the individuals, regardless of other individuals choices.
This algorithm requires each user to state the net value vi(g) for their preference simultaneously.
Unlike the original Clarke Tax mechanism, our formulation does not require a  xed cost to be paid by the n co-owners.
We consider the  xed cost to be equal to 0.
The tax levied by user i is computed based on the Clarke Tax formulation as follows:  i(g ) =Xj6=i vj (arg max g G Xk6=i vk(g))  Xj6=i vj(g ) (3) Note user i s tax  i(g ) for selecting outcome g  is composed of two portions, that are computed over a group of users excluding user i.
The  rst portion computes the new outcome that would have been the societal if user i s values had been ignored and then computes the social utility for such an outcome.
The second part computes the social utility for the outcome g  over the subgroup of users excluding user i.
The tax  i(g ) de ned as the di erence between the  rst and second portions.
Assume each co-owner i can essentially opt for privacy preferences stated in terms of connecting path distance, which take values from g   {0, nRSet,  }, denoting owners only (0), n-distant viewers of relations in RSet and public ( ), respectively.
In case nF riends is the winning option, the set of  nal viewers is identi ed as the conjunction of the pivotal users friends  set.
That is, P rof ile1.F riends           P rof ilen.F riends.
Each user indicates a value vi(g) for each of the preferences in (g   {0, nRSet,  }).
Figure 1, shows an example including three users, each user i places their values vi(g) as indicated in the  gure.
Note that the


 n    i(g )





















 ui u1 u2 u3 Pi vi(g) Pi6=1 vi(g) Pi6=2 vi(g) Pi6=3 vi(g) Figure 1: Clark Tax Example.
outcome g = {n} maximizes the social value with a value of
 for their contributions to the social value function.
User u2 only contributed v2(n) = 1 which was not pivotal to the decision made, thus user u2 is not taxed.
The Clarke-Tax approach ensures that users have no incentive to lie about their true intentions.
We can brie y show why the Clarke-Tax approach maximizes the users  truthfulness by an additional, simpler example.
Consider two individuals a, b : user a feels that the privacy settings on the picture should be private (option g = 0), and va(0) = 20 is what he is willing to spend in order to keep the picture private among the owners.
User b, on the other hand, is willing to spend vb( ) = 10 to keep the picture public (option g =  ).
We refer to maximum users a and b are willing to spend by va and vb respectively.
Additionally, we refer to the best response for users a and b by  va and  vb respectively.
The charge mechanism in this case is as follows:  a =(0  vb  va <  vb  va    vb (4) Essentially, if user a wins he will be charged an amount that is as equal to the loss of the other owner, user b follows a similar formulation.
In this case, user a s best response is:  va =([0,  vb), [max{0,  vb}, va), va <  vb va    vb (5) Notice that va =  va is always assured to fall in the range for the best response in both cases.
If a and b declare the truth, a option will prevail, and a will have to pay tax to the SN  a = 10 in order to see his option enforced.
If a aims at spending less and declares, falsely,  va = 11, a will still win, but according to equation since 11 > 10, still have to pay a tax  a = 10.
So, underestimating the real value is not going to change the result of the voting process.
Similarly, even if b declares less than what he thinks the real value is, since the numeraire is not going to be reimbursed at him, he is not going to get any advantage by lying.
That is, truthful revelation is weakly dominant, a more general proof is available in [11].
The simplicity of strategy is highly desirable in the design of solutions for this type of privacy problems, where users most likely are going to make intuitive and simple decisions to address their privacy considerations.
Additionally, the Clarke-Tax mechanism satis es several other desirable criteria, including the  Condorcet winner  (a choice that would have beaten every other choice in pairwise votes is guaranteed to be chosen by the mechanism [6]),  independence of irrelevant alternatives  (removal of any unchosen preference from the set of alternatives will not change the outcome [33]) and that the identity of a voter has no in u-ence on the outcome.
The Clark-Tax approach is far from perfect.
One signi cant drawback is the assumption of users  should be able to compute the value of the di erent preferences.
We assume users can map the value to the number of users able to access the shared data, and this is possible using several social network indicators, such as the set of friends, set of common friends, and on several small world network metrics such as node degree, centrality, betweenness, trust paths, mixing patterns, and resilience [31, 3].
The approach proposed in previous section requires manual input for each of the pictures co-owned.
Users may have up to hundreds of pictures, and a signi cant percentage of them may be co-owned.
As such, asking users to bid for each of them may be, in the long run, very cumbersome.
An e ective idea to overcome this limitation is to utilize inference-based techniques; and leverage previous decisions to free the users from the burden of going trough the voting process numerous times.
It is easily veri able, that most users appear in pictures with more or less the same small set of users (typically directly related among each other), and, that the sensitivity of a given picture depends also upon the context in which the picture has been taken.
Building upon these observations, we suggest using tags and similarity analysis to infer the best privacy policy to use for pictures shared among owners who have an history of shared pictures.
As discussed in Section 2, users add words, referred to as tags, to associate a context or a topic with their content.
In the case of pictures, content tags can be added to each picture, or at the album level3.
For simplicity we focus on the case where users add up to one tag each per picture.
As such, for a given picture owned by k users, we associate at most k tags, {t1, .
.
.
, tk}.
This meta-data is used to conduct similarity analysis with pictures already shared by the same set of users.
  t 1,   t 2, ..., For convenience, we represent each picture as a vector of   tags.
That is, let T ={ t n} be a set of pictures   shared among the set of owners OwnU set.
Let t be the picture whose policy is to be de ned.
In order to identify   the best policy to associate with t , we conduct similarity analysis among the pictures in T and Similarity analysis requires two major steps to be undertaken.
First, tags  similarity needs to be conducted.
To be able to utilize similarity metrics, we rely on the informal classi cation system resulting from the practice of collaborative tagging.
This user-generated classi cation system, is referred to as folksonomy [27], and is generally de ned in terms of a collection of posts, each associated with one or more tags.
  t .
Definition 2.
A folksonomy is a tuple F := (U; T; R; Y ) where U, T, and R are  nite sets, whose elements are users, tags and resources, respectively.
Y is a ternary relation between them, i. e., Y   U   T   R. A post is a triple (u; Tur; r) with u   U , r   R, and Tur := {t   T |(u; t; r)  
 By relying on a folksonomy, we can compare two pictures
 we used to identify pictures  potential owners.
ated with each of them.
Tags relatedness can be constructed according to several metrics [32, 23].
In our case, we employ the following notion, which is based on occurrence of tag pairs.
w(t1; t2) := card{(u; r)   U   R|t1; t2   Tur} (6) Based on these notions, we de ne similarity of pictures as the overall relatedness among the tags associated with   t  their similarity is the pictures.
Given two pictures determined as follows.
  t ,   t ,   t  ) = sim( k n Xi=1 Xj=1 w(ti, t  j) (7) Note that similarity is commutative, i.e., sim(   t  ) =   t ).
The equation 7 returns a similarity value ex  t  ,   t , sim( pressed as non-negative number.
Second, once the list of similarity values among all the pictures in T shared by OwnU set is computed, the picture   champ = max{sim( t )} with the highest similarity score is selected.
  t 1), .
.
.
, sim(   t  ), sim(   t n,   t ,   t , Example 5.
With reference to Example 4, let us assume Alice tags the shared picture as party, while Bob uses the word fun and John night.
Suppose that Alice, Bob and John   already share two pictures, say t2 , tagged using other   t1 was tagged using gathering, fun, freely-chosen words.
  game; while t2 using words friend, beer, home.
Let us as  t2 ) = 92.
sume that the sim(   t , its privacy policy will be Since proposed to the three owners.
  t1 is the most similar to   t1 ) = 100 and that sim(   t1 and   t ,   t , The privacy policy associated with champ is prompted to all the users in OwnU set.
If the users agree on the inferred privacy policy, the same is used, and the numeraire intakes is the same as the one originally spent for the championed picture.
If the users do not agree, or a picture signi cantly similar to is not found, the auction mechanism is proposed to the end users.
A temporary policy, chosen among previously adopted ones is then used, until a  nal decision is not taken.
  t


 We have implemented a proof-of-concept social application of the proposed approach for the collaborative management of shared data, referred to as Private Box.
Private Box is fully integrated with Facebook social network platform [13].
Private Box supports the following features: controlled sharing of pictures; automatic detection of pictures  co-owners based on id-tags; collective privacy policies enforcement over shared pictures based on auctions.
Private Box has been implemented in PHP and uses Facebook platform REST-like APIs for PHP and Facebook Markup language (FBML).
REST-like APIs are used to retrieve and prompt all the information related to a Facebook user pro le such as the Facebook user identi er and its friends identi- ers.
FBML is an evolved subset of HTML that gives our Private Box application the same style of Facebook web site.
The information related to a Facebook user pro le such as the user identi er, list of friends identi ers, the user photos and albums identi ers are stored in a MySQL database.
The implementation consists of a set of PHP  les where each  le implements one of the main features of Private Box.
Figure 2 represents the interaction  ow of a user with Private Box application.
First, AddPhotos page allows a user to select those photos from his/her Facebook albums on which he wants to have a  ne-grained control.
Once photos have been selected, Private Box determines the set of potential co-owners of the photos based on the id-tags, as described in Section 3.
Each potential co-owner is noti ed through a standard Facebook noti cation message about the possible co-ownership.
Then, PrivateBox page displays the photos stored in the Private Box, including the pictures added by him/her, and those have been added into Private Box when the user was granted ownership.
Finally, the Auction page is the core of the application, and it enables the collaborative enforcement of privacy policies on co-owned data as it is described in Section 5 (see Figure 2).
Auction page shows the user s updated bid score (i.e., numeraire) each time the user adds pictures, grants ownership or obtains ownership.
Moreover, it allows a user to start an auction using the Clarke-Tax for a co-owned photo by specifying a bid value vi(g) for each possible privacy preference g associated with the photo.
vi(g) represents the perceived bene t of the user by exposing the photo with privacy preference g. The only possible privacy preferences g that are supported by Private Box are  share with co-owners  and  share with friends  because in Facebook it is not possible to connect users based on social relationships other than  friends .
The user can monitor anytime the progression of an auction that the user has started which is not completed yet.
To ensure correctness of the mechanism, however, he/she can only bid once, and cannot view others  bids.
During an auction, the photo under auction is visible only to the co-owners that appear in the photo to avoid that any of the co-owners privacy preferences are violated before the privacy setting that maximizes the social utility F (.)
is determined.
The user can also view the ongoing auctions started by its friends (but not the bids), and choose to join one of them.
When the user joins an auction he/she has to specify the bid score for his/her privacy preference g associated with the photo under auction.
Finally, the user can also view the results of previous completed actions.
Note that only when an auction is completed the user can see the vi(g) speci ed for each privacy preference g by the other users (Figure 2, step 5).
Private Box has additional functionalities, to visualize friends  and co-owners  pictures.
The Co-owner list page, for example, displays the list of the co-owners.
Once a co-owner is selected, the photos the ownership of which is shared between the co-owner and the current user are visualized.
Another supported feature is the ownership request, managed in the Request Ownership page.
The Request Ownership displays a list of pictures where the current user has been tagged, i.e., is a potential-owner.
The user can select the pictures of which he wants to obtain co-ownership.
A noti cation is sent to the user who has uploaded the picture and it is displayed in his/her Notifications tab.
Finally, Friends page displays the pictures that the friends of the user have uploaded in Private Box.
The inference component of the system is not currently implemented, and its deployment is part of our future work.
According to research related to face recognition in online albums there are between 2 to 4 faces per photo [29, 9].
We have evaluated the scalability of the collaborative privacy policies enforcement based on auctions by varying from 2 to 12 the number of co-owners that appear in a photo under auction.
Figure 3 reports the execution times to perform Clarke-Tax algorithm once all the co-owners have placed a bid, while varying the number of co-owners.
In other words, the graph shows the execution time of  nding a privacy setting which satis es each co-owner privacy preference, and of calculating the bid score to be levied to the pivotal users.
The execution time linearly increases with the increase of the number of co-owners because the Clarke-Tax algorithm has to  nd the maximum for function F (.)
over a greater





 ) s m ( e m i t n o i t u c e x







 Number of Co-Owners





 Figure 3: Auction execution times number of co-owners bid scores.
However, the increase is negligible with respect to the number of co-owners.
The execution time is so fast that the collaborative enforcement of privacy policies is transparent to the user.
Security and privacy in Social Networks, and more generally in Web 2.0 are emerging as important and crucial research topics [15, 4, 14, 19].
SNs have been studied by scholars from di erent disciplines: sociologists, HCI, computer scientists, economists etc.
In this section we overview some of previous work that is most relevant to collaborative privacy management for SNs.
Several studies have been conducted to investigate users  privacy attitudes, and possible risks which users face when poorly protecting their personal data [34] in SNs.
Gross et al.
[1] provided an interesting analysis of users  privacy attitudes across SNs.
Interestingly, Ellison et al.
[30] have highlighted that online friendships can result in a higher level of disclosure due to lack of real world contact.
According to Ellison et al. [30] there are ben-e ts in social capital as a result of sharing information in a social network that may limit the desirability of extensive privacy controls on content.
Following such considerations, our proposed approach does not simply block users  accessibility to shared data, but it ensures that sharing occurs according to all the stakeholders  privacy interests.
The need for solutions addressing the problem of information leakage in this context is also reported in [22] where an extensive analysis of the more relevant threats that SNs users currently face is reported.
To cope with security and privacy problems, SNs sites are currently extending their access control based mechanisms, to improve in  exibility and limit undesired information dis-paradigm of access control needs to be developed [18, 15, 5].
A  rst attempt along this direction has been taken by Gollu et al.
[18], where a social-networking based access control scheme suitable for online sharing was presented.
They proposed an approach that considered identities as key pairs, and social relationship on the basis of social attestations.
Access control lists are employed to de ne the access lists of users.
Carminati et al.
[5] have proposed a rule-based access control mechanism for SNs that is based on enforcement of complex policies expressed as constraints on the type, depth, and trust level of existing relationships.
Furthermore, Carminati et al. proposed using certi cates for granting relationships  authenticity, and the client-side enforcement of access control according to a rule-based approach.
In this paper, we employ privacy policies using a simpli ed version of the access rules used by Carminati at al. More recently, Carminati at al. [4] have extended their previously proposed model to make access control decisions using a completely decentralized and collaborative approach.
Their proposed work is orthogonal to the work proposed in this paper.
Our analysis of collaborative privacy management does not relate to the privacy of users  relationships.
Rather, we focus on collaborative approaches for privacy protection of users  shared content.
Recently, Gates [16] has described relationship based access control as one of the new security paradigms that addresses the requirements of the Web 2.0.
Hart et al.
[21] proposed a content-based access control model, which makes use of relationship information available in SNs for denoting authorized subjects.
However, those works do not address collaborative privacy issues.
Another interesting work related to ours is HomeViews [17], an integrated system for content sharing supporting a lightweight access control mechanism.
HomeViews facilitates ad hoc, peer-to-peer sharing of data between unman-aged home computers.
Sharing and protection are accomplished without centralized management or coordination of any kind.
This contribution, although very interesting, is designed around a very di erent environment, and it considers sharing of content without taking into account multi users privacy implications.
Mannan et al.
[25] proposed an interesting approach for privacy-enabled web content sharing.
Mannan et al. leveraged the existing  circle of trust  in popular Instant Messaging (IM) networks, to propose a scheme called IM-based Privacy-Enhanced Content Sharing (IMPECS) for personal web content sharing.
This approach is consistent with our ideas of sharing of privacy controls, and presents an interesting implementation design.
On the other hand, IMPECS is a single-user centered solution: that is, only one user is involved in the decision of whether to share his/her content within his/her trust circle.
Finally, with regards to game theoretic approaches related to our solution, our work is related to [20, 36].
Varian [36] conducts an analysis of system reliability within a public goods game-theoretical framework.
Varian focuses on two-player games with heterogeneous e ort costs and bene ts from reliability.
He also adds an inquiry into the role of taxes and  nes, and di erences between simultaneous and sequential moves.
Grossklags et al.
in [20] generalize [36] and build from public goods literature to model security interactions through three well-known games, introducing a novel game (weakest target, with or without mitigation) for more sophisticated scenarios.
Similarly, in our work we model the collective privacy problem as a new game, using the results from game security economics.
The adoption of our carefully selected technique ensures the design of a N-player game, in which truthfulness and correctness are the winning strategies.
The Clarke-Tax algorithm [8, 7] has been recognized as an important social decision protocol.
The approach has been applied to address problems of di erent nature [10, 12,
 time a voting protocol of this kind is utilized for collective privacy problems.
In [10, 12] the Clarke-Groves mechanism has been introduced into arti cial intelligence, using it to explore multiagent planning.
At each step, instead of negotiating over the next joint action, each agent votes for the next preferred action in the group plan and individual preferences are aggregating using a voting procedure.
Recently, Wang et al.
[37], proposed an interesting secure version of the Clarke-Tax voting protocol.
Following the security requirements identi ed by Wang in [37], we implement a system which guarantees full protection of users  privacy and universal veri ability.
However, Wang s solution heavily relies on cryptographic primitives, and encryption techniques, implying a level of sophistication of users which may not be appropriate in Web 2.0 settings.
In this paper we discussed a novel model for privacy management across social networks, where data may belong to many users.
We presented a theoretical representation of the collective privacy management problem, and proposed a solution which builds upon well-known game theoretical results.
We implemented a tool prototype hosted in Facebook, and carried out performance analysis.
Our next step is to conduct extensive user studies, to assess the users  perspective of this type of approach.
In a preliminary investigation, we observed high interest from users toward approaches allowing users  control over shared content.
As part of future work, we also would like to extend our mechanism to support more sophisticated and  exible policies.
We will investigate policies network-based policies, to include predicates related to the users  geographic locations.
Finally, we will investigate further the implications of our approach in case of revocation or leave of some co-owners.
This work was partially funded by the National Science Foundation (NSF-CNS-0831360) and National Security Agency (NSA H98230-07-1-0231).
We would like to thank Marco Rossi for his useful advices on the Clarke-Tax algorithm and Shitij Kulshreshtha for his help with the development of the private box application.
