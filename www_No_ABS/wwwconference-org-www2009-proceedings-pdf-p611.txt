An increasing amount of online ontologies and semantic data is available on the Web, enabling a new generation of intelligent applications that exploit this semantic information as source of knowledge [5].
However, the growing amount of semantic content also leads to an increasing semantic heterogeneity.
Heterogeneity is something inherent to the current (and future) Semantic Web.
Far from assuming an  ideal  Semantic Web (under the authority of a few high-quality large ontologies, with maximal coverage and expressivity levels), we advocate to learn how to deal with the  real  one instead, Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
where semantics is constructed by di erent authors, for very di erent domains, and with very di erent levels of expres-sivity and richness.
The redundancy problem.
As a direct consequence of heterogeneity in the Semantic Web, the problem of redundancy arises.
It is characterized by the excess of di erent semantic descriptions, coming from di erent sources, to describe the same intended meaning.
For example, if one searches ontological terms to describe the keyword  apple  in the Swoogle semantic web search engine1, 262 di erent results are obtained2.
Each of them represents a possible semantic description for a particular meaning of  apple .
Obviously, this number is quite above the real polysemy of the word  apple .
For example, Word-Net corpus3 identi es only two possible meanings for this word (the fruit and the tree), while Wikipedia4 identi es around 20.
However, it is still far from the number of possible senses given by Swoogle.
If we take a look at the  rst page of results in Swoogle, the redundancy problem becomes evident (e.g., the meaning of  apple  as a kind of  fruit  appears repeated in  ve, out of ten, ontological terms).
In order to solve the problem of redundancy, we propose a method to cluster the ontology terms that one can  nd on the Semantic Web, according to the meaning that they intend to represent.
Achieving such a clustering is not only essential for human users to better apprehend the results of semantic web search engines, and generally the content of the Semantic Web, but is also crucial for applications that rely on the knowledge from the Semantic Web (see e.g.
[5]).
Indeed, eliminating redundancy is a way to improve performance for such applications, but more importantly, knowing which ontological entities refer to the same sense, and which refer to di erent meanings, makes possible the automatic selection of relevant knowledge in a more accurate and precise way.
Proposed solution.
Such an ambitious goal cannot be tackled without relying on an index of the ontological elements accessible through the Semantic Web.
For this, our work is supported by the Watson system [4].
Watson is a gateway to the Semantic Web that crawls the Web to  nd 1http://swoogle.umbc.edu
 (localname:apple) for the query.
3http://wordnet.princeton.edu 4http://en.wikipedia.org access point to online semantic information.
It also provides e cient services to support application developers in exploiting the Semantic Web voluminous distributed and heterogeneous data.
To tackle the problem of redundancy reduction on the Semantic Web, we de ne a clustering technique that we apply to the base of ontological terms collected by Watson, and that creates groups of ontological terms having similar meanings.
The result is a set of concise senses for each keyword one can  nd in Watson.
For example, a search of  apple  will return all the ontology terms that refer to the meaning  fruit , grouped together as a single integrated sense.
Di erent integration levels are possible, to adapt the  granularity  in senses discrimination to the requirements of client applications, and to the different points of view of users.
Let us imagine that a user considers that  Apple  as  electronics label  and  Apple  as  computer company  should be treated di erently, while for many others they intend the same meaning.
Therefore, it is important to make the integration  exible, preserving the original source of information still accessible, just in case the user is not satis ed with the proposed integration.
To achieve this goal of sense integration we do not start from scratch.
As a matter of fact, in [21] we presented a technique that integrates various keyword senses, when they are similar enough.
It was developed for a system that analyzes a keyword-based user query in order to automatically extract and make explicit, without ambiguities, its semantics.
However, this technique was initially targeted to a small scale context, the one given by the few keywords involved in a user query.
Therefore, an additional goal that motivates this paper is to adapt these small scale integration techniques to be used in a much more ambitious context: the Semantic Web reachable by the Watson gateway.
The idea of adapting these integration techniques, to be imbricated in the process of indexing the Semantic Web, leads to another more speci c task, that is a scalability study, in order to check the feasibility of these techniques when applied to a much larger body of knowledge.
In this work we have tackled both the redundancy and scal-ability problems.
First we propose a large scale integration method that applies our semantic techniques, in order to reduce the redundancy problem on the current Semantic Web, and second we contribute with a scalability study to evaluate the feasibility of our proposal.
The rest of the paper is organized as follows.
In Section 2 we present some previous work which serves as background for the present study.
In Section 3, our proposal for large scale redundancy reduction is presented.
Section 4 focuses on how to optimize the integration level and, in Section 5, we detail the scalability study that we have performed in order to check the suitability of our techniques.
Some illustrating examples and further discussion can be found in Section 6.
Section 7 reviews some related work and,  nally, conclusions and future work appear in Section 8.
In this section we brie y present some background work that serves as basis for this study.
In a previous work [21] we have developed a system that analyzes a keyword-based user query in order to automatically extract and make explicit its semantics.
Firstly, it discovers candidate senses for these keywords among pools of web ontologies (accessed by Watson [4] or Swoogle [8]).
Also other local ontologies and lexical resources, as Word-Net [15], can be accessed.
Secondly, it extracts the ontological context that characterizes each candidate sense, and an alignment and integration step are carried out in order to reduce redundancy.
Finally, a disambiguation process is run to pick up the most probable sense for each keyword, according to the context.
The result can be eventually used in the construction of a well-de ned semantic query (expressed in a formal language) to make explicit the intended meaning of the user [3, 21].
It was not the  rst technique to match and merge ontological information from di erent ontologies [7].
Nevertheless, it was quite novel in combining the use of any available pool of online ontologies (without being restricted to a set of prede ned ones) as source of knowledge, as well as in the extraction and use of only the relevant portions of semantic descriptions, instead of the whole ontologies (following the recommendation in [1]).
Also the absence of pre-processing tasks is a remarkable characteristic of this method.
Among all the di erent techniques developed in [21], we will focus, in this work, on the alignment and integration services.
They give us the tools we need to recognize similarities among ontological terms, and to integrate them when necessary.
The latest version of the alignment service receives the name of CIDER (Context and Inference baseD alignER), and it was successfully tested during the Ontology Alignment Evaluation Initiative (OAEI) 2008 [10].
As already mentioned, Watson [4] is a gateway to the Semantic Web.
It makes use of a set of dedicated crawlers to collect online semantic content.
A range of validation and analysis steps is then performed to extract information from the collected semantic documents, including information about the ontological terms they describe and their relations.
This information provides a valuable base of meta-data, employed to create indexes of the semantic content currently available on the Web.
Watson provides both a Web interface for human users5, as well as a set of Web services and APIs for building applications, exploiting the semantic information available on the Web.
These services and this interface provide various functionalities for searching and exploring online semantic documents, ontologies, and ontological terms, including keyword search, metadata retrieval, the exploration of ontological term descriptions and formal query facilities.
This combination of mechanisms for searching, retrieving and querying online semantic content provides all the necessary elements enabling applications to select and exploit online semantic resources, without having to download the corresponding ontologies or to identify them at design time.
Many applications have already been developed that dynamically exploit online semantic content in an open domain thanks to Watson [5].
5http://watson.kmi.open.ac.uk In this section we summarize our proposed large scale redundancy reduction technique.
The idea is to carry out an o line process that analyzes the whole set of ontology terms indexed in Watson (classes, properties and individuals), in order to identify clusters that group the terms according to their intended meaning, by exploring how similar they are.
These equivalent ontology terms are also integrated, to show a uni ed semantic description of the meaning they represent.
A second processing step is carried out at run-time, when the system needs to be adapted, in terms of the granularity of the provided clustering, to accommodate the user s view or the application requirements.
An overview of the approach is shown in Figure 1, and will be explained in the following subsections.
Figure 1: Scheme of the approach
 Let us call C,P ,I the sets of all classes, properties, and individuals, respectively, indexed by Watson.
In the rest of the paper, we will call ontology term (and denote ot) any element of the union set: ot   C   P   I.
We will denote OE the set of all ontological elements that can be accessed by Watson, considering as elements not only ontology terms, but also any other semantic information that characterizes an ontology: hierarchies, axioms, lexical entries, etc.
(we adopt here the characterization of ontology given in [6]).
A keyword k will represent, throughout this paper, an element of the set of strings S (sequences of letters of any length over an alphabet), with any special signi cance.
We will represent the set of all possible keywords as K.
De nition 1.
Watson search.
Let us denote P (C), P (P ) and P (I) the  algebras formed by the power sets (sets of all subsets) of C, P, I respectively.
We de ne the function Watson Search as: wSearch(k) : K   P (C)   P (P )   P (I) from a keyword k to the set of ontology terms retrieved from Watson for this keyword.
In particular, we employ a dedicated function of the Watson API corresponding to the search of any ontological term that matches the keyword in its label or identi er.
The matching that is used is said to be exact, as it would discard any ontological term for which the keyword is only a part of the identi er or label6.
De nition 2.
Extraction.
Given the power set (set of all subsets) of OE, that we denote P (OE), we de ne extraction as a function ext : C   P   I   P (OE) that, for each ontology term, retrieves its neighbouring ontological elements characterizing its meaning.
We are not being more speci c in this de nition deliberately, leaving open the particular criteria of what is considered as part of the  neighbourhood  of an ontology term.
For our purposes, we use the extraction described in [21, 10].
De nition 3.
Ontological context.
Given an extraction function ext, we de ne ontological context of an ontology term ot, and denote OCot, as the element of P (OE) such that OCot = ext(ot) Finally, in the following subsections we will introduce the use of a similarity measure.
Recalling the de nition given in [7], similarity is a function from a pair of objects to a real number expressing how similar they are.
For our purposes, the compared objects will be subsets of OE, even when, for simplicity, we say  similarity between ontology terms  instead of  between their ontological contexts .
Due to the huge amount of available semantic information on the current Semantic Web, it is not computationally feasible to establish comparisons among all accessible ontology terms, in order to cluster them.
We have conceived, instead, a pre-processing step to reduce the space of comparisons, by establishing an initial grouping of all ontology terms with identical labels.
In the following, we call them keyword maps.
simple tokenization techniques to properly match compound terms independently of the employed separators (e.g.
 cup-of-tea  matches  CupOfTea ).
Ontology termsSynonymexpansionintegrationSenseclusteringKeyword mapsSynonym mapsSenses(eachsynonymmap)Watsonsimilarity >threshold?more ont.terms?yesyesnonoSenseExtractionSimilarityComputationrisethreshold?IntegrationSenseClusteringDisintegrationyesnoModifyingintegrationdegreeCIDER modifyintegration?yesWWW 2009 MADRID!Track: Semantic/Data Web / Session: Linked Data613De nition 4.
Keyword map.
Given a keyword k   K, and a set of ontology terms OT   P (C)   P (P )   P (I), we de ne the keyword map of k as the tuple (cid:104)k, OT(cid:105), such that OT = wSearch(k) Here, and in the rest of the paper, we will use the term map to mean any abstract structure that contains, among other information, an associative array (a collection of unique keys and their associated values) of ontology terms, where their URIs act as unique keys.
We subsequently enrich these keyword maps by including all the possible synonym labels, obtained by the Watson synonymy service7.
We call these sets of ontology terms direct synonym maps (or simply synonym maps).
More formally: De nition 5.
Direct synonymy.
We de ne direct synonymy between two di erent keywords k1, k2   K, and denote it as k1   k2, as a relationship that satis es: wSearch(k1)   wSearch(k2) (cid:54)=   That is, di erent keywords are considered synonyms if they return the same ontology term in a Watson search.
De nition 6.
Direct synonym map.
We de ne direct synonym map as the tuple (cid:104)SYN , OT(cid:105), where SYN   K and OT   P (C)   P (P )   P (I), such that  ki   SYN  kj   SYN | ki   kj  ot   OT  k   SYN | ot   wSearch(k ) Up to this point, we have grouped all ontology terms corresponding to an equivalent syntactical expression.
It is symbolized in Figure 2, where a synonym map is represented, not only grouping the ontological terms that correspond to  apple  but also to any of its direct synonyms.
Note that if multilingual ontologies have been accessed, terms in di er-ent languages could be provided as synonyms, as for example  manzana 8: wSearch( manzana )   wSearch( apple ) (cid:54)=  
 Each execution of this process receives as input a synonym map, containing all the ontology terms that are candidates to describe the meaning of certain keyword.
An iterative algorithm takes each ontology term, extracts the ontological context that describes the term in the ontology, and computes its similarity degree with respect to each of the other terms (and corresponding ontological contexts) in the synonym map, by using the measure described in [10, 21].
The extracted ontological context depends on the type of term, and it includes synonyms, comments, hypernyms, hyponyms, domains, ranges, etc.
A reasoner enhances this extraction step, by adding some inferred facts that are not

 Figure 2: Synonym map: expanding the keyword map with synonyms.
present in the asserted ontology.
Although di erent inference levels can be applied, we use simple inferences based on the transitivity of the subclass relation, due to the fact that it has shown a good balance between quality of results and processing time according to our experiments.
Also the access methods in the Watson API provide ways to access classes and subclasses inferred through transitivity (thus saving us much time in local processing and reasoning).
The similarity computation explores the ontological context of the compared terms, providing a measure of how similar they are.
When the obtained value is under a given threshold, we consider them as di erent senses, and the algorithm continues comparing other terms.
If, on the contrary, the similarity is high enough, the terms are considered equivalent and they are integrated into a single sense.
The comparison process is then reinitiated among the new sense and the rest of terms in the synonym map.
If we  nd a new equivalence between the new sense and another ontology term, a further integration is done.
We can consider it as an agglomerative clustering technique [14].
In the following de nitions we clarify what we mean by integration of ontology terms and equivalence between two terms.
De nition 7.
Integration.
Let us call T either C, P or I.
Given a certain extraction function, we de ne integration of n   N ontological terms, and denote int(ot1, ..., otn), as the function int : T n   P (OE) such that, int(ot1, ..., otn) = OCot1   ...   OCotn De nition 8.
Equivalence.
Let us call T either C, P or I.
Given a similarity measure sim between two sets of ontological elements, and a certain threshold, we de ne equivalence between two di erent ontology terms ot1, ot2   T with respect to sim, as a relationship, denoted ot1   ot2, such that  t   T k (k   N) that satis es one of the following: sim(OCot1 , int(ot2, t))   threshold sim(int(ot1, t), OCot2 )   threshold sim(OCot1 , OCot2 )   threshold   ot1   ot2 (in case t =  ).
That is, two ontology terms are equivalent if their contexts are similar enough, or if one of them belongs to an integration which is similar enough to the other.
In general, we use sim(ot1, ot2) as an equivalent notation for sim(OCot1 , OCot2 ).
The output of this process is a set of integrated senses.
Each sense groups the ontology terms that correspond to the same intended meaning, as exempli ed in Figure 3.
We will store the obtained integrated senses in the output structures that we de ne in the following: De nition 9.
Sense map.
We de ne sense map as the tuple (cid:104)SYN , OT(cid:105), where SYN   K, and OT   P (C)   P (P )   P (I) such that,  oti, otj   OT   oti   otj  k   SYN  ot   OT | ot   wSearch(k) Sense maps group together ontological terms that, according to their similarity, are expected to represent the same meaning.
As we have to deal with their integrated ontological information, they are part of the more complete de ni-tion of sense.
The following de nition accommodates (and slightly extends) the idea of sense presented in [21]: De nition 10.
Sense.
We de ne a sense as the tuple (cid:104)SYN , OT, int(OT ), descr, pop, syndgr(cid:105), where (cid:104)SYN , OT(cid:105) is a sense map, int(OT ) is the integration of the involved ontology terms, including also the graph that describes topologically the integration, descr is a textual description, pop (popularity) is the number of integrated ontology terms: pop = |OT|, and syndgr is a value corresponding the resultant similarity degree of the integration.
Our process never  destroys  semantic information because, even when the  nal sense includes an integration, containing only parts of the involved ontologies, it also preserves all original ontological terms in a sense map.
It makes possible that, when needed, a particular application traverses the sense map to reach further information from the original ontologies.
This clustering process is repeated with the rest of the synonym maps, to create eventually a pool of integrated senses which covers all ontology terms in Watson indexes.
As commented before, a threshold is given as input of our system, to decide whether to integrate or not two senses.
Later, in Section 4, we explore possible ways to decide an  optimal  threshold.
However, a  xed threshold leads to a particular integration scheme.
That is, the obtained clusters represent what our application interprets as optimal integration, but may di er from other opinions or necessities.
Therefore, we consider that di erent integration levels should be possible, to adapt the  granularity  in senses discrimination to the requirements of client applications, or to the di erent points of view of users.
We propose a subsequent run-time process that further integrates or disintegrates the initial clusters, as represented in Figure 4.
Without entering into details, the idea is that, given a new threshold, and given the set of senses associated to a certain keyword, the system have to explore how to modify the integration, according to the new threshold.
Figure 3: Sense maps obtained for  apple  Note that, by de nition, a sense map only contains ontology terms of the same type.
Therefore classes, properties and individuals are treated separately, thus never trying to integrate ontology terms of di erent nature.
Figure 4: Integration/disintegration example.
lower integration level.
Each considered sense s is disintegrated, and the sense clustering algorithm is applied again to its ontology terms s|OT .
More than one new sense can be created during the process, covering the meaning that the initial s represented.
A falling threshold (newthreshold < threshold) means a higher integration level.
In this case all senses are candidate for further integration, so the clustering algorithm is applied again on all the senses (not necessarily on their elemental ontology terms, as in the disintegration case).
A number of senses equal or lower than the initial set is created.
As mentioned before, a threshold is used to decide whether two terms are similar enough or not.
However, one question arises: What is the best threshold value to identify a correct synonymy between ontology terms?
We have di erent ways to decide it:
 That is, running our similarity measure with known alignment test cases, and comparing results with the provided reference alignments.
direct or indirectly some human evaluators assess the correctness of our measure.
to di erent integration degrees, we expect di erent execution times as well.
We can try to discover a threshold that minimizes response time.
In the rest of the section, we explore all these possibilities and provide a method to decide the optimal threshold, which relies on the latter point (that is, it prioritizes time response).
Regarding a comparison to benchmarks, we submitted CIDER system (our alignment service) to the OAEI 08 com-petition9, in order to evaluate the capabilities of our semantic similarity measure for ontology matching tasks [10].
We participated in benchmark and directory tracks, obtaining good results in both of them.
As benchmark track was open (organizers provided the reference alignment), we were able to deduce the threshold for the similarity measure (around 0,13 in an interval [0,1]) that lead to the best results.
Nevertheless, we do not trust this value very much, as the nature of the comparisons in the benchmark data sets is quite restricted10, not representing the great variety of real semantic descriptions one can  nd on the Semantic Web.
In general, we expect higher thresholds when integrate senses because, with benchmark test cases, it is highly probable that minimal similar information, as for example same labels, assures that two terms are equivalent (as ontologies are very similar).
However, equal labels are not enough, in 9http://oaei.ontologymatching.org/2008
 some modi cation rules applied, and always in the bibliographic domain.
general, to assess equivalent meanings when more heterogeneous semantic data are involved.
Therefore, we accept the obtained threshold, but only as a lower bound for an optimal one, to be used in a more general scenario.
Another possibility, as it was above mentioned, is to devise an experiment where human evaluation is present to assess the quality of the matching that our measure provides.
We already did such an experiment in [9], where a method to improve a background-based ontology matching technique was proposed.
Without entering into further details (see [9]), our measure was used to add a con dence level to 354 mappings initially provided for the background-based technique, between two ontologies of the agricultural domain.
We applied di erent thresholds to this con dence level, to decide the validity of the mappings, and we compared this to an assessment made by human observers.
A particular optimum threshold did not arise from this experiment, depending the particular choice on the balance between precision and recall one needs.
However, a range of reasonable good values was found between 0.2 and 0.3.
timization We have measured the variation of the response time of the system, during alignment and integration, when the threshold is modi ed.
Our goal is to discover in which ranges our integration works better, in terms of performance.
In fact, we expect di erent response time depending on the threshold, because the number (and size) of clusters di ers with the integration level, therefore consuming di erent time when comparing and merging them during our iterative integration algorithm.
For this purpose we have set up an small experiment with 505 randomly selected keywords, with a number of asso- ciated ontology terms ranging from 2 to 511, which were used as input of our alignment and integration system.
We have experimented with di erent levels of integration (that is, corresponding to di erent thresholds).
We evaluated the integration level as integration level = 1   # f inal integrated senses # initial ontology terms (1) The computer used for this test had the following characteristics: 4 x Intel R(cid:13) CoreTM2 Quad CPU Q6600 at 2.40GHz,
 Figure 5 shows the averaged integration levels obtained.
As expected, the higher the threshold, the lower the integration level, which is consistent with the discussion in Section 3.5.
In Figure 6, the average time that our similarity computation and integration processes took in the experiment, for di erent thresholds, is shown.
Analyzing the results, we can see two local maximum values in the graphic, corresponding to the highest and the lowest integration levels respectively.
In fact, each iteration, in our integration algorithm, needs to compare a new ontology term to the others already examined.
If there are no (or few) clusters, this number of comparisons could be high, thus consuming more time.
On the contrary, if there is a high integration level, the time is spent merging ontological data, as well as comparing each new integration with the rest automatic, not needing human supervision.
tion level is dependent on the user s view and on the requirements of particular applications.
Therefore, it is reasonable to chose an initial integration that can be established quickly, letting the user, or client application, chose later how it should be modi ed to  t his particular needs.
Therefore, we propose to automatically train the system with a large number of test cases, analyzing the response time with respect to the threshold.
Then, we deduce the threshold that minimize the time value, that will be proposed as the optimal threshold of our system.
Finally, the information of subsequent  manual  tuning of the threshold can be stored, and utilized to improve the initial threshold by using machine learning schemes.
In this section we explore the scalability of the techniques we presented in [21], when used to align and integrate ontological information extracted from Watson indexes.
This way, we can check whether the scenario described in Section 3 is feasible or not.
The method developed in [21] to integrate senses of user keywords, was conceived initially for a context of use where only a reduced set of keywords (corresponding to a user query) were involved.
For this case, the set of tests we run showed a good behaviour of the method, in terms of performance.
However, the new goal we describe in this paper is much more ambitious, as the context of application grows enormously in scale.
Therefore, a scalability study is needed in order to  gure out whether our integration algorithm is applicable to a large scale context (the whole set of online ontologies indexed by Watson) or not.
Generally speaking, we say that a system is scalable when the cost per unit of output remains relatively constant with proportional changes in the number of units (or size) of the inputs.
There are two ways of analyzing the cost in time of an algorithm: theoretically or empirically.
We have chosen the second way, because we already dispose of an advanced implementation of the system.
To empirically study the response time of the system, we need to run it with a statistically signi cant sample of input data.
Then, we will infer the performance of the system from the obtained results.
Experimental setup.
For these experiments, we used the set of synonym maps that Watson indexes hosted on June 2008, from which we randomly selected a 10%, taking a rep resentative keyword from each map to be used as input for our experiment, resulting in an initial number of 28800.
After removing the keywords with only one ontology term associated (it makes no sense to apply our integration process on them)11, we counted 9156 di erent keywords, associated to 73169 di erent ontology terms to be clustered in this experiment.
We used a threshold = 0.27 during our sense clustering step.
due to di erent technical reasons, was also removed (e.g., not available in Watson indexes at the time the test was run).
Figure 5: Integration level with di erent threshold.
Figure 6: Response time with respect to threshold.
of the ontology terms again.
The results show that the best performance corresponds to an integration level around 0.5.
In particular, we found the best performance for a threshold value of 0.22 in this experiment.
As conclusion of our experience, described in previous paragraphs, we advocate for an optimal threshold established to maximise performance.
The main reasons are:
 Figure 6 shows how an optimal threshold selection can easily double or triple the speed with respect to other thresholds.
is compatible with the range of good ones found by comparing the results with human evaluations (Section 4.2).
ber of humans, or reference alignments, in order to assess the behaviour of a similarity measure.
On the
 tics: 2 x Intel R(cid:13) Xeon R(cid:13) CPU X5355 at 2.66GHz, 6GB RAM (with Red Hat Enterprise Linux 5).
The target of this experiment is to study the response in time of our system, with respect to the number of ontology terms per keyword that have to be processed during the clustering step.
We expect that the greater the number of ontology terms associated to a keyword, the longer it takes to be integrated.
Our objective is to  gure out whether this time is always limited and controllable, even for a high amount of terms, or not.
It is something hard to predict in advance, due to the great variability of input data, in terms of the quantity and quality of semantic information.
In Figure 7 we see how  nal results distribute.
It shows the total response time per number of ontology terms in each keyword map.
Each point corresponds to a test case (a total of 9156 cases were run, one per di erent input keyword).
We have adjusted the given results to di erent curves,  nding that the best  t is a linear regression with equation y =
 the distribution of time responses, the mean value of all cases is 2.5 seconds, and the median value is 0.5 seconds
 As an additional result of our scalability study, we were interested in con rming that, due to the use of the Watson inverted indexes, the size of the accessed ontologies has not e ect on the response time of our techniques.
Our results con rmed it, as it is shown by the cloud shaped dispersion in Figure 8 (zoomed for convenience), where the time result according to the size of the involved ontologies appears.
After having statistically analyzed the results, we have not found any dependence between ontology size and response time of our system.
This independence between the size of ontologies and the response time of our system, even if expected, shows a remarkable advantage of accessing the ontological information by using Watson, instead of other sources that requiere a preliminary load step before using the ontologies.
In that cases, the time taken for downloading (and querying) the ontologies depends on their size.
Figure 7: Time vs. number of initial ontology terms per keyword.
From the given results we observe: 1.
99% of cases corresponds to keywords with less than
 system s response takes no more than a few seconds, thus con rming its capacity of being used for real-time purposes (as it was  rst conceived in [21]).
be approximately predicted for any value of the input, as the ratio processing time/num.
ontological terms remains constant.
As our alignment and integration algorithms show a controllable behaviour, we can focus more on technical optimization issues to improve performance (parallelization, cache schemes, etc.)
when the real application scenario requires it (e.g., it is expected that Watson indexes continue growing with time).
Figure 8: Time vs. size of accessed ontologies.
The main target of our test series has been to study the scalability and performance of our integration techniques.
In this section we want to give also an idea of the quality of the results, by inspecting some selected clustering examples.
A larger human-based evaluation is pending as future work.
Meanwhile, we have inspected the results given with these polysemic words:  turkey ,  plant , and our motivating example  apple .
Due to space limitations, we only detail the  rst example, summarizing the results of the other two.
Case 1: Turkey.
After synonymy expansion, the synonym map (cid:104)SYN , OT(cid:105) contained the keywords SYN = {Turkey, T urkei, T urkiye}, and a set of 58 ontology terms: |OT| = 58.
Our sense clustering step was carried out with threshold =
 into 9 di erent senses (integration level = 0.84, according to Equation 1).
Table 1 summarizes some aspects of the integrated senses12.
at http://islab.hanyang.ac.kr/damls/Country.daml, http://139.91.183.30:9090/RDF/VRP/Examples/tap.rdf, http://reliant.teknowledge.com/DAML/Economy.daml,








 type class class class class individual individual individual individual individual pop syndgr

















 comments It appears as  a country  in Country.daml ontology.
In tap.rdf ontology (however its ontological context does not clarify its meaning).
Integration of  turkey  as  a livestock  from the Economy.daml and Economy.owl on-tologies.
Integration of  turkey  as  a bird  from the ontosem.owl and mesh.owl ontologies.
It integrates many instances of  turkey  as  light meal .
It integrates many instances of  turkey  from annotated conversations in social webs, most of them referring to meanings similar to  place ,  country ,  area .
Integration of  turkey  as an instance of  a place , from di erent versions of epitaph.rdf ontology.
It integrates  turkey  as an instance of  document  in annotated blogs.
It comes from a test ontology, being an instance of  Foo .
Table 1: Summary of obtained senses for  turkey .
In order to experiment with di erent integration levels, we decreased the threshold to 0.17, resulting in an integration level = 0.88.
In particular senses 6 and 7 were integrated into a single one (with pop = 37, syndgr = 0.38), as well as senses 3 and 4 (pop = 4, syndgr = 0.38).
The others remained the same.
On the contrary, if we increase the threshold to 0.37, the number of obtained senses rises to 12 (integration level decreases to 0.79).
Particularly, sense 4 is split in two, and sense 5 is split in three new senses.
Case 2: Plant.
The obtained synonymy expansion was {plant, plants}.
Watson retrieved 52 ontology terms for these keywords.
After running our clustering step, with threshold = 0.17, 16  nal senses were obtained: 14 classes and two indi- viduals (integration level = 0.73).
The most remarkable one was a class that integrated 32 ontology terms, all with the meaning of  plant  as a kind of  living organism .
Case 3: Apple.
Watson retrieved initially 38 terms for this keyword.
The synonym expansion did not add signi cant synonyms.
Our sense clustering step, with threshold = 0.27, grouped the initial ontology terms into nine di erent senses (integration level = 0.76):  ve individuals and four classes.
Among the possible senses, we  nd  apple  as  a fruit , as an  ingredient , as an annotation tag in Flickr13, and as an instance of  document  in many annotated blogs (most of them refer to the meaning of  the electronics company ).
Discussion.
Our examples show the ability of our system to reduce the number of repeated senses one can discover on the Semantic Web, to represent the meaning of a keyword.
It is specially useful to cluster terms form di erent versions of the same ontology.
Also references in social tags, where semantics is quite poor, are easily grouped.
We have also found that very di erent meanings are not wrongly mixed easily (e.g.,  turkey  as  country  with  turkey  as  food ).
On the other hand, it is hard to obtain a total integration of all terms of the same type referring to the same meaning.
It is due to the very di erent semantic descriptions some-http://reliant.teknowledge.com/DAML/Economy.owl, http://morpheus.cs.umbc.edu/aks1/ontosem.owl, http://www.berkeleybop.org/ontologies/obo-all/mesh/mesh.owl, and http://139.91.183.30:9090/RDF/VRP/Examples/epitaph.rdf 13http:// ickr.com/ times used to model the same concepts.
We have also found that some expected meanings do not appear among our results (e.g.,  apple  as  a tree ).
It is a consequence of the still limited coverage of the current Semantic Web.
Our proposal represents the  rst large-scale e ort, so far, to provide a pool of clusters of cross-ontology terms free of redundancy.
Therefore, direct comparison with other alternatives is quite di cult.
Clusty14, for example, is a metasearch engine that groups search results in clusters that group pages about a similar topic.
The general idea could be equivalent, but our search domain is the Semantic Web instead of the Web, and we are interested in clustering senses instead of web pages.
Regarding other systems that index online semantic content [17, 8, 11], neither of them have incorporated redundancy reduction techniques yet.
Technologically speaking, even though we reuse ontology matching and integration techniques, our work is neither solely about ontology matching [7, 6], nor about ontology integration or merging [18, 13, 12], in the sense of obtaining a new ontology from matched ontologies [7].
It is closer, however, to the idea of ontology construction from online ontologies given in [1].
In his work, Alani proposes a general strategy to dynamically segment, map and merge online ontologies in order to support ontology construction.
Our target is not the same, but we share some commonalities, and we think that our proposal could indeed support and bene t such kind of systems.
Regarding our particular techniques, notice that our definitions of extraction, ontological context and equivalence, given in Section 3, are general enough to accommodate other similarity measures, always that they come with a clear notion of neighbourhood, as for example in [19] (where they propose the idea of virtual documents, containing neigh-bouring information, to describe the intended meaning of the ontology terms).
Also, other segmentation techniques [20,
 main target of these techniques is usually to reduce the size of large ontologies, to make them treatable and scalable, instead of characterizing a single sense.
Furthermore, the good results given by the CIDER alignment service, when compared to others in the OAEI contest [10], con rm the 14http://www.clusty.com applied to ontology matching tasks.
We have proposed a method to perform a large scale integration of ontology terms, to cluster the most similar ones into integrated senses, when indexing huge amounts of online semantic information.
The method is  exible enough to dynamically modify the integration level according to the user point of view, by tuning the similarity threshold.
We have also explored possible methods to  nd an optimal integration level,  nally proposing one that is based on performance optimization.
The work presented here is quite innovative in some aspects.
For example, it is the  rst large-scale e ort, so far, to provide a pool of clusters of cross-ontology terms free of redundancy (to a certain extent).
A scalability study has been performed in order to investigate the feasibility of our proposal.
The results show that our techniques lead to processing times that are controllable and predictable, thus making scalability possible.
As future work, we plan to complete the implementation of the system, only partially covered by our current prototype.
It will be provided as a service in Watson eventually.
In this way, many applications, that exploit Watson to dynamically access online semantic content, will take advantage of our method.
We plan also to perform a large scale test with human opinion, to judge the quality of the obtained senses.
Acknowledgments.
We thank Jordi Bernad for his valuable comments on our mathematical formalization.
This work is supported by the CICYT project TIN2007-68091-C02-02, and the Government of Aragon-CAI grant IT17/08.
Mathieu d Aquin is partly funded by the NeOn project sponsored by the European Commission as part of the Information Society Technologies (IST) programme.
