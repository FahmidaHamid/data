Recommender systems enable users to navigate vast collections of items.
Amazon suggests products users may like based on their ratings, clicked items, and purchased items [17].
Users of Digg receive news articles based on other articles they  nd interesting [25].
Members of Net ix receive movie recommendations based on their movie ratings [3].
In each of these scenarios, recommender systems choose a few items a user will like most from among thousands, or even millions, of possibilities.
This task, which we call recommend, is one of two tasks supported by nearly all recommender systems [26].
For the second common task, predict, recommender systems predict which rating a user will assign to a particular item.
For Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
example, a user of  Rate Your Music 1 might receive a predicted rating of 4.2 out of 5 stars for the album  White Blood Cells  by the White Stripes based on a  ve star rating for  In Rainbows  by Ra-diohead.
For both the recommend and predict tasks, recommender systems help users understand an unknown relationship between themselves and an item by comparing a user s behavior (e.g.
album clicks and ratings) to patterns of behavior in other users.
Tagging systems offer users an alternate way to address the recommend and predict tasks.
Shirky suggests that since tags are created by users, they represent concepts meaningful to them [31].
Because tags are easily comprehended by users, tags serve as a bridge enabling users to better understand an unknown relationship between an item and themselves.
In previous work, we validated this relationship by  nding that certain types of tags help users to  nd and make decisions about items [29].
For example, Alice2 is a real user in the MovieLens movie recommendation community we study.
She enjoys animated movies, and has assigned  ve star ratings to  Shrek,   Pinnochio,  and  Toy Story .
If Alice visits the web page for the movie  Ratatouille  she would see that 5 users have applied the tag animated to it.
Based on these tags, she might decide she would enjoy the movie (the predict task).
Alice might then click on the tag pixar to discover the related movie  The Incredibles  (the recommend task).
Recommender algorithms that incorporate tagging information promise to combine the best elements of both types of systems.
Lamere refers to these tag-based recommendations as  tagomenda-tions  [16].
We similary refer to tag-based recommender systems as tagommenders.
Tagommenders offer the automation of traditional recommender systems, but retain the  exibility of tagging systems.
Schafer et al. found that users enjoy specifying feedback about items along a variety of dimensions [27].
Tagommenders enable recommenders to use the dimensions of items that users consider most important.
In this paper, we design tagommenders inspired by the way in which humans use tags to evaluate items.
Figure 1 presents the model we explore in the movie recommendation domain in this paper.
The bottom of the  gure shows that traditional recommender systems infer users  preferences for movies based on their movie ratings.
The top half of the  gure describes the two main components of tagommender algorithms.
First, we infer users  preferences for tags based on their interactions with tags and movies.
Second, we infer users  preferences for movies based on their preferences for tags.
We de ne a user s preference for a tag as the user s level of interest in movies exhibiting the concept represented by the tag.
For example, Alice indicated that she likes animated movies and swashbucklers, but dislikes movies about serial killers.3 Her (dis)interest 1http://www.rateyourmusic.com

 licious5, must generate recommendations based on these implicit signals.
Other systems with tags, such as Amazon, support explicit signals of interest in the form of item ratings.
Our last two research questions explore the performance of tagommenders in both types of systems.
RQ2: How well do tagommenders perform in systems with-RQ3: How well do tagommenders perform in systems with out ratings?
ratings?
Fig. 1: Our model of movie tagommenders.
Traditional recom-mender systems (bottom) generate predictions for movies based on movie ratings or clicks.
Tagommenders (top)  rst infer users  preferences for tags (upper left).
Based on these inferred preferences for tags, tagommenders generate movie recommendations (upper right).
Users  preference for tags can be inferred based on signals of interest in tags (tag applications, searches), or signals of interest in items (movie ratings, clicks).
In order to use item signals to infer users  preferences for tags, they must be translated to tag signals (upper left).
in these concepts may have in uenced her 4.5 star rating for  The Mask of Zorro,  and her 1.5 star rating for  Hannibal.  In the  rst half of our paper we explore algorithms that infer users  preferences for tags: RQ1: Can we infer users  preferences for tags?
We consider tag preference inference algorithms that analyze signals of a user s interest in a tag or movie (Figure 1, upper left).
For instance, Alice s application of the tag shipwrecked may suggest that she is interested in swashbucklers (a signal of tag interest).
In addition, Alice s rating of 4.5 stars for  The Mask of Zorro  and her click on a hyperlink leading the movie  The Pirates of Pen-zance  may also have been a result of her liking for swashbucklers (signals of movie interest).
One other signal of tag preference we consider is a tag s quality.
We de ne a tag as high quality if it helps the community understand an important aspect of an item.
For instance, Alice considers the tags serial killer and animated as high quality tags that capture important movie concepts but she considers sure thing as a low quality tag.4 Since high quality tags capture important movie concepts, we evaluate an algorithm that infers users  preference for a tag based on the tag s quality.
We evaluate eleven tag preference inference algorithms using 118,017 tag preference ratings collected in a user survey on the MovieLens movie recommender website.
In the second half of this paper we analyze algorithms that predict users  ratings for movies based on their preferences for tags (upper right of Figure 1).
We separate our algorithms by the type of signals they use: implicit only, or both implicit and explicit.
Implicit signals such as clicks and searches occur during users  natural interactions with tags and items.
Tagommenders for sites that do scribe in Section 3
 based quality ratings we describe in Section 3 We evaluate RQ2 and RQ3 using movie ratings and tags created by MovieLens users.
Our work offers three contributions to researchers and practitioners:   We develop and evaluate algorithms that infer users  preferences for tags.
  We develop tag-based recommendation algorithms that infer users  preferences for movies based on their inferred preferences for tags.
  We evaluate the end-to-end predictive performance of tag-ommender algorithms that combine tag preference inference algorithms with tag-based recommenders.
We believe this work to be important for two reasons.
First, we hope that sites with an abundance of tagging activity, such as Delicious or  ickr6, can use our algorithms to improve item recommendations.
Second, we believe that tagommenders offer a  exible and comprehensible alternative to traditional recommender systems.
Many  rst generation recommenders such as the GroupLens [24] Usenet recommender employed user-based algorithms: given a particular user, recommend movies that similar users like.
Sarwar et al. s item-based algorithm transposed this model: predict users  ratings for an item based on their ratings for similar items.
During the Net ix Prize, two trends emerged in recommender systems research [3].
First, researchers adopted Simon Funk s7 singular value decomposition algorithm (SVD) due to its accuracy, ef ciency, and ease of implementation [8].
Second, researchers such as Bell et al. combined the output of multiple recommender algorithms to improve performance [2] .
Our research differs from these collaborative  ltering algorithms by using tags as intermediary entities.
Collaborative  ltering (CF) algorithms such as the user-based, item-based and SVD algorithms rely on patterns between user ratings, but do not use data about items.
They do not, for instance, know that  Toy Story  is an animated movie.
Balabanovic et al.
were among the  rst researchers who investigated content-based systems that make use of the data about an item such as a movie s genre.
Other researchers have studied methods for combining collaborative  ltering with content-based systems [22] .
Our research extends existing techniques for content-based recommendation in two ways.
First, since tags are maintained by community members instead of expert editors their quality varies [29].
We explore how estimates of tag quality improve recommender performance.
Second, unlike earlier content-based algorithms, we automatically learn relationships between tags and movies based on inferred tag preferences and movie ratings.
5http://del.icio.us 6http://www. ickr.com
 have consistently referred to him by his pseudoname, so do we.
of tag interest build on existing work in user pro le extraction for content-based recommenders [1] and web-based systems [21].
However, our algorithms face challenges not present in other domains.
While other systems  data are created by domain experts, tags are created by ordinary users.
Koutrika et al. suggest that this transfer of control allows tagging systems to be  threatened  by both  malicious  and  lousy  taggers [15].
We differ from previous work on pro le extraction by designing algorithms that are robust to differences in tag quality.
Although public bookmarking systems such as Fab [1], and Pharos [4] have been available since the 1990 s, Millen et al. point to tagging as a key reason current social bookmarking systems have enjoyed greater success [19].
In early academic research on tagging communities, MacGregor and McCullogh [18] explore the relative merits of controlled versus evolved vocabularies, arguing that evolved ontologies engage users but lack the precision of their controlled counterparts.
In earlier work, we show that the tags a user sees in uence the tags they create themselves [29].
We also classify tags as generally factual, subjective, or personal (intended for the tag creator themselves), and  nd that users generally prefer factual tags over subjective tags and strongly dislike personal tags.
Our work furthers these studies of tagging communities by analyzing how tags can be incorporated into recommender systems.
In earlier work we explore user interfaces that help systems determine a tag s qulity [28].
In of ine results we  nd that thumb rating feedback signi cantly improves a system s ability to identify good tags compared to simple implicit signals of tag quality.
We verify our results using an online study in the MovieLens movie recommender system [30].
We extend this work by using tag quality (among other signals) to infer users  preferences for tags.
Several researchers have explored algorithms that recommend tags for an item [29] [13].
Hayes et al. examine how tags can be used to cluster bloggers and posts and suggest that tags can be used as a gold standard for cluster coherency [10].
Brooks et al. propose hybrid algorithms drawing on both blog tags and blog text to accurately cluster blogs [5].
We build on this work by providing an end-to-end algorithm that infer preferences for tags and generates movie recommendations based on those preferences.
Three researchers have directly studied tag-based recommenders.
In a blog post, Lamere suggests a metric for identifying similar sets of items in tagging sites by measuring the cosine similarity of the tags applied to items [16].
Diederich et al. describe an exploratory study in which users create tag pro les corresponding to their interests and receive recommendations based on those tag pro les [7].
Niwa et al. propose a cluster-based algorithm for recommending webpages based on the pages users have tagged, and the tags applied to web pages [20].
All three researchers base their recommendations on the similarity of TF-IDF tag pro le vectors.
We extend this existing research in three main ways.
First, we investigate 11 different signals of a user s interest in tags, including tag searches, and item ratings.
Second, we explore  ve different algorithms for calculating item preferences based on tag preferences.
Third, we conduct an empirical evaluation using 118,017 star-ratings of tag preference and 1,720,390 star-ratings of item preference.
We conduct our analyses using data collected from the Movie-Lens website.
MovieLens primarily serves as a movie recommender system.
Users receive movie recommendations in exchange for rating movies on a  ve star scale.
MovieLens was created in 1997 and maintains an active base of approximately 1,200 users per week.
We conduct our analyses using  ve sets of data from MovieLens Fig. 2: Tags as they appear on the MovieLens search results screen.
Fig. 3: Tags as they appear on the MovieLens movie details screen.
described in Table 1.
We now describe the data contained in each dataset along with details not speci ed in the table.
Movie Ratings: MovieLens users rate movies on a one to  ve star scale.
Movie Clicks: We logged clicks on links to detailed information about a particular movie for approximately 17 months starting in December 2006.
Tag Applications: MovieLens members can tag movies, and use tags contributed by others in the community to  nd and evaluate movies.
MovieLens users most commonly interact with tags through the search results screen (Figure 2) and movie details screens (Figure 3).
The movie details screen displays movie information including up to 30 of a movie s tags.
Since we introduced tagging features to MovieLens in January 2006, MovieLens members have created 84,155 tag applications resulting in 13,558 distinct tags (a tag is a particular word or phrase, a tag application is a three way relationship between a user, tag, and item).
Further details of MovieLens and the MovieLens tagging system can be found in [29].
Tag Searches: Tag searches are textual searches for tags, or clicks on tag hyperlinks.
1,000 users have searched for at least  ve distinct tags.
107 users have searched for at least 50 distinct tags.
Tag Preference Ratings: In our model for tag-based recommendation, we  rst infer users  preferences for tags.
In order to evaluate our tag preference inference algorithms, we conducted a survey of tag preferences for MovieLens users.
We emailed invitations to 8,361 active users.
995 users responded (11.9% response rate).
In the survey, we showed each user a collection of tags, and asked them to  estimate how much  they  would like movies with each tag using a one to  ve star scale, or unsure.  We asked each user to complete at least 60 tag ratings, but gave them the option to complete more if they wished.
In total, users supplied 118,017 ratings for 9,889 distinct tags (mean 117 tags per user, median 78).
800 users completed the requested 60 ratings, while seven provided more than 1,000 ratings.
The breakdown of the survey responses by star ratings is as follows: 6% (7,641) were 5 stars, 17% (20,597) were 4 stars, 22% (26,499) were 3 stars, 13% (15,135) were 2 stars, 13% (15,155) were 1 star, and 28% (32,990) were unsure.
The average tag rating was 2.89.
The unsure rating was used differently by different users.
Among the 25% of users who used the unsure rating most often, unsure ratings accounted for 43% of ratings.
Among the 50% of users who used the unsure rating least often, unsure ratings accounted for only 18.3% of ratings.
Pruning: Although we draw on all data when analyzing tag preference inference algorithms in Section 4, we pruned the data sets the number of the entities the dataset contains.
Num-users is the number of users that generated those entities.
For example, the  rst two columns in the third row indicate that 84,155 tags have been applied by 3,582 users.
The last two columns indicate the same numbers after the pruning we apply for our analyses in the second half of this paper.
before pruning dataset movie ratings movie clicks tag apps tag searches tag pref ratings count




 num-users




 after pruning count num-users



 n/a



 n/a Fig. 4: Inferring a user s preference for a tag based on her direct interactions with a tag such as her searches for a tag and her applications of a tag.
for our analyses of tag-based recommendations in section 5.
In order to reduce the computational requirements of our analyses, we focused on a set of movies with a minimum threshold of tags, and a set of users with a rich pro le of MovieLens behavior.
We began pruning the tag-recommendation dataset by selecting movies that had been tagged with at least  ve distinct tags.
We wanted to focus on tags that represented concepts applicable to multiple movies, so we required that each tag be applied to at least  ve movies.
We iteratively repeated this pruning until we reached a stable set of movies and tags.
After movie ratings, movie clicks are the most abundant source of behavioral information we have for MovieLens users.
Since we wanted to explore the effectiveness of tag-based recommendations for domains without tag ratings, we only included users that had clicked  ve or more movies.
After pruning, 1,720,390 ratings remained from 5,637 users for
 we also track tag applications created by users in the pruned set.
In total, 1,315 users in the pruned set applied 50,060 tags (mean of 38 tags per user, median of 2).
More statistics are shown in Table 1.
In this section we address RQ1: RQ1: Can systems infer users  preferences for tags?
We consider two approaches to inferring tag preference.
First, algorithms can directly infer a user s preference for a tag based Fig. 5: Inferring a user s preference for a tag indirectly based on her interactions with items having a tag such as her rating of items with the tag.
on her direct interactions with the tag (Figure 4).
For example, if Alice searches for animation, she is probably interested in it.
Second, an algorithm may indirectly infer a user s preference for a tag based on her interactions with items having the tag (Figure 5).
For example, Alice has assigned  ve-star ratings to three movies tagged with animation:  Shrek ,  Pinnocchio , and  Toy Story .
Based on these movie ratings, we may infer that she would enjoy other movies tagged with animation.
We consider three algorithms based on direct signals of a user s interest in a tag (Figure 4).
Users may be more interested in tags they themselves apply.
Tag-applied infers higher preference for those tags a user has applied.
Users may also be interested in the tags for which they have searched.
Tag-searched infers higher preference for tags for which a user has searched.
Both tag-applied and tag-searched use a simple 0 or 1 numeric coding.
We also use a third implicit tag signal: a tag s quality (tag-quality).
As we mentioned in the introduction, a user s preference towards a tag may be correlated with the tag s quality.
In order to examine this relationship, we include the best performing tag quality prediction algorithm from our previous research [30].8 The algorithm draws on many signals of tag quality including the number of users who apply a tag, and the number of users who search for a tag.
In order to make our results more generalizable to other sites, we do not draw on the tag quality thumb ratings unique to MovieLens.
All tag preference algorithms translate between a score (i.e. 0 or 1 for tag-applied) and a one-to- ve star inferred tag preference according to a simple linear relationship.
This relationship is estimated by performing a least-squares regression between the algorithm scores and users  actual preference for tags as reported in the survey.
In this section, we explore algorithms that calculate a user s preference for a tag based on her interactions with movies related to the tag (Figure 5).
We found that our inference algorithms performed better when they took into account the relevance of a tag to a movie.
For example, if we wish to infer a user s preference for the tag cars we might treat her interactions with each of 46 movies tagged with cars as equally important.
However, cars may be more relevant for certain movies than others.
For instance, cars accounts for 9 of the
 applications for  Cast Away.  To account for differences in a tag s relevance, each inference algorithm in this section includes a weighting quantifying the relevance of a tag to a movie similarly to Vig et al. [32].
As a measure of a tag s relevance to a movie, we use the tag quality measure we discussed in the previous section [30].
We found that applying a sigmoid transformation improved the performance of weighting by tag quality.
If w(m, t) represents the relevance weighting between a movie and tag:9 w(m, t) =
 e tag-quality(m,t) .
that the tag quality-based weighting performed within 3% of the best novel weighting algorithm (a graphical bayesian network).
The tag quality-based weighting also outperformed TF-IDF.
For simplicity, we choose to build on our existing research instead of introducing a new weighting scheme.
sum to 1.0.
We explore six different algorithms for calculating a user s preference for a tag based on her interactions with items having the tag.
The six algorithms can be grouped according to the type of signal of movie interest that they use.
The  rst two algorithms (movie-clicks, movie-log-odds-clicks) use clicks on movie hyperlinks as a signal of a user s interest in a movie.
The third and fourth algorithms (movie-r-clicks, movie-r-log-odds-clicks), analyze the speci c movies a user chooses to rate.
The last two algorithms (movie-ratings, movie-bayes) draw on a user s numeric ratings for movies.
Movie-clicks: The movie clicks algorithm is based on the hypothesis that users click on movies with tags they like more often.
This algorithm estimates a user s preference for a tag based on the fraction of clicked movies that have the tag.
Instead of weighting each movie equally, we weight movies according to the relevance weighting based on quality we described above: If clicked(u) is the set of movies clicked by user u, then:
 movie-clicks(u, t) = m clicked(u) |clicked(u)| .
w(m, t) Movie-log-odds-clicks: Similar to movie-clicks, movie-log-odds-clicks assumes that users click movies with tags they like more often, but it adjusts for overall tag popularity.
Movie-log-odds-click uses the log odds metric to compare the movie-speci c tag frequency to the overall tag frequency.
If M is the set of all movies, and Mt is the set of all movies with tag t, logit(p) = log movie-log-odds-clicks(u, t) = 1   p .
    p
 logit(movie-clicks(u, t))   logit( m Mt w(m, t) ).
Movie-r-clicks: Users  movie viewing decisions may correlate with their tag preferences.
For instance, a user may choose to watch a movie because it contains  violence.  We assume that users have watched the movies they have rated.
Based on this, we consider a version of the movie-clicks algorithm that substitutes the movies a user has rated for the movies they have clicked.
Movie-r-log-odds-clicks: Similar to movie-r-clicks, movie-r-log-odds-clicks uses the movie-log-odds-click algorithm, but substitutes the movies a user has rated for the movies they have clicked.
Movie-ratings: Perhaps users rate movies with a particular tag consistently.
For example, Alice consistently rated three animated movies  ve stars.
Movie-ratings draws on this signal by predicting that a user s preference for a tag is the user s average rating for movies with the tag.
As with the previous inference algorithms, we draw on tag quality for the tag relevance weighting w. If ru,m is user u s rating for movie m:

 m Mt w(m, t)   ru,m .
m Mt w(m, t) movie-ratings(u, t) = The sums in both the numerator and denominator ignore movies the user has not rated.
Movie-bayes: Movie-bayes is a bayesian generative model for how users rate movies with a particular tag [12].
Figure 6 describes the model.
For every user u and tag t we select a user-tag-speci c distribution N ( t,u,  t,u) from hyper-distributions.
For each rating ru,m by user u for movie m with tag t, the tag may be a relevant Fig. 6: Movie-bayes is a generative model for how users rate movies with a particular tag.
For every user u and tag t we select a user-tag-speci c distribution N ( t,u,  t,u) from hyper-distributions.
For each rating ru,m by u for movie m with tag t, the tag may be relevant, or irrelevant for the movie.
If t is relevant, the rating is chosen from the user-tag-speci c distribution.
If t was not relevant, the rating is chosen from the user s background ratings distribution N ( u,  u).
We estimate the hyperparameters for hyperdistributions N ( ,  ) and  ( k,  ) using the empirical bayes methodology.
We calculate the expected parameters for a particular user and tag,  t,u and  t,u, using MCMC.
tag for the movie, or an irrelevant for the movie.
If t is a relevant tag, the rating is chosen from the user-tag-speci c distribution.
If t is not relevant, the rating is chosen from the user s background ratings distribution N ( u,  u).
We adopt the bayesian paradigm of considering all possible user-tag-speci c normal distributions[9].
For each distribution, we calculate the probability that that distribution generated the user s ratings.
We then take the expectation of the mean for a particular tag and user ( t,u) over all possible distributions by applying bayes rule based on the user s ratings.
In the following formulas Ru,t is the set of ratings by user u for movies tagged with t, and E(X) denotes the expectation of random variable X.  ( k,  ) and N ( ,  ) specify the gamma and normal hyperdistributions for the standard deviation and mean respectively.
movie-bayes(u, t) = E( t,u|Ru,t).
 





         = =     p  ,  |Ru,t, N ( ,  ),  ( k,  )     p(Ru,t| ,  )   p  ,  |N ( ,  ),  ( k,  ) .
    .
      p        | ( k,  )  |N ( ,  )     p(Ru,t| ,  )   p .
    = (1) In equation 1 the second term, p(Ru,t| ,  ), is the probability of the user s ratings for movies with a tag based on a user-tag-speci c ratings distribution.
To calculate this probability, we treat the ratings as independent events.
As described earlier, each rating may be the result of the user s background ratings distribution, or a rating may be the result of the user s tag speci c distribution.
The user s background distribution, N ( u,  u), is  t to all of the user s ratings.
The user-tag-speci c distribution is chosen with probability equal to the relevance weighting w(m, t): p(r| ,  ) = p(Ru,t,| ,  ) =
 r Ru,t p(r|N ( ,  ))w(m, t) + p(r|N ( u, u))(1   w(m, t)) r Ru,t The third term in Equation 1, p( |N ( ,  )), is the prior probability of a mean for a particular user-tag-speci c normal distribution.
h i towards the tag.
We also evaluated linear combinations of tag preference algorithms.
We combined algorithms using a least square regression between tag preference responses and algorithm outputs.10 All-implicit is the best combination of all tag preference inference algorithms that do not use movie ratings.
All is the best combination of all tag preference algorithms.
 All-implicit  outperformed each individual implicit feature.
 All  outperformed all other algorithms.
In the following section we use the tag preference algorithms in tag-based recommendation algorithms.
We either use  all , or  all-implicit  depending on whether the tag-based algorithm is designed for systems with or without ratings.
In the previous sections we evaluated methods for inferring users  preferences for tags based on signals of interest in tags and items (Figure 1, upper left).
We now shift our focus to using those inferred tag preferences to predict ratings for movies (Figure 1, upper right).
We present  ve tag-based recommendation algorithms   two based on implicit data, and three based on explicit data.
We then describe our methodology including our evaluation metrics and baseline recommender algorithms.
Finally, we present results for all algorithms as they relate to our research questions.
We  rst consider two tag-based recommendation algorithms that use implicit data in order to support sites without item ratings.
As input, these algorithms use tag preferences inferred by all-implicit, the top performing implicit tag preference algorithm.
As output, these algorithms produce a score suitable for ranking items in a recommendation list.
Recommender systems that do not collect ratings generally do not predict ratings; therefore, the values output by the implicit tag recommendation algorithms are suitable for the recommend task but not the predict task.
In the previous section we saw that the average tag preference differed by tag.
For example, users generally preferred a high-quality tag to a low-quality one.
During our analyses of tag-based recommenders, we found that these per-tag differences skewed results.
We accounted for these per-tag differences by normalizing each tag s inferred preference to have mean 0 and standard deviation 1.
In addition, we found that more active users generally had higher tag preferences than less active ones.
To neutralize this effect, we subtracted the average tag preference for each user.
We use these normalized tag preference values throughout this section.
We now describe the two implicit tag-based recommender algorithms: Implicit-tag: The implicit-tag algorithm is inspired by algorithms from information retrieval that calculate the similarity between a user s pro le vector and a document s term vector [23].
In information retrieval, the columns in each vector correspond to words.
In the implicit-tag algorithm, the columns correspond to tags.
To generate a prediction for a movie m, implicit-tag calculates the dot product between users  preferences for movie m s tags and the weighting w(t, m) between tag t and movie m. We use prob-informed as a weighting based on its strong performance in section

 signi cant improvement.
Fig. 7: Pearson correlation between inferred tag preference, and actual tag preference.
All pairwise differences are signi cant at the 0.05 level according to a two-tailed t-test.
Algorithms based on explicit item signals performed best, followed by those based on implicit item signals, followed by those based on tag signals.
All, a linear combination of all algorithms, performed best.
We assume the user-tag-speci c mean is drawn from a normal distribution with hyper-parameters chosen using the empirical bayes methodology [9]: p( |N ( ,  )) = p( |N (2.885, 1.0)).
     | ( k,  ) The fourth term in equation 1, p , is the prior probability a of a standard deviation for the user-tag-speci c normal distribution.
We assume the deviation is drawn from a gamma distribution with hyper-parameters chosen using the empirical bayes methodology: p( | ( k,  )) = p( | (2.0, 1.0)).
The choice of a gamma and normal distributions as hyper-distributions for a normal distribution is common in the Bayesian literature [9].
We evaluate the complete integral using a Markov Chain Monte Carlo estimate.
As an evaluation metric, we calculate the Pearson correlation between each one to  ve star survey preference response and the inferred value from a particular algorithm (e.g.
log-odds-clicks).
Figure 7 shows pearson correlations for all tag preference inference algorithms.
95% con dence intervals are displayed on the graph.
All pairwise differences are signi cant.
The two algorithms based on explicit item rating signals (movie-ratings, movie-bayes) outperformed all implicit measures.
Both the click-based and ratings-based movie-log-odds-click algorithms performed poorly.
The two tag-based algorithms (tag-searched and tag-applied) did not perform as well as the movie-clicks algorithm.
We suspect that this is due to the relatively small amount of tagging activity on MovieLens.
For example, only 2.8% of the survey rating responses were associated with a tag the user had applied.
Similarly, only 0.6% of the survey responses were associated with a tag the user had searched for.
Tag quality performed best among the algorithms using tag signals.
Based on its relatively strong correlation (0.17), the movie.
If Tm is the collection of all tags applied to movie m: cosine-tag(u, m) = t Tm

 t Tm sim(m, t)   ntp(t, u) sim(m, t) +  ru.
One choice in this algorithms is the tags over which the average should be calculated.
We found that the algorithm performed best when averaging over the 10 most similar tags.
Linear-tag: Since cosine-tag predicts a weighted average of a user s inferred tag preferences, the variability of movie predictions it outputs depend on the variability of its inputs (inferred tag preferences).
Linear-tag models a more complex relationship between an inferred tag preference and a predicted movie rating.
For each tag t applied to movie m, linear tag estimates a least-squares  t yt,m(u) between users  inferred tag preferences for t and their ratings for m: yt,m(u) =  t,mntp(t, u) +  t,m + t,m.
In the linear equation above,  t,m is the coef cient between tag t and movie m,  t,m is the intercept, and t,m is the residual error term.
Linear-tag generates user u s prediction for movie m by averaging the values predicted by each of the linear  ts yt,m(u).
We found that weighting by inverse residual improved performance because it gave greater importance to more accurate  ts.
If Am is the set of all tag applications for movie m, then: linear-tag(u, m) = t tags(Am)  

   t tags(Am)     +  ru.
yt,m(u)/t,m 1.0/t,m As with cosine-tag, we experimented with averaging over different sets of tags.
Averaging over the 5 tags with smallest residual performed best.
Regress-tag: The linear-tag model treats each linear  t between a tag and a movie as independent.
This may not be optimal.
For example, both animated and animation have been applied to  Toy Story.  It seems plausible that users  inferred preferences for these two tags would correlate.
Algorithms aware of relationships between tags may perform better than those that do not.
Regress-tag constructs a linear equation for each movie m. The input variables are all users  inferred tag preferences for tags applied to m. The output is each user s rating for m. If m has tags t1, .
.
.
, tn then: regress-tag(u, m) = h0 + h1ntp(u, t1) + .
.
.
+ hnntp(u, tn).
We experimented with three methods for choosing the coef cients hi: simple least-squares multiple regression, regularized multiple regression, and regression support vector machines.
We found that the least-squares and regularized multiple regressions over t movies with few ratings.
For example, several movies had the same number of tags and ratings applied to them.
In this case, multiple regression can build an equation that perfectly  ts the input data.
These  ts often lead to large values hi that seemed intuitively incorrect and performed poorly.
SVMs performed best due to their robustness to over tting.
We used the libsvm library based on its java implementation and ef cient performance for linear kernels [6].
We found that libsvm performed best when c, the tradeoff between the margin and error penalty, was set to 0.005.
implicit-tag(u, m) = ntp(u, t)   w(m, t).
t Tm Implicit-tag-pop: Implicit-tag ignores the overall popularity of a particular movie, an important signal of a users s liking for a movie.
We next consider implicit-tag-pop, a version of the algorithm that adds pop(m), a term estimating a movie s popularity: implicit-tag-pop(u, m) = implicit-tag(u, m) + pop(m).11 We experimented with a variety of signals of popularity for a movie based on the number of clicks, tags, clickers, and taggers for a movie.
For each possible signal, we  t a function between the signal value for each movie (e.g.
num clicks) and the average rating for the movie.
We evaluated each signal of popularity based on how well implicit-tag-pop performed using the signal.
Although we omit the detailed results due to space, we found that tags outperformed clicks, counting users (clickers) outperformed counting events (clicks), and log transforming signals improved results.
The best overall estimate was a linear estimate based on the log of the number of users who tagged a movie.
If users(Am) is the set of users who applied a tag to movie m, then: pop(m) = 0.31   log(|users(Am)|) + 3.16.12
 The  nal three tag-based algorithms are intended for sites with item ratings; as a result, they rely on both implicit and explicit data.
As input these algorithms use tag preferences inferred by all, the top performing tag preference algorithm using both implicit and explicit signals.
Since these algorithms output a value between 1.0 and 5.0 corresponding to a star rating for a movie, they support both the predict and recommend tasks.
We choose three algorithms that model increasingly complex relationships between tag preferences and movie ratings.
Cosine-tag: The success of the traditional item-based rating models that use cosine similarities inspired us to create a similar model based on tags.
Cosine-tag predicts that a user s rating for a movie is a weighted average of the user s preferences for the movie s tags.
Cosine-tag weights a particular tag according to the adjusted cosine similarity between ratings for a movie and inferred preferences for a tag.
We refer to user u s mean movie rating as  ru, and Um is the collection of users who rated movie m. The adjusted cosine similarity between movie m and tag t is:
 s X u Um u Um (rm,u    ru)   ntp(t, u) (rm,u    ru)2 .
ntp(u)2 s X u Um sim(m, t) = Note that ntp(t, u) is already average adjusted, so there is no additional adjusting performed.
Given this de nition of similarity, cosine-tag constructs a prediction for a movie as the average of the user s preferences for its tags, weighted by the tags similarities to
 found 1.0 to perform optimally.
mendation but not for prediction.
Although we report the the intercept value (3.16), the choice of intercept does not affect the relative ordering.
Therefore, the intercept is unnecessary - we could simply use 0.0.
We compare the tag-based recommendation algorithms to three naive baselines: Overall-avg: Overall-avg generates a prediction equal to the overall average rating (3.55): overall-avg(m, u) = 3.55.
In the recommend task, movies are ordered by predicted rating.
Since overall-avg returns the same value for every movie, we randomly order recommendation lists.
User-avg: User-avg predicts a user s average for all of his or her movies.
If Ru is the set of all movie ratings by user u:
 user-avg(m, u) = rm,u .
r Ru |Ru| As with overall-avg, given a particular user, user-avg returns the same value for every movie.
Thus, we randomly order recommendation lists.
User-movie-avg: User-movie-avg begins by average adjusting all of a user s ratings.
The prediction for a movie is the average of all users  adjusted ratings for the movie.
If Um is the collection of users who rated movie m, then:
     ) rm,u0   user-avg(u
 user-movie-avg(m, u) = u0 Um |Um| +user-avg(u).
While these three naive baselines provide insight into algorithm performance, we ultimately compare our tag-based algorithms to top-performing traditional CF algorithms.
We consider three traditional algorithms: Explicit-item: We include the item-based algorithm introduced by Sarwar et al. based on its accuracy and popularity in real-world systems such as Amazon [17].
The explicit item-based model calculates similarities between the ratings for each pair of movies.
In order to predict for a particular movie m, the item model constructs a weighted average of the user s ratings for the movies most similar to m. The rating weights used for the weighted average are based on the similarities to m.
Implicit-item: We compare our implicit tag-based algorithms to Karypis et al. s item-based algorithm for unary data (such as click and transaction data) [14].
We selected this algorithm based on its accuracy and popularity.
The item-based model calculates similarities between each pair of movies based on the number of times movies co-occur in user baskets.
In order to predict for a particular movie m, the movie model sums the similarities between m and the movies in the user s basket that are most similar to m.
Funk-svd: We include Simon Funk s Singular Value Decomposition algorithm due to its strong performance in the Net ix competition [8].
The Funk SVD approximates the full users   movies rating matrix using a matrix of lower dimension, and uses regularization to manage the sparsity of the ratings matrix.
We used  ve-fold cross validation in our analyses.
For each each of the  ve test / train splits, we hide 30% of user ratings in the test set, and evaluate the performance of an algorithm by comparing the ordering of a recommendation list to the hidden ratings.
Herlocker et al.  nd two important classes of evaluation metrics: those that evaluate an algorithm s performance on the predict task, and those that focus on the recommend task [11].
We choose one evaluation metric from each class: Top-5: As an evaluation metric for the recommendation task, we use top-5, the fraction of the top  ve recommended movies Fig. 8: Top-5 precision for recommender algorithms.
95% con dence intervals are displayed for each algorithm.
Higher top-5 values correspond to better performance.
CF algorithms are displayed in solid bars and tag-based algorithms are displayed in striped bars.
The best of the tag-based algorithms perform better than the best CF algorithms.
for a user that are rated four stars or higher by the user.13 We only consider elements the user has rated when selecting the top
 (n = 28, 185).
MAE: In addition to top-5 we report mean absolute error (MAE), the average absolute difference between the value predicted by a recommender system and the user s actual rating value.
MAE re ects an algorithm s performance on the predict task.
We examined the distribution of MAE values produced in our analyses and found the 95% con dence intervals for MAE was  0.001 (  =
 the implicit algorithms support only the recommend task.
Therefore, we only report MAE for explicit algorithms.
Figure 8 shows the top-5 precision for the  ve tag-based algorithms, the three naive baselines, and the three collaborative  lter-ing (CF) baselines.
Higher top-5 values correspond to better performance.
The traditional CF algorithms are displayed in solid bars and the tag-based algorithms are displayed in striped bars.
We also include hybrid, a simple linear combination of the best performing tag-based algorithm (regress-tag) and traditional algorithm (funk-svd).14 95% con dence intervals are displayed for each algorithm.
Implicit-tag, user-tag, and overall-tag all achieve a top-5 of 53%, the same as randomly ordering a recommendation list.
Differences between other pairs are signi cant (p   0.05) except for those between user-movie-avg and implicit-tag-pop, and between regress-tag and hybrid.
The tag-based algorithms generally perform well.
Implicit-tag-pop, the best implicit algorithm, achieves a top-5 of
  rst star rating higher than the overall average rating of 3.55 stars.
We experimented with other values for n in top-n: 1, 3, 5, 10, 20.
We found the results to generally be consistent regardless of choice of n.
ments of ten, and found that a 50-50 average performed best a top-5 of 83%.
Figure 9 shows the MAE for all explicit algorithms.
The traditional CF algorithms are displayed in solid bars.
Lower MAE values correspond to better performance.
The tag-based algorithms are displayed in striped bars.
All pairwise differences are signi cant (p   0.05) except hybrid and funk-svd.
As discussed in Section 5, the implicit algorithms only support the recommend task.
Therefore, we only report MAE for explicit algorithms.
In general, the tag-based algorithms outperformed the naive baselines, and the traditional CF algorithms outperformed the tag-based algorithms.
Regress-tag performed best among the tag algorithms, achieving an mae of 0.584.
As with top-5, cosine-tag performed poorly, achieving an mae of 0.639.
Among the CF algorithms, funk-svd performs best, achieving an mae of 0.555.
Given these results, we return to our second research question: RQ2: How well do tagommenders perform in systems without ratings?
As shown in Figure 8, implicit-tag-pop (77%) performs significantly better than the popular implicit-item algorithm (69%) according to top-5.
We wondered if the strong performance of implicit-tag-pop compared to implicit-item was due to its inclusion of popularity.
To test this possibility, we experimented with different methods for building popularity into the implicit-item algorithm.
None of them signi cantly improved its performance.
We conclude that the tagommender algorithm performs better than traditional CF algorithms in our evaluation without ratings.
Finally, we address our last research question: RQ3: How well do tagommenders perform in systems with ratings?
Among the explicit tag algorithms, regress-tag performs best in both top-5 (83%) and MAE (0.584).
Among the traditional CF algorithms, funk-svd performs best in both top-5 (80%) and MAE (0.555).
Both these differences are signi cant (p < 0.05).
We conclude that tagommenders appear to perform better than traditional CF algorithms for the recommend task, but worse for the predict task.
However, the recommend task seems to be more prevalent in real world systems.
Among all popular recommender systems we investigated, only three (Net ix, MovieLens, Rate Your Music) offer predicted ratings.
Most recommender systems now follow Amazon s model.
Amazon does support the recommend task.
However, instead of supporting the predict task through CF, they offer users rich product data, user reviews, and average user ratings.
Thus, tagommenders perform better than traditional CF algorithms in the task most important to real world recommender systems.
We conclude by noting that hybrid, the linear combination of funk-svd and regress-tag, offers the best of both tag-based and CF algorithms.
Hybrid achieves a top-5 of 83%, equal to that of the top performing tag-based algorithm, and signi cantly better than traditional CF algorithms.
In addition, hybrid s MAE equals that of funk-svd, the best performing traditional CF algorithm.
Thus, a simple hybrid algorithm performs better than any CF algorithm on the recommend task and it matches the best CF algorithm on the predict task.
In this paper we introduced and evaluated tagommenders, rec-ommender algorithms that make use of tags.
We evaluated a wide variety of tag preference inference algorithms and found that algorithms combining a variety of signals performed best.
We constructed implicit and explicit tag-based recommendation algorithms Fig. 9: MAE for explicit algorithms.
We do not include implicit algorithms since they do not support the predict task.
95% con dence intervals are displayed for each algorithm.
Lower MAE values correspond to better performance.
CF algorithms are displayed in solid bars and tag-based algorithms are displayed in striped bars.
In general, tag-based algorithms perform better than naive baselines but worse than their CF counterparts.
based on users  inferred tag preferences.
These tagommenders outperformed existing CF algorithms in the recommend task most critical to real world recommender systems.
Finally, we showed that a hybrid tag and CF algorithm combines the strong predict performance of CF algorithms with the strong recommend performance of tag based algorithms.
We believe that tagommenders may lead to novel interfaces for recommender systems.
Since tagommenders use tags as an intermediary entity, their recommendations can be explained based on users  preferences for tags.
MovieLens users often ask for the opportunity to rate movies on a more diverse set of dimensions.
Tag-ommenders might prove a way to meet that desire.
Relationships between ratings and tags may also be used to infer the tags that should be applied to a movie.
Although previous researchers have investigated the tag inference problem [13], they have not made use of patterns in users  ratings of items.
For each movie, the cosine tag recommender calculates the similarity between between users inferred preferences for each tag and their ratings for the movie.
For example the most similar tags for the movie  Last of the Mohicans  starring Daniel Day-Lewis are the tags tribal, sword  ght, cavalry charge, historical, and stirring.
The only one of these tags that users actually applied to the movie is tribal.
Figure 6 lists 10 <tag, movie> pairs with highest similarity scores.
One important question related to our  ndings is how tagom-menders will perform in domains other than MovieLens.
While we cannot be certain, the high tag density of a system such as Delicious might lead to more accurate recommendations.
Finally, we believe there are a number of fundamental issues surrounding the relationship between preference and quality.
It may be challenging to design interfaces that collect ratings along both the quality and preference dimension without confusing users.
Perhaps because of this, most systems such as YouTube and Net ix only collect ratings along the preference dimension.
Future research might explore interfaces for differentiating between quality and preference and examine the role quality and preference play in different domains.
similarity between tag preferences and movie ratings.
We do not include movies in a series (e.g.
trilogies), and only include the top entry for tags or movies that appear more than once.
movie Pearl Harbor (2001) Runaway Bride (1999) Beauty and the Beast (1991) Armageddon (1998) Cinderella (1950) Inconvenient Truth (2006) The Little Mermaid (1989) Gone in 60 Seconds (2000) My Best Friend s Wedding (1997) Billy Madison (1995) tag disaster girlie movie talking animals will smith cartoon documentary musical exciting chick  ick very funny cosine sim











 The authors thank Arindam Banerjee, F. Maxwell Harper, Andrew Sheppard, Melissa Skeans, Katy Sen, and the entirety of Grou-pLens for their feedback on the ideas presented in this paper.
We also thank the MovieLens community for their ratings, feedback and suggestions.
This paper is funded in part by National Science Foundation grants IS 03-24851 and IIS 05-34420.
