The Internet provides an unparalleled opportunity for organizations to deliver digital content to their visitors instantaneously.
Content consumers usually have short attention span, while possibly a large number of content venders.
The Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
biggest challenge most organizations face is not lack of content, but how to optimize the content they already own by identifying the most appropriate customers at the right time.
Personalized recommendation has become a desirable feature of e-business Web sites to improve customer satisfaction and customer retention [8], by tailoring content presentation to suit an individual s needs rather than take the traditional  one-size- ts-all  approach.
Personalized recommendation involves a process of gathering and storing information about site visitors, managing the content assets, analyzing current and past user interactive behavior, and, based on the analysis, delivering the right content to each visitor [31].
Search engines help index available content assets and return relevant information to users, if the users are looking for something speci c that can be summarized as a keyword query.
However, in many cases, users are looking for things might interest them, but do not have concrete desideration in mind when browsing a Web site.
In such cases, it is a recommendation engine that presents the most plausible content that the user may want, based on her interests as demonstrated by her past activities.
Traditional recommendation engines could be distinguished into three di erent approaches: rule-based  ltering, content-based  ltering, and collaborative  ltering [32].
Rule-based  ltering creates a user-speci c utility function and then applies it to the items under consideration.
This approach is closely related to customization, which requires users to identify themselves, con gure their individual settings, and maintain their personalized environment over time [21].
It is easy to fail since the burden of responsibility falls on the users.
Content-based  ltering generates a pro le for a user based on the content descriptions of the items previously rated by the user.
The main drawback of this approach is the recommended items are similar to the items previously seen by the user.
Mladenic [30] provided a survey of the commonly used text-learning techniques in the context of content  ltering.
Collaborative  ltering (CF) is one of the most successful and widely used recommender system technology [37].
CF analyzes users  ratings to recognize commonalities between users on the basis of their historical ratings, and then generates new recommendations based on like-minded users  preferences.
CF provides a good solution to  a closed world , where overlaps in ratings across users are relatively high and the universe of content items is almost static.
In many scenarios, such as news  ltering [15], where the content universe changes rapidly and signi cant portion of users are new users, CF will su er from the cold-start problem.
Several hybrid recommender systems have been devel-more recommendation techniques.
The inability of CF to recommend new items is commonly leveraged by coupling with a content-based  ltering, such as in Fab [3], a recom-mender system for the Web content.
Burke [10] provided a comprehensive analysis of approaches to generating hybrid recommendation engines.
Although hybridization can alleviate some of the weaknesses associated with CF and other recommendation techniques, there are still a few important issues that haven t been well studied in literature:   Dynamic Content: We consider not only the item set undergoes insertions and deletions frequently, but also the content value and then the appraisement from users are changing rapidly as well.
For example, the lifetime of breaking news on the Internet is usually a couple of hours, and the value of the news (such as click through rate) is decaying temporally as people get to know it, see Figure 3(a) for an example.
Traditional recom-mender systems usually treat users  feedback static, so that feedback on the same items given at di erent time stamps is still comparable.
This assumption doesn t hold on dynamic content.
Rebuilding the model on very recent data is typically an expensive task, and tends to lose long-term interests of users.
On dynamic content, recommender systems always face the cold-start problem for new items.
  Users with Open Pro les: A typical user pro le in a CF system is a list of ratings on items of interest.
In practice, we can legally collect user information to develop a general pro le for a site visitor [19], which is not limited to the content universe only.
The general pro le may include declared demographic information, activities on relevant sites, consumption history, etc.
The objective is to provide valuable insight into users  preferences, interests and wants.
Clearly, the general pro le can help tackle the cold-start problem on new users.
Demographic recommender systems, e.g.
[34], aim to segment users based on personal attributes and make recommendations according to demographic classes.
However, the history of user ratings and content features haven t been jointly exploited to form  people-to-people  correlation.
In this paper, we propose a machine learning approach to handling both issues in personalized recommendation.
The key idea is to maintain pro les for both content and users, and build a feature-based bilinear regression model to quantify the associations between heterogeneous features by  tting the historical interactive data.
The feature-based predictive model can then be applied to recommending new and existing items for both new and existing users.
The goodness of dynamic content over time is a crucial ingredient in content management.
We insert dynamic features, such as instantaneous click-through rate (CTR) to indicate temporal popularity, into the content feature set.
We continuously update these dynamic features in the delivery phase by aggregating users  interactions over content items in a real-time manner.
We demonstrate that maintaining content pro les with dynamic features is an e ective strategy to overcome the cold-start problem on dynamic content.
Figure 1: An illustration of unfolding a multidimensional event.
The open pro les of users provide valuable information about user preferences and interests that helps in recommending content for new users.
Historical feedback given by users on content of interest, such as ratings or click stream, directly reveals users  opinion on the content universe.
The bilinear regression models we proposed can discover association patterns between the general user pro les and the content features by exploiting the interactive data (the typical user pro le in traditional CF).
The established associations are then applied to evaluating individualized appraisement over currently available items for accurate and prompt personalized recommendations in real time.
This work is motivated by a personalized content optimization task for the Today-Module on Yahoo!
Front Page.
The e ectiveness of the bilinear models is veri ed on a large-scale real-world data set collected in the application.
This approach results in an o ine model except online tracked dynamic features in content pro les.
The computational overhead in online recommendation is minor compared with recommender systems that require online retraining.
The framework is general and  exible, which can be adapted to other personalized tasks.
The paper is organized as follows: We introduce data representation in Section 2, which includes content pro ling, user pro ling and interactive feedback; In Section 3 we describe a family of probabilistic bilinear models in detail that covers training algorithms and further discussions on potential capabilities; We review related work in Section 4; We report the experimental results on the data set collected from the Today-Module with comparison against six competitive alternatives in Section 5 and conclude in Section 6.
The observational data is naturally recorded in multidimensional format.
A logistic event is associated with at least three types of objects, user   content   timestamp.
These multidimensional events can always be  attened into two-way form without loss of generality, see Figure 1 for an illustration.
In personalization on dynamic content, we can treat content timestamp as items of interest.
Note that the dimension of timestamp is usually not considered in traditional recommender systems.
The  attened dimensions form a new content item space, in which features are extracted for pro ling.
We generate and maintain three sets of data: content pro les, user pro les, and interactive feedback on content items of interest.
When a content is either created or acquired, the informa-User(cid:13)Item(cid:13)Timestamp(cid:13)User(cid:13)Item at(cid:13)timestamp(cid:13)WWW 2009 MADRID!Track: Social Networks and Web 2.0 / Session: Recommender Systems692tion related to the content, such as manufacturer, product name and categories etc., constitutes an initial part of the pro le.
Continuous re nement of the content pro le helps to optimize the use of the content assets.
In the delivery phase, the content is delivered to users and interactions on the content are logged and analyzed, providing the ability to assess the content popularity in a real-time manner.
The content popularity over time is a crucial ingredient in content management, since the commercial value of most content is varying or decaying temporally, especially for breaking news.
We consider generalized content items here, which are re-de ned with both temporal characteristics and other conditions.
In a content pro le, there are at least two groups of features:   Static descriptors: Such as categories, manufacturer name, title, bag of words of textual content etc.
  Temporal characteristics: Such as popularity, click-through rate (CTR) and price at current time stamp or the hours elapsed after content acquisition.
We can collect any features related to the content items.
For example, in search the items become webpages fused with a query, and then joint features, such as contextual co-occurrences, can be constructed.
Each content is represented as a vertical vector, denoted by z, where z   4C and C is the number of content features.
The objective of collecting visitor information is to develop a user pro le that describes a site visitor s interests, consumption history, and other descriptors important to the site owner.
A review of various user pro ling techniques is provided in [19].
Explicit pro ling requests each visitor to declare personal information, such as age, gender and occupation, or to  ll out questionnaires that explicitly state their preferences.
Implicit pro ling tracks the visitors  behavior and it is generally transparent to the visitor.
Browsing and purchasing patterns are the behaviors most often assessed.
The pro le combined with demographic, transaction, and navigation data implicitly represents a user s preferences and recent interests.
The user feature space is spanned by legally usable features.
Each user is represented as a vertical vector, denoted by x, where x   4D and D is the dimensionality of the user feature space.
Interactive Feedback In traditional collaborative  ltering (CF), the feedback given by users on content of interest are used as user pro les to evaluate commonalities between users.
In our regression approach, we separate the feedback from user pro les.
The feedback on content of interest is utilized as targets that relate patterns in user features to content features.
Although the interactions between the users and the available items vary depending on the types of items involved, we can always observe or measure some feedback from user side.
For example, a user may purchase a product or a service after review, and even rate it later.
For a content posted on a Web page, a user may click to see more details.
The ratings and actions (click or not, purchase or not) provide explicit feedback.1 There are a range of e orts attempted to measure various kinds of implicit feedback indicators from linger time [13] to eye movements [36].
We focus on two types of feedback in this paper:   Continuous scores: most implicit feedback and ratings can be converted as continuous scores.
  Binary actions: such as click or not, purchase or not after reviewing an item.
We have collected three sets of data, including content features, user pro les and interactive data between users and items.
Let index the i-th user as xi and the j-th content item as zj, and denote by rij the interaction between the user xi and the item zj.
We only observe interactions on a small subset of all possible user/item pairs, and denote by  the set of observations {rij}.
The user and content pro les provide timely descriptions of users and items respectively.
As the two feature spaces are usually dichotomous, it is hard to apply the contextual data mining techniques [9] here.
However, the interactive feedback reveals the correlations between user patterns and content features.
In this section, we describe a family of predictive bilinear models to discover pattern a nities between heterogeneous features.
A set of weight coe cients is introduced to capture the pairwise associations between user and content features.
The parametric model is optimized by  tting the observed interactive feedback.
The bilinear models can be regarded as a special case in the Tucker family [14], which have been widely applied in machine learning applications.
For example, Tenenbaum and Freeman [39] developed a bilinear model for separating  style  and  content  in images, and recently Chu and Ghahramani [11] derived a probabilistic framework of the Tucker family for modeling structural dependency from partially observed high-dimensional array data.
We de ne an indicator as a bilinear function of xi and zj in the following: sij = xi,bzj,awab, (1) C:a=1 D:b=1 where D and C are the dimensionality of user and content features respectively, zj,a denotes the a-th feature of zj and xi,b denotes the b-th feature of xi.
The weight variable wab is independent of user and content features and quanti es the a nity of these two factors xi,b and zj,a in interactions.2 The scalar sij is generated by mixing these basis vectors with coe cients given by the Kronecker product of xi and zj.
The indicator can be equivalently rewritten as sij = w (cid:62) (zj   xi),
 plicit feedback in other collaborative  ltering literature since these may not re ect real user preferences.
For example, a user may  nd that an article is uninteresting after clicking and reading it.
However, we refer these actions as explicit feedback since the user intentions of these actions are clearer than those of other implicit feedback such as linger time and eye movement.
denotes the Kronecker product of xi and zj, a column vector of entries {xi,bzj,a}.
In matrix form, eq(1) can be rewritten as sij = x (cid:62) i W zj, (cid:62) (2) (cid:62) i,dzj,d, (cid:62) i Ws, x a=1  xi,azj,a.
(cid:62) i,szj,s +  x zj,d  =  x where W denotes a D  C matrix with entries {wab}, which describes a linear projection from the user feature space onto the item feature space.
The projected user pro le W(cid:62)xi is aligned to the item features, denoted by  xi, which can be explained as users  preferences on item characteristics accordingly.
Then the indicator becomes a dot product, i.e.
sij =  x(cid:62) To further examine the feature functions, let us distinguish dynamic features in the item feature vector as zj = i zj =2C  zj,s zj,d , where zj,s denotes static features and zj,d denotes sij =Dx dynamic features that vary along time.
The indicator sij can then be rewritten as follows, i WdE zj,s where Ws and Wd denote the columns in W associated with the static and dynamic item features respectively, and  xi,s and  xi,d denote the i-th user s preferences on the static and dynamic item features respectively.
Note that a user s score sij on an item is composed of three parts:  x(cid:62) i,szj,s re ects long-term personal preferences on content features learnt from historical activities; zj,d is of dynamic characteristics, in our work which include temporal popularity over the whole user population, i.e. article quality; the tradeo  between static personal preferences and article quality is determined by  xi,d.
On cold-start with new items, the user s preferences on static item features  x(cid:62) i,szj,s play an important role, as the dynamic features couldn t be accurately estimated at the beginning stage.
Similarly, on cold-start with new users, recommendations are fully determined by the users  preferences on content features  xi, which are projected from the user pro le xi.3 As we will show in the following, the projection W can be learnt from the historical interactive feedback.
We employ appropriate likelihood functions to relate the indicator sij to di erent types of observed interactions.
  Continuous scores with Gaussian measurement noise: p(rij|sij) =
 2  exp  (rij   sij)2 2 2 , where   stands for the noise level.4 each user.
The  nal scalar is evaluated as sij = C:a=1 D:b=1 where  i   4 denotes a user-speci c o set.
Here  i is used to tradeo  the user s activity level, since some users are active clickers while some are casual users.
enough to be transformed into preferences on item characteristics.
This condition can be easily satis ed in practice.
priate value based on the signal/noise ratio.
xi,bzj,awab +  i,  L(w)  wab =   log p(rij|sij)  sij xi,bzj,a, (7)   Binary actions with rij   { 1, 1}.
The logistic function is widely used as the likelihood function, which is de ned as p(rij|sij) =
 1 + exp( rijsij +  ) , where   denotes a bias term, usually set at 1.
Given a set of w, the likelihood of observing the interactive data can be evaluated by p(rij|sij), (3) p(|w) =;ij (4) (5) where the index ij runs over the observational set .
weight variables as a priori, We also specify a standard Gaussian distribution over the p(w) =
 2  where   2 is the variance.
exp 2ab w2 2  2 , ab Based on the Bayes  theorem, the posterior distribution of w is proportional to the product of the likelihood and the prior, where p(w) is the prior distribution de ned as in eq(4) and p(w|)   p(|w) p(w).
p(|w) is the likelihood de ned as in eq(3).
In this section, we describe a training algorithm in batch mode to estimate the posterior distribution of the weight coe cients p(w|) as in eq(5).
For continuous scores with Gaussian noise, the posterior distribution is still a Gaussian due to the conjugate property.
With non-Gaussian likelihood functions, the posterior distribution becomes non-Gaussian.
However we can always approximate the true distribution by a Gaussian distribution.
One of the most popular techniques is the Laplace approximation [26], which  nds the mode of the true posterior as the approximate mean and approximates the inverse covariance matrix by the Hessian matrix, the second order derivatives with respect to the weights at the mode point.
The mode, also known as the maximum-a-posteriori (MAP) estimate, can be found by maximizing the joint probability p(|w)p(w).
The optimization problem is equivalent to minimizing the negative logarithm of the joint probability, i.e.
min w L(w) = log p(rij|sij), (6) where   2 plays a role of tradeo .
The gradient with respect to wab can be computed as follows, w2 ab  :ij
 2  2:ab   2  :ij wab and gradient-decent packages can then be employed to  nd the minimum.
Note that the objective functional is convex and the minimum is unique.
The detailed formulations are given in Table 1 and the gradient-descent algorithm is summarized as in Table 2.
Each objective/gradient evaluation costs O(N CD), where CD is the size of w and N is the size of the observed set .
Note that matrix inverse can  rst-order derivatives.
  log p(rij|sij ) log p(rij|sij) Target Continuous Binary   (rij sij )2 2 log(2 )   log(1 + exp( rijsij +  ))   1 2 2  sij sij rij  2 rijp( rij|sij) Table 2: The gradient-descent algorithm for MAP.
Initialize w = 0, given  2 and   2 Compute the objective as in eq(6); Compute the gradients for w as in eq(7); Return the objective/gradients to the package.
be applied directly to the case of continuous targets for an solution, but the computational cost is O(N C 2D2 + C 3D3).
It is very expensive for the cases having a large number of features.
The MAP estimate, denoted as wMAP, is then applied to new user/item pairs for prediction.
For any pair of xi and zj in test, the best guess of the indicator sij is determined as follow,  sij = xi,bzj,awMAP ab , (8) C:a=1 D:b=1 is an entry of the MAP estimate wMAP.
ab where wMAP
 In this section, we discuss model selection and some potentials of the framework we proposed, such as online learning and active learning.
The prior variance   2 is an important model parameter in the regression framework.
The most common approach in practice to determine the best model setting is cross validation.
In k-fold cross validation, the original training data is randomly partitioned into several folds, whereas in our application having time series of dynamical features we have to split the training data by a temporal point into two folds, usually with size ratio 2 : 1.
Given a particular set of model parameters, we run the training algorithm on the fold of earlier data to estimate the weight coe cients, and test the resulting model on the left-out fold to obtain the validation error.
The predictive performance indicates the goodness of the model parameter setting.
We try grid search over a set of parameter values to  nd the optimal one on which we observe the best performance on the validation data.
The optimal weight coe cients in the regression model are  -nally obtained by training on the whole training data set using the best set of model parameters.
In this work we only focus on training an o ine model coupled with dynamic features, whereas the probabilistic framework we employed provides the capacity of online learning as well.
Assumed-density  ltering (ADF) is a one-pass, sequential method for computing an approximate posterior distribution [17].
In ADF, observations are processed one by one, updating the posterior distribution which is usually approximated as a Gaussian before processing the next observation.
The approximate posterior is found by minimizing KL-divergence to preserve a speci c set of posterior expectations.
Recently, Expectation Propagation [29] extends ADF to incorporate iterative re nement of the approximations, which iterates additional passes over the observations and does not require corresponding with time of arrival as in time series.
Learning could be made more e cient if we can actively select salient data points.
Within the probabilistic regression framework, the expected informativeness of a new observation can be measured by the change in entropy of the posterior distribution of the weight coe cients after inclusion of the candidate [24].
The new posterior distribution with the inclusion of the unused sample can be approximated as a Gaussian by ADF-like online learning algorithms.
Based on information-theoretical principles, the entropy gain on the posterior distribution of weight variables can then be applied as the criterion for candidate election.
Our work is closely related to adaptive news systems, one of the most popular types of personalized Web-based service [6].
The most relevant previous work to our study would be the Google News recommender system [15], a content-agnostic system which combines three di erent algorithms using a linear model to generate recommendations in News domain.
However, since the proposed approach is a pure collaborative  ltering, it does not solve the cold-start problem for new users.
Even though ratings from new users can be updated in near real-time by gridifying their algorithm, it still needs to wait until new users provide ratings or clicks before making recommendations.
Also, the reported results are based on two heavy user data sets (top 5K heavy users with 370K clicks and 500K users with 10M clicks), where e ects of new and casual users haven t been considered.
In our application of the Today-Module on Yahoo!
Front Page, 40% of clickers are new clickers with no historical clicks, 82% of clickers have less or equal to 5 historical clicks, 92% of clickers have no more than 10 historical clicks as shown in the Figure 3(b).
Another key di erence lies in that Google News [15] is a content-agnostic system which doesn t resort to either content features or user information.
YourNews [2] allows users to customize their interest pro les through a user model interface.
The study on user behavior shows the bene t from customization but also cautions the downside on system performance.
In our application, we build up user and content pro les without any solicitation on users.
Newsjunkie [18] provided personalized news feeds for users by measuring news novelty in the context of stories the users have already read.
Our content pro les can also maintain dynamic features in addition to context novelty, such as popularity and freshness.
Our model also leverages user pro les to facilitate cold-start on new users.
Our work is also related to personalized search, though the tasks are quite di erent.
Micarelli et al. [28] gave a nice review on this direction.
Personalized search builds models of short-term and long-term user needs based on observed user actions, which is able to satisfy the users better than standard search engines based on traditional Information Retrieval (IR) techniques.
Speretta and Gauch [38] devel-pro les to re-rank the results returned by an independent search engine by giving more importance to the documents related to topics contained in the user pro le.
Ahn et al.
[2] designed the TaskSieve system that utilizes a relevance feedback-based pro le for personalized search.
Both systems employ the traditional linear approach to combine personal preferences and query relevance.
The combined score is calculated as  f (xi, zj)+(1 ) r(zj), where xi is a user pro le, zj is a content item fused with the query and   is the trade-o .
There is a strong correspondence to the terms in eq(2).
By replacing the dynamic features of zj by the query relevance r(zj) and implement f (xi, zj) in the parametric form of the long-term preferences as in eq(2), our bilinear model provides a  exible framework to learn the personal preference function and the tradeo  term from the click stream in a principled manner.
A personalized service may not be exactly based on individual user behaviors.
The content of a website can be tailored for a prede ned audience, based on o ine research of conjoint analysis, without online gathering knowledge on individuals for service.
Conjoint analysis is one of the most popular market research methodologies for assessing how customers with heterogeneous preferences appraise various objective characteristics in products or services.
Analysis of tradeo s driven by heterogeneous preferences on bene- ts derived from product attributes provides critical inputs for many marketing decisions, e.g.
optimal design of new products, target market selection, and pricing a product.
In very early studies [40], homogeneous groups of consumers are entailed by the use of a priori segmentation.
For example, consumers are assigned to groups on the basis of demographic and socioeconomic variables, and the conjoint models are estimated within each of those groups.
This is closely related to demographic recommender systems [23, 34], in which recommendations are based on demographic classes categorized by users  personal attributes.
However, the criteria in the two steps are not necessarily related: one is the homogeneity of customers in terms of their descriptor variables and another is the conjoint preferences within segments.
Traditionally, conjoint analysis procedures are of two-stage: 1) estimating a parametric function which represents customers  preference at individual-level in terms of user pro les, e.g.
hierarchical Bayesian methods [25]; 2) through clustering algorithms, grouping users into segments where users share similar individual-level preferences.
Jiang and Tuzhilin [22] experimentally demonstrated both 1-to-1 personalization and segmentation approaches signi cantly outperform aggregate modeling.
In the extreme cold-start setting with dynamic content and a large amount of new users, traditional collaborative  l-tering methods cannot provide recommendation e ectively.
A number of hybrid methods, which combine information  ltering and other collaborative  ltering techniques, have been proposed, such as [12] of an online newspaper and the Fab system [3].
Good et al.
[20] improved accuracy by introducing personal agents, and Park et al.
[33] further improved its performance in cold-start situations by adding small number of arti cial users who have rated all items.
However, this approach performs better only if a user has rated a few items but does not solve the cold-start problem [5] utilized social infor-directly for new users.
Basu et al.
mation in content-based  ltering.
Melville et al.
[27] em-Figure 2: A snapshot of the default  Featured  tab in the Today Module on Yahoo!
Front Page.
There are four articles displayed at footer positions.
One of the four articles is highlighted at the story position.
ployed a content-based predictor to enhance existing user data, and then provided personalized suggestions through collaborative  ltering.
Basilico and Hofmann [4] developed a framework that incorporates all available information by using a suitable kernel or similarity function between user-item pairs.
Hybrid methods are especially useful when data is sparse, for example in cold-start situations [35], but to our best knowledge none of previous work has been integrated with continuous online attributes, such as popularity or freshness.
In this section, we verify the capacity of the proposed bi-linear models on a real-world application.
We start with an introduction of the problem settings in Yahoo!
Today-Module and describe the attributes we collected in user/content pro ling.
We also de ne performance metrics to evaluate predictive results and report experimental results with comparison to competitive approaches.
Today-Module is the most prominent panel on Yahoo!
Front Page, which is also one of the most visited pages on the Internet, see a snapshot in Figure 2.
The default  Featured  tab in Today Module highlights one of four high-quality articles, mainly news, while the four articles are selected from a daily-refreshed article pool curated by human editors.
As illustrated in Figure 2, there are four articles at footer positions, indexed by F1, F2, F3 and F4 respectively.
Each article is represented by a small picture and a title.
One of the four articles is highlighted at the story position, which is featured by a large picture, a title and a short summary along with related links.
At default, the article at F1 is highlighted at the story position.
A user can click on the highlighted article at the story position to read more details if she is interested in the article.
The event is recorded as a story click.
If a user is interested in an article at F2 F4 positions, she can highlight the article at the story position by clicking on the footer position.
To draw visitors  attention, we would like to rank available articles according to visitors  interests, and highlight the most attractive article at F1 position.
It is di cult to adopt a traditional collaborative  lter-ing algorithm such as user-user [7] or item-based [16] in the ilarity evaluation in the online service is hard.
Lifetime of an article is very short (only a few hours) and old articles will be pulled out of content pool regularly.
Another di culty is that we always need to recommend new items and signi cant portion of users is taken by new users.
As shown in Figure 3(b) and (c), 40% clickers in the test data are the  rst time clickers without any historical clicks, and on average 60% articles are new everyday.
Thus, traditional collaborative  ltering methods su er from the cold-start problem.
Furthermore, the article popularity is temporally decaying, see Figure 3(a) for an example where CTR decreases to 1/6 of its peak value at the end of the article s lifetime.
It is di cult to compare users  feedback on the same article received at di erent time slots.
We collected events from a random bucket in July 2008.
In the random bucket, articles are randomly selected from the content pool to serve users.
An event records a user s action on the article at the story position, which is either  view  or  click  encoded as  1 and 1 respectively.
Note that a user may click on the same article multiple times but at di erent time slots.5 In our approach, these binary events are distinguishable, because a content item is de ned by both the article and the time slot of the event of interest, see Figure 1.
This is a conceptual di erence from traditional approaches.
We collected about 40 million click/view events by about 5 million users from the random bucket before a certain time stamp for training.
We also collected about 0.6 million click events after that time stamp for test.
The features of users and items were selected by  support .
The  support  of a feature means the number of users having the feature.
We only selected the features of high support above a pre xed threshold, e.g.
10% of the population.
Then each user is represented by a vector of more than one thousand categorical features, which include:   Demographic information: gender (2 classes) and age discretized into ten classes;   Geographic features: about two hundred locations of countries or U.S. States;   Behavioral categories: about one thousand binary categories that summarize the user s consumption behavior within Yahoo!
properties; Each article is pro led by a vector of about one hundred static features and a dynamic feature.
The static features include:   URL categories: tens of classes inferred from the URL of the article resource;   Editor categories: tens of topics tagged by human editors to summarize the article content; The dynamic feature is of estimated click-through rate (CTR) at events of interest, which di erentiates the same article at di erent time slots.
We adapted the Kalman  lter designed
 one user on the same article could be at most 1 within a single time slot (e.g.
5 minutes).
Figure 3: (a) A typical pattern of article CTR; (b) Historical click counts of clickers in test; (c) New article percentage per day in test.
and implemented by our team [1] for CTR tracking, which yields a good indicator of article quality and popularity temporally.
Note that other dynamic features, e.g.
freshness, can be added into the content pro les as well.
  Categorical features are encoded as binary vectors with nonzero indicators.
For example,  gender  of two classes is translated into two binary features, i.e.,  male  is encoded as [0, 1],  female  is encoded as [1, 0] and  unknown  is [0, 0].6 As the number of nonzero entries in these binary feature vectors varies, we further normalized each vector into unit length, i.e., nonzero entries in the normalized vector are replaced by 1/ k, where k is the number of nonzero entries.
For user features, we normalized behavioral categories and the remaining features (age, gender and location) separately, due to the variable length of behavioral categories per user.
For article features, we normalized URL and Editor categories together, and kept the CTR term (a real value) intact.
Following conventional treatment, we also augmented each feature vector by a constant term 1.
Each content item is represented by a feature vector of 83 entries, while each user is represented by a feature vector of 1193 entries.
For each user in test, we computed predictive scores as in eq(8) for all available articles at the time stamp of the event, and ranked these articles in descending order according to the scores.
On click events, we measured the rank position of the article being clicked by the user.
The  rst metric we use is the number of clicks in each rank position.
A good predictive model should have more clicks on the top-ranked positions and lesser clicks on the lower-ranked position.
In our application, we mainly concern the performance on the top 4 positions.
We also proposed a simple utility function to quantify the predictive performance, which is de ned as follows: 4:r=1
 contribution to our linear models.
Ur 2r 1 , (9)
 tion r in the whole test clicks.
We implemented three sets of competitive approaches for comparison purpose.
As a baseline, we aggregated clicks and views per article along time over the whole population, and ranked articles by the global CTR only.
The CTR online tracking was implemented with the Kalman  lter designed in [1], which has yielded very strong performance in the product.
In this approach, denoted by EMP, users are served with the same content (the estimated most popular article) at the same time stamp.
Segmentation Level (GM and SEG5) Presupposing the existence of heterogeneity in users  preferences on articles, we carried out two conjoint analysis methods on the Today-Module data: a) GM: We simply grouped users into 6 clusters based on rules of their demographic variables (age and gender); b) SEG5: We estimated users  preferences on article features  rst following the hierarchical Bayes approach discussed by [25] and then clustered homogeneous users with similar preferences by K-means.7 At segmentation level, we aggregated clicks and views per article within user segments, and estimated the article CTR within segments.
A user will be served with the most popular article in the segment she belongs to.
The CTR estimation within segments su ers from low-tra c issues when the number of segments is large.
On this application, we tried 2, 5, 10 and 20 segments and found the performance of 5 segments is the best out of the four settings.
At both aggregate and segmentation level, we applied the same online CTR tracking technique [1], which was also used for updating the dynamic feature of article CTR at aggregate level in content pro les.
We implemented three alternative individual level approaches to compare against our bilinear models.
Item-based collaborative  ltering (IBCF).
We implemented a standard item-based collaborative  l-tering algorithm as in [16].
We update the item-item similarity matrix in every hour by calculating cosine similarity of two articles in the user click behaviors.
When the algorithm cannot recommend anything due to lack of user information (i.e. new users), the algorithm rank candidate articles based on the aggregate CTR.
Content-based  ltering (CB).
As we discussed in Section 5.2, each user xi and item zj can be represented as a vector of categorical features (without dynamic features) as xi = {xi,1, .., xi,D} and zj = {zj,1, .., zj,C} respectively.
We normalized user and item vec-k=1 zj,k = 1.
tors into unit sum, i.e. 2D k=1 xi,k = 1 and2C
 be applied to our application.
We resorted to the MAP estimate via gradient descent methods.
More details of segmentation analysis will be reported in another paper.
For each vector, xi,k = 1|xi| if the user has the k-th user-feature and xi,k = 0 otherwise, where |xi| denotes the number of nonzero features in xi.
We maintained two a nity matrices between heterogeneous features for click and view events respectively.
For a click/view event of xi and zj, the click/view a nity between the b-th user-feature and the a-th item-feature is accumulated with xi,bzj,a.
Note that the total contribution from a single event is always one.
For each pair of heterogeneous features, we aggregated the contributions over all click/view events in the training samples, and then calculated the a n-ity ratio between click and view, denoted as  ab.
After we learnt the a nities for all feature pairs, the preference score given by a user xi on an item zj is calculated as b=1 xi,bzj,a ab which is analogous to our bilin-ear model in eq(1) but is of di erent feature normalization and a nity estimation.
cij =2C a=12D CB with online CTR (CB+EMP).
Since the estimated CTR of an item at time t, denoted by CT Rj,t, provides tremendous insight on the quality of the item at time t, we followed the hybrid approaches [10] to combine CTR with the score from the content-based approach.
Motivated by the combinations proposed in personalized search [38, 2], the  nal score given by a user xi on an item zj at time t was evaluated by (1    ) cij +   CT Rj,t where CT Rj,t denotes CTR of the article zj at time t, and 0       1 is a trade-o  parameter determined by cross vali- dation.
We found   = 0.8 yields the best validation results.
We implemented two versions of the probabilistic bilin-ear models.
One treated the feedback as continuous scores (RG), whereas another took the click-or-not events as binary targets (LRG).
We employed a gradient-descent package for the MAP estimate in the posterior distribution of the weights as described in Section 3.3.
The model parameter   2 was determined by cross validation on [0.01, 0.1, 1, 10], as discussed in Section 3.5.1.
For each user in test, we computed the expected score  sij as in eq(8) for all available articles at the event, and ranked these articles in descending order according to the scores.
In Table 3, we presented the portions of clicks at the top 16 rank positions of all the methods we have implemented and computed the utility function de ned as in eq(9).
We also carried out Wilcoxon rank sum tests on the predicted click ranks to evaluate the signi cance of the di erence between the LRG s ranks and other methods  predictions, and reported the p-values in Table 3.
A p-value close to zero means the two predictive results are signi cantly di erent, while near 1 means the di erence is not signi cant.
Usually we set the level of signi cance at 0.05.
LRG greatly outperforms other methods on both click portions on the top
 results also show the improvement is signi cant over all competitors.
GM, a rule-based segmentation, doesn t result in much improvement, compared with SEG5 that fuses pro les and feedback for user segmentation.
CB relying on user/item static features only recommends items similar to what a user has already rated, but it is hard to capture new features that the user might like and to provide serendipity  nding which collaborative  ltering can do.
CB+EMP, a combined approach, performs slightly better

















































































 2.41e-24


















 3-e201




















































 Table 3: Click portions on predictive rank positions, along with Ranksum Test and Utility results.
Rank Position

 Ranksum Test 7.109e-212 1.145e-182 Utility





 We proposed a feature-based bilinear regression framework for personalized recommendation on dynamic content.
We quanti ed associations between attributes in user pro les and content pro les through learning a parametric bi-linear regression function from interactive feedback.
This approach results in an o ine model but with the dynamic features in content pro les, which provides the capacity of recommending new high-quality content promptly and accurately.
In contrast to traditional recommender systems, our approach also greatly alleviates the cold-start issue of recommending for new users, by leveraging interest patterns in user pro les recognized from regression over historical interactive feedback.
We found the personalized predictive models signi cantly outperform six competitive approaches at aggregate, segmentation or individual levels on the application of Yahoo!
Front Page Today-Module.
The potentials of the probabilistic bilinear regression framework haven t been fully exploited.
It is straightforward to implement online learning algorithms within the proposed regression framework, which may be useful in tracing users  short-term interests.
Based on information-theoretical principles, e cient learning could be achieved by actively electing salient samples.
The techniques we proposed for dynamic content can be adapted for personalized search as well.
We plan to investigate these directions in future work.
We thank Raghu Ramakrishnan, Scott Roy, Deepak Agar-wal, Bee-Chung Chen, Pradheep Elango, and Ajoy Sojan for many discussions and helps on data collection.
