Document summarization is the process of generating a short version of a given document to indicate its main topics.
With the rapid growth of the Web, summarization has proved to be an essential task in many text processing areas.
For example, in response to a user query, search engines often provide short summaries in the form of snippets for each document in the result list.
In this way, users can save time by browsing the summaries before deciding whether or not to read the whole documents.
Document summarization can generally be categorized as abstract-based and extract-based [26].
Abstract-based sum-marization can be seen as a reproduction of the original document in a new way, while extract-based summarization focuses on extracting sentences from the original document [13, 16, 17, 26].
In this paper, we consider generic extract-based summarization of a single document.
Several learning-based methods have been proposed for extract-based summarization.
They usually utilize a set of features constructed from the document and extract a subset of sentences from the document using some machine learning methods.
For example, some approaches consider extract-based summarization as a binary classi cation problem, and use the information contained in each sentence and its popularity to decide whether it should be included in the summary.
Unfortunately, those approaches tend to give rise to serious problems such as redundancy, unbalance and low recall in the generated summaries.
According to our observations, we argue that an e ective summarization method should properly consider the following three key requirements:   Diversity: A good document summary should be concise and contain as few redundant sentences as possible, i.e., two sentences providing similar information should not be both present in the summary.
In practice, enforcing diversity in summarization can e ec-tively reduce redundancy among the sentences.
  Coverage: The summary should contain every important aspects of the document.
By taking coverage into consideration, the information loss in summarization can be minimized.
ous aspects of the document in a balanced way.
An unbalanced summary usually leads to serious misunderstanding of the general idea of the original document.
The goal of this paper is to systematically explore the above three aspects through a structure learning framework.
The main idea is to utilize structural SVM with three types of constraints to enforce diversity, coverage and balance in the generated summaries.
Speci cally, based on the relations between sentences, we propose a so-called independence graph to model the structure of a given document, aiming at representing the dissimilarity between pairs of sentences and reducing the search space in the structure learning process.
We also adapt the cutting plane algorithm to solve the resulting optimization problem and then use the trained model for summary generation.
A set of experiments is conducted on the DUC2001 data sets to evaluate our proposed method.
Firstly, we compare the performance of our method with several state-of-the-art supervised and unsupervised methodes for summarization.
Secondly, we evaluate the e ectiveness of our three proposed constraints on enforcing diversity, coverage and balance of summaries.
The experimental results show that explicit enforcement of diversity, coverage and balance results in sig-ni cant improvements over state-of-the-art summarization methods.
The two main contributions of our work are:
 into consideration in extract-based summarization and enforce them through the construction of three types of constraints.
to solve the summarization task, utilizing structural SVM with e ective modeling of sentence relationships using independence graphs.
The rest of the paper is organized as follows: Section 2 presents related works.
We cast extract-based summariza-tion as a structure learning problem in Section 3.
In Section 4, we discuss in detail the requirements of a high-quality summary, emphasizing diversity, coverage and balance issues, and how to employ constraints to handle them.
Section 5 presents the details of our structure learning method including the construction of the independence graphs.
A presentation of several useful features for the training process is also discussed.
In section 6, we focus on the experimental results together with some analysis.
We conclude the paper with some pointers to future research directions in Section 7.
Document summarization has been an active topic in many research areas, such as natural language processing, information retrieval and machine learning.
We discuss prior works and put our contributions in context.
Abstract-based summarization is an important problem in natural language processing.
Knight and Marcu proposed a framework to generate summaries by creating sentences from the document s original content [16].
In addition, Jing and McKeown developed a cut-and-paste-based text sum-marizer, which modi es extracted sentences by discarding unimportant words and produces new sentences by merging result phrases [13].
Most of those methods focus on producing sentences from extracted sentences.
Extract-based summarization is usually viewed as a machine learning problem: selecting a subset of sentences from a given document.
Several supervised learning methods have been developed for training accurate models for extract-based summarization.
For example, the SVM-based method focuses on constructing a decision boundary between summary sentences and non-summary sentences [24].
However, it relies on the assumption that sentences are independent from each other and ignores the correlation between sentences.
The HMM-based method proposed in [5] relaxes this assumption by modeling the relations between di erent sentences through hidden Markov models.
Unfortunately, the training process of the HMM-based method becomes intractable when the feature space becomes very large.
The CRF-based method, proposed by Shen et al.
[26], gives a better solution to the problem of sentence dependency, leading to a sound result compared with other approaches.
Many unsupervised methods also contribute greatly to document summarization.
Zha in [32] proposed a mutual reinforcement principle for sentence extraction using the idea of HITS [15].
Several related methods such as TextRank [21], LexPageRank [7] and CollabSum [30] gain remarkable performance also by exploring methodologies used in PageR-ank [2] and HITS [15].
These methods mostly focus on the dependence, for instance, similarity, between sentences of the same document or within multiple documents.
Several clustering methods have also been employed for preprocess-ing, which indeed improve performance.
Several methods also cast the extract-based summariza-tion as a sentence ranking problem, those include ranking through standard IR methods or identifying semantically important sentences by employing the latent semantic analysis technique [10].
Several learning to rank methods such as ranking SVM, support vector regression and gradient boosted decision trees are also applied to the summariza-tion task [19].
Nomoto and Matsumoto take the diversity issue into consideration with a preprocessing step in which the sentences of a given document are  rst clustered into several groups [23].
CollabSum [30] reduces sentence redundancy by discarding the highly overlapping sentences with already extracted highly ranked sentences.
Goldstein et al. consider this redundancy issue by employing a metric for reducing redundancy and maximizing diversity in the selected passages [9].
An earlier work by Carbonell and Goldstein [3] uses the idea of maximum marginal relevance for document reordering.
In fact, most supervised methods which claim to deal with the diversity issue follows similar approaches by using a pre-de ned  xed criterion.
Our approach distinguishes itself by directly incorporating the diversity requirement into the training process.
The coverage issue is also important in summarization.
IBM s many aspects document summarization tool takes this issue into consideration by automatically highlighting a set of sentences that are expected to cover the di erent aspects of the document s content [1].
However, to the best of our knowledge, most state-of-art approaches [17, 19, 26] employ 0-1 loss functions to measure the coverage of the ground-truth summary sentences.
This kind of measurement is quite coarse and it does not capture a signi cant part of fers from this by introducing a systematic measurement to make full use of the information contained in training data to enforce the coverage ratio of the summarization.
We also mention that summarization has always had a wide range of applications for web documents [27].
Compared with generic text summarization, web document sum-marization usually explore more data sources, such as click-through data, metadata, or hyperlinks.
Summarization is also used for web document classi cation [25], and particular for providing the snippets for search results.
The Diversity issue has be researched in many areas other than summarization.
From the point of view of searching for diversi ed results, Yue and Joachims [31] have employed structural SVM to predict diverse subsets using loss functions to penalize low diversity.
The approach taken by the DD-PREF modeling language tackles the diversity problem by measuring diversity on a feature basis [6, 29].
As another example, a framework presented by Clarke et al. rewards diversity in retrieval evaluation [4].
Besides, Kennedy and Naaman also propose a tool to generate diverse image search results [22].
Our method di ers from above methods by incorporating diversity as a constraint for the training process, which proves to be a novel solution for the summarization task.
Given a document x = {x1, x2, .
.
.
, xn}   X , where xi represents the i-th sentence in the document, and X is the space of all documents, the summarization task is to predict a subset y from the space of all possible subsets Y.
The general approach we pursue is supervised in nature, learning a summarization model from a set of training examples.
To this end, assume that we have a set of labeled training data, {(x(i), y(i)) | i = 1,       , n}, where y(i) is the ground-truth summary of the document x(i).
Given the training set, our goal is to construct a discriminant function F (x, y) : X   Y   R, which judges whether the subset y is a suitable summary for the document x.
The higher F (x, y) is, the better y summarize the document x.
Therefore, we can predict the summary of a document x by maximizing F (x, y) over the subset y   Y.
Formally, study, a loss function similar to F1 measure is applied:  (y,  y) = 2pr p + r , p = < y,  y > <  y,  y > , r = < y,  y > < y, y > .
(3) Here, given two subsets a and b, < a, b > denotes the number of common elements they share.
In this study, we will explore structural SVM to train a robust model for the summarization task.
For a given training set {(x(i), y(i)) | i = 1,       , n}, structural SVM is employed to learn a weight vector w for the discriminant function F (x, y) through the following quadratic programing problem [8, 28, 31]: Optimization Problem 1.
(Structural SVM) min w, 0

 kwk2+ c n n
 i=1  i, (4) subjected to:  i,  y   Y \ y(i),  i   0, wT  (x(i), y(i))   wT  (x(i), y) +  (y(i), y)    i.
In Equation (4), the parameter c controls the tradeo  between the model complexity 1 i=1  i, the sum of the slack variables  i.
The constraints for the optimization problem enforce the fact that the ground-truth summary y(i) should have a higher value of discriminant function than other alternatives y   Y.
As discussed above, we focus on training a summarization model which can enforce the diversity, coverage and balance of a summary.
This can be achieved by introducing three additional types of constraints to the optimization problem de ned in Equation (4).
In the following section, we  rst de ne the notion of subtopics of a document and then describe our constraints for diversity, coverage and balance in terms of subtopics.
Documents tend to contain several subtopics, and each subtopic can be represented by a cluster of sentences [11, 12].
Our constraints are based on this notion of subtopics.
One important thing to notice is that the structure of subtopics of documents are only used in the training process through additional constraints in the optimization problem (4), it is not used in the testing phase when we need to generate a summary for a new document.
y  = argmax F (x, y).
y Y (1)
 We describe each pair (x, y) through a feature vector  (x, y) the exact form of which will be discussed later.
The discriminant function F (x, y) is assumed to be linear in the feature vector  (x, y), i.e., F (x, y) = wT  (x, y) (2) To measure the summarization performance, a loss function  (y,  y) : Y   Y   R is employed to quantify the penalty of the predicted summary  y compared with the ground-truth summary y.
In our Each subtopic of a document generally consists of a subset of the sentences of the document.
For each document, we also de ne its subtopic set T as a set of subtopics that covers the document, i.e., T = {t1, t2, .
.
.
, tk}, where subtopic ti is associated with set of words.
We now de ne cover(ti, s) as the degree of coverage of the sentence s for the subtopic ti: cover(ti, s) = |ti   s| |ti| .
(5) Here s is an arbitrary sentence in the document.
Speci cally, cover(ti, s) represents the proportion of the words in the subtopic ti that are also present in the sentence s. Furthermore, for each sentence s, its coverage of the give subtopic vi = cover(ti, s).
It seems that we need to carry out a preprocessing step to construct the subtopic set for each document in the training set.
However, this is not necessary in our context, because for each training document x(j), we have the ground-thruth summary y(j).
Therefore, we simply use the i-th sentence in the ground-truth summary as the subtopic ti for the document.
Diversity argues a summary should not contain similar sentences.
In other words, sentences in a summary should have little overlap with one another in order to reduce redundancy.
Formally, we enforce diversity with the following constraint: vi, uj is the In constraint (8), the quantity Pk where k is the number of subtopics, u = Pyi y j-th component of the vector u, and  u = Pk j=1(uj    u)2 measures the variation of summary y s subtopics degree of coverage.
An unbalanced coverage of the subtopics results in a large Pk j=1(uj    u)2, which in turn leads to a relatively low score of wT  (x, y).
j=1 uj/k.
Considering the three types of constraints, we propose to train a summarization model enforcing diversity, coverage and balance through the following optimization problem: Optimization Problem 2.
min w, 0

 kwk2 + c n n
 i=1  i, (9) Constraint 1.
subjected to: wT  (x(i), y(i))   X y y(i) wT  (x(i), y)     +  i (6) Generally, adding the slack variable   tends to give slightly better performance.
In Equation (6), for the sentences in the ground-truth summary, the sum of their unique score wT  (x, y) should be no more than the overall score when they are regarded as a whole set.
In other words, we prefer summaries with each sentence focusing on di erent subtopics.
In this way, the commonly shared features will be associated with relatively low weight.
As the sentences similar to each other usually share lots of those features, a sentence set with less redundancy tends to be predicted.
Coverage means that the generated summary should cover all subtopics as much as possible.
Poor subtopic coverage is usually manifested by absence of some summary sentences.
The following constraint is employed to enforce coverage: Constraint 2.
wT  (x(i), y(i))   wT  (x(i), y) + (1   k X yi y vik)    i.
(7) For a given summary y, (1   kPyi y As de ned above, vi denotes the coverage of yi, the i-th sentence in summary y, of the subtopics of a given document.1 vik) quanti es loss of coverage of the subtopics.
As in (7), a large amount of loss of coverage leads to a relatively low score of wT  (x, y).
When predicting, sentences covering more subtopics tend to be extracted.
Balance requires that the generated summary should have relatively equal degree of coverage for each subtopic.
The extraction of unbalanced information tend to cause heavy loss of information, given that the size of the summary is limited.
We address this using the following constraint: Constraint 3.
wT  (x(i), y(i))   wT  (x(i), y) + k
 j=1 (uj    u)2    i, (8)
  i,  y   Y \ y(i) :  i   0,





 vik    i, j=1(uj    u)2    i.
The space Y of all possible subsets is complex.
In the following we construct an independence graph to guide our selection of y   Y.
We then solve the optimization problem (9) following the general cutting plane algorithm [28, 31], and also utilizing the independent graph.
The exploration of the whole space of Y in either the training or predicting process is known to be intractable.
So, the summarization task will greatly bene t from limiting the output space to the most probable subspace.
We achieve this by building an independence graph for a document.
Given a set of sentences S = {s1, s2, .
.
.
, sk}, where si represents the i-th sentence in the document, each sentence is considered as a node in the independence graph.
There is an edge between si and sj if their similarity is below a pre-de ned threshold  . Speci cally, the nodes si and sj with i < j are connected if and only if sim(si, sj) <  , where sim(si, sj) = m log(|si|) + log(|sj|) .
Here m is the number of times the same word appears in both sentences.
By making use of the paths in the independence graph, we shrink the searching space by avoiding the extraction of two similar sentences as we will demonstrate below.
In order to solve the optimization problem de ned in Equation (9), we employed the cutting plane algorithm [28, 31].
It iteratively adds constraints until the problem has been solved with a desired tolerance .
We start with a group of empty working sets yi, y i , for i = 1, .
.
.
, n. Then, we iteratively  nd the most violated constraints  y,  y i, y ,  y




 i =  , y for i = 1, 2, .
.
.
, n do i =   for i = 1, .
.
.
, n Algorithm 1 Cutting plane algorithm
 2: yi =  , y 3: repeat




     wT ( (x(i), y(i))    (x(i), y))   H(y) =  (y(i), y)     (y) = (1   kPyi y
 (y) = Pk j=1(uj    u)2    
 Compute:  y = argmaxyH(y), vik)    


 = wT  (x(i), y(i))   Py y(i) wT  (x(i), y)    
  y = argmaxyH
 (y),  y
 = argmaxyH (y)





 (y)} (cid:16)  Compute actual slack:  i = max{0, maxy Wi H(y), max (y), max





 y y i y y i if (cid:16)H

 <  i + (cid:17) or ( y) >  i + (cid:17) then (H( y) >  i + ) ( y) >  i + (cid:17) or (cid:16)H Add constraint to working set yi   yi   { y}, i   { y i   y y w   Optimize over  i(yi   y0 i   y i   { y i   y00 i ) }, y or }









 17: until no working set has changed during iteration.
end if end for Algorithm 2 Greedy algorithm using the independence graph

 3: for i = 1, 2, .
.
.
, k do
 y   argmaxy / yP(x, y,  y   {y}), where  y   {y} forms a path in the independence graph  y    y   {y}
 6: end for 7: return  y for each (x(i), y(i)) corresponding to the three constraints
 added to the corresponding working sets, and w is updated with respect to the new combined working set.
The learning algorithm is presented in Algorithm 1.
It is guaranteed to halt within a polynomial number of iterations [28].
For each iteration, we need to solve argmax P(x(i), y(i), y)   argmax  (y(i), y) + wT  (x(i), y) y Y y Y vik), or Pk for  (y(i), y) =  (y(i), y), or (1 kPyi y j=1(uj   u)2.
A greedy algorithm using the independence graph, described in Algorithm 2, is proposed to solve this problem where we repeatedly select the sentence y satisfying the following condition:  y   {y} is the sentence set having the highest score while its corresponding node set forms a path in the independence graph.
The algorithm ends with an extracted sentence set of size k. This algorithm has the same approximation bound as the greedy algorithm proposed by Khuller et al. [14] to solve the budgeted maximum coverage problem, that is to say, a (1   1 e )approximation bound.
So Algorithm 1 has a polynomial time complexity overall.
According to (1) and (2), we predict the summary for a given document x using: argmax P(x,  , y)   argmax F (x, y) = argmax wT  (x, y), y Y y Y y Y which is a special case that can be e ciently solved by Algorithm 2.
We discuss several features we use for the summarization task.
For a document x and a sentence set y, each component of the feature vector  (x, y) is set to 1 if the corresponding feature holds true, and 0 otherwise.
We  rst generate the feature vector for each types of features, and then concatenate them to construct the whole feature vector  (x, y).
These basic features are the most widely used features in summarization, and can be readily computed.
Word Frequency: Word frequency can be used to generate a set of features [31].
To denote the coverage degree of a document for a certain word and the importance of a certain word to a document, two kinds of levels are de ned: Term-importance: It de nes the percentage of a certain word in a sentence.
For example, the level 0.1 indicates that the frequency of a certain word in some sentence exceeds
 key words in the document.
A key word is de ned by certain rules, which will be discussed later.
Term-extent: It denotes the percentage of the sentences containing a certain word.
For example, the level 0.1 denotes that a certain word appears in no less than 10% of the sentences in the document.
For term-importance level A and term-extent level B, there are three binary features: A word in the term-importance level 0 (appears at least once in a sentence) appears in at least a B fraction of sentences at a term-importance level A.
A word in the term-importance level A appears in at least a B fraction of sentences at a term-importance level 0.
A word in the term-importance level A appears in at least a B fraction of sentences at a term-importance level A.
Let TI be the total number of term-importance levels, and TE be the total number of term-extent levels.
A total of T = (3   TI   2)   TE features can be then obtained.
For each word, we can get a feature vector of length T. The vectors of all words are summed to get a  nal feature vector.
Position: Every word of the  rst sentence of a document is labeled as a key word.
Thematic Word: Thematic words are de ned as the words with the highest frequency after having deleted the words in the stop-word list.
Each thematic word is labeled as a key word.
Length: The number of words in each sentence after removing the words in the stop-word list.
We prefer sentences with a length located in a certain range.
Several length levels are introduced to denote the length of a sentence.
The a feature vector can be generated with each component denotes a length level.
Upper Case words: Each upper case word is labeled as a key word.
We also consider several complex features that prove to be of great help for summarization.
Though requiring considerable e orts to generate, the numerous variants of these features are very helpful.
PageRank: The PageRank value of each sentence is calculated in a recursive manner as follows: PR(si) = (1 d) PR(si)+X d (PR(sj) sim(si, sj)) (10) where si is the i-th sentence in the document, PR(si) is the PageRank value of si and d is damper factor, which is set to be 0.85 in our experiments.2 There are two kinds of PageRank values: 1) innPageR-ank: Only the similarity between sentences in the same document is calculated.
2) interPageRank: Only the similarity between sentences from di erent documents under the same theme is calculated, other similarity scores are set to zero.
Finally the PageRank value of the sentence si is treated as a special key word with a word frequency PR(si).
Two-grams: Two successive words in the same sentence are regarded as a phase.
To generate this feature, each phase is denoted as a bi-word and de ned by an added special importance level of word frequency.
In the experimental study, we aim at addressing the following issues:
 approaches?
The results demonstrate that it achieves a remarkable improvement over those approaches.
ance in our framework improves the performance of the summarization?
According to our experiments, our proposed constraints do enhance diversity, coverage, and balance of the summary as well as improve performance.
summarization leads to the best performance?
A detailed analysis will be proposed in a later section.
The DUC2001 data set is used for evaluation in our experiments.
The data set, denoted as Bigset, contains around
 mainly two subsets for the summarization task, denoted as Docset1 and Docset2.
The respective ground-truth summaries are generated by manually extracting a certain number of sentences from each single document.
In the data set there are several themes, and under each theme there are several documents, which enables the interPageRank calculation.
For our summarization task, each document set is split into a training data set and a testing data set.
A 10-fold cross validation process is employed in the experiments to account for the uncertainty in the data set partition.
That is to say, the whole data set is divided evenly into ten folds.
eration by less than a given threshold (0.001 in our experiment).
Then 9 folds are used for training while the other one is kept for testing.
We use the following preprocessing steps: 1) We eliminate stop words from the original documents and execute stemming using the Porter s stemmer.
2) We calculate each word s frequency and tag each key word de ned according to the previous sections.
Other features are also calculated and represented in the input  les under various forms.
F1 measurement is widely used in summarization evaluation.
In F1 evaluation, the predicted summary  y and the ground-truth summary y are compared directly and the precision, recall, F1 scores are calculated as follows:  (y,  y) = 2pr p + r , p = < y,  y > <  y,  y > , r = < y,  y > < y, y > .
The ROUGE measure [18] is widely used for evaluation.
In fact, the DUC contests usually employ ROUGE measures for automatic summarization evaluation.
In ROUGE evaluation, the summarization quality is measured by counting the number of overlapping units, such as n-gram, word sequences, and word pairs between the predicted summary  y and the ground-truth summary y.
There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N which contains three sub-metrics:
 follows: ROUGE-N-R = Py y Pgramn y Countmatch(gramn) Py y Pgramn y Countground(gramn)
 as follows: ROUGE-N-P = Py y Pgramn y Countmatch(gramn) Py y Pgramn y Countpred(gramn)
 lows:



 Here n denotes the length of the n-gram, and gramn   y denotes the n-grams in the document y. Countmatch(gramn) is the number of gramn co-occurring in the predicted summary  y and the ground-truth summary y, Countground(gramn) represents the occurrence number of gramn in the ground-truth summary y, and Countpred(gramn) represents the occurrence number of gramn in the predicted summary  y.
According to Lin and Hovy [18], among all the evaluation measures in ROUGE, ROUGE-1 and ROUGE-2  t the single document summarization evaluation task very well.
As a result, for simplicity, in our experiment, only three ROUGE metrics are employed: ROUGE-1, ROUGE-2, ROUGE-W, where ROUGE-W is based on the weighted longest common subsequence.
The weight W is set to be 1.2 in our experiments.
In this section, several baselines, including both supervised and unsupervised methods are introduced for comparison.
This series of experiments is conducted on Docset1, Docset2, and Bigset.
To give a concise comparison, only F1 and ROUGE-2-R are employed for evaluation.
Among the supervised methods, we choose Support Vector Machine (SVM) (not structural SVM), Naive Bayes (NB), Logistic Regression (LR), Hidden Markov Model (HMM) and Conditional Random Field (CRF): SVM: SVM is widely used as a binary classi er, which is appropriate to distinguish summary sentences from non-summary sentences.
NB: This is a approach to classify single class variables in dependence of several feature values.
This model follows the assumption that the input variables are conditionally independent.
LR: LR can be regarded as a discriminative version of NB.
It is employed to model the posterior probabilities of k classes via linear functions in output variables.
HMM: HMM is an extension to NB for sequentially structured data also representing the dependencies of the input and output as a joint probability distribution.
CRF: CRF di ers from HMM by proposing a conditional probability model instead of a joint probability model.
We also compare with four unsupervised methods: Random: A summary is generated by selecting sentences randomly from a given document.
LEAD: This is a popular baseline on DUC2001 data set.
It works by selecting the lead sentences as the summary.
LSA: We identify semantically important sentences using the Latent Semantic Analysis technique and select the k most important sentences as the summary.
HITS: One of the Mihalcea s [20] algorithms based on the authority score of HITS on the directed backward graph.
The sentences with the highest authority score are selected in the summary generation.
The results of the baselines  performance on the Bigset are reported in [26].
Our approach is denoted as IndStr-SVM.
In our framework, several models can be trained by adapting di erent strategies in adding constraints and tuning parameters.
We set   = 0.5 and   = 0.4 in our experiments.
Our  rst experiment is conducted based on only the basic features.
Table 1 gives the performance of the unsupervised methods for this experiment, while Table 2 presents the results for the supervised methods.
As shown in Table 1 and Table 2, The random algorithm gives the worst performance as expected.
The performance of LEAD is much better than the Random algorithm.
LSA achieves better performance than the LEAD algorithm.
The result of HITS is the best among the unsupervised approaches, which shows the close relationship between summarization and the graph structure constructed from the sentences.
Compared with those unsupervised methods, supervised methods are generally better with the exception of HITS.
As extensions of NB, LR and HMM both result in a remarkable improvement over the performance of NB.
SVM proves to be a e ective algorithm by achieving a performance similar to that of LR and HMM.
CRF takes a big step forward as it achieves much better performance.
The comparison between CRF and HMM informs us that CRF does a better job in exploring the dependent structures.
Our method obtains the best performance by achieving an increase of 2.0% and
 F1 respectively and outperforming HITS by 7.4% and 6.0% on the Bigset in term of ROUGE-2-R and F1 respectively.
This can be attributed to the fact that the employment of the three constraints enforcing diversity, coverage, and the balance in the summary.
We also notice that Docset2 is much harder for summa-rization than Docset1, and our approach makes a greater improvement over other approaches on this task.
This demonstrates the robustness of our approach.
According to the comparison of the performance on Docset1, Docset2 and Bigset, the gap in the performance between our approach and other approaches is larger on small document sets.
This demonstrates that our approach performs better with less training data.
This phenomenon may due to the fact that in the generation of a good summarization model, the involvement of the diversity, coverage, and balance issues can partly replace the role played by a large training data set.
Now we discuss experiments using both the basic and complex features.
To save space, we presents the results in Tables 3 for the performance of the supervised methods only.
According to Table 3, our approach is still the best while CRF again achieves the second best performance.
This again illustrates that the dependence between sentences is important to summarization.
Compared with the best unsupervised method HITS, our approach achieves an improvement of 23.7% measured by ROUGE-2-R and 16.3% measured by F1 on the Bigset.
Our approach also outperforms the CRF methods by 10.4% and 2.1% on the Bigset in terms of ROUGE-2-R and F1, respectively, which is a much larger improvement than when only considering basic features.
It illustrates that our approach works better with complex features, i.e., it has larger capacity.
Our approach is still robust when using all the features as the gap in the performance between our approach and other approaches is still larger on Docset2.
The robustness of our approach illustrates that the diversity, coverage, and balance issues is important to a summary.
We believe that those issues provide the just experience that a good summarization model should learn, and reduce the model s dependence on the training data.
To identity how diversity, coverage, and balance enhance the quality of a summary, we provide several models through a strategy selection based on our approach.
Generally speaking, strategies can be sorted into two categories: constraint selection and parameter tuning.
Each time, we tune one strategy while others are  xed.
This series of experiments is conducted on the Bigset, and the performance is measured in terms of ROUGE-1, ROUGE-2, and ROUGE-W.
To understand the e ect of each proposed constraint, a series of experiments is conducted by employing di erent sets vided into those based on the basic features and those based on all the features.
We denote Indstr-SVM as the model trained with no constraint, Indstr-SVM-C1 as the model trained with the diversity-biased constraint involved, Indstr-SVM-C2 as the model trained with the coverage-biased constraint involved, Indstr-SVM-C3 as the model trained with the balance-biased constraint involved, and Indstr-SVM-All as the model trained with all three constraints involved.
According to Table 4, when using only the basic features, di erent constraints lead to various degrees of improvement.
Indstr-SVM-C2 achieves the best performance among the models trained with a single constraint as it outperforms Indstr-SVM by 4.5% and 4.1% in terms of ROUGE-2-R and ROUGE-2-F respectively.
This emphasizes the importance of the coverage issue in summarization.
Indstr-SVM-C1 outperforms Indstr-SVM by 4.4% and 3.4% while Indstr-SVM-C3 outperforms Indstr-SVM by 4.2% and 4.0% in terms of ROUGE-2-R and ROUGE-2-F respectively.
Indstr-SVM-All achieves the best performance as it outperforms Indstr-SVM by 4.7% and 4.7% respectively.
Table 5 presents us the performance on di erent constraint set using all the features: Indstr-SVM-C2 again results in the best performance by increasing 3.6% in terms of ROUGE-2-R and 4.0% in term of ROUGE-2-F over Indstr-SVM.
For the other two constraints, Indstr-SVM-C1 achieves an improvement of 1.0% and 1.2% respectively, as measured by ROUGE-2-R and ROUGE-2-F compared with Indstr-SVM.
Indstr-SVM-C3 outperforms Indstr-SVM by 2.3% and
 Indstr-SVM-All is still the best model, and outperforms Indstr-SVM by 3.9% and 4.4% respectively.
According to the above results, it can be concluded that the coverage-biased constraint makes the greatest contribution to summarization.
The result can be explained that the coverage requirement is more important than the other two in the summarization task.
When adding all three constraints, a robust model with the best performance of the summarization task is learned.
We also notice that there is a little overlap among the e ects of those constraints as Indstr-SVM-All just outperforms the models trained with single constraint.
The key parameter in our framework is the c used in (9), which aims at keeping a balance between weights and slacks.
Figure 1 shows the performance of Indstr-SVM-All based on basic features with the variation of c. The results in Figure 1 also show that the robust model performs better when c is small.
This indicates that our constraints focus on weight modi cation rather than on low training loss.
Figure 2 shows the performance of the robust model based on all features when the value of c varies.
According to the results in Figure 2, although there is a few ups and downs as the c parameter varies, the model trained with a small value of c performs relatively better.
This shows the same trend as when only considering basic features, which gives another proof that the constraints concerning the diversity, coverage, and balance issues play an important role in summarization.
In this paper, a novel approach is proposed to train a robust model for document summarization.
As a good sum-


t l u s e
 e g a r e v












 Figure 1: Performance of robust model based on basic features when varying the c value








 t l u s e
 e g a r e v





 Figure 2: Performance of robust model based on all features when varying the c value mary heavily depends on the diversity, coverage, and balance issues, our structure learning approach employs a structural SVM with several constraints to enforce them in a summary.
In our approach, we  rst build an independence graph to capture the structure of the output variable.
Then we employ a cutting plane algorithm to solve our proposed optimization problem.
Finally, the model obtained in the training process is used to predict the summary when given a new document.
Experimental results on the DUC2001 data set demonstrate the good e ectiveness of our approach.
The performance of our method achieves a remarkable improvement over a set of state-of-art supervised and unsupervised methods and the involvment of diversity, coverage, and balance in summarization proves to be of great help.
In future work, we plan to explore how to enforce diversity, coverage, and balance through feature generation.
We will also extend our framework to several applications, including web page summarization and snippets generation for which we will need to modify our method for query-biased summarization.
In web page summarization task, there will Docset1 Docset1 Docset2 Docset2 Bigset Bigset

































 Table 2: Results of supervised approaches based on basic features Docset1 Docset1 Docset2 Docset2 Bigset Bigset





 IndStr-SVM








































 Table 3: Results of supervised approaches based on all features Docset1 Docset1 Docset2 Docset2 Bigset Bigset





 IndStr-SVM








































 Table 4: Performance on di erent constraint set based on basic features constraint set








 Indstr-SVM Indstr-SVM-C1 Indstr-SVM-C2 Indstr-SVM-C3 Indstr-SVM-All












































 Table 5: Performance on di erent constraint set based on all features constraint set








 Indstr-SVM Indstr-SVM-C1 Indstr-SVM-C2 Indstr-SVM-C3 Indstr-SVM-All












































 for summary structure generation, which is expected to lead to better performance.
[14] S. Khuller, A. Moss, and J. Naor.
The budgeted maximum coverage problem.
Information Processing Letters, 70(1):39 45, 1999.
We thank the anonymous reviewers for their valuable and constructive comments.
Gui-Rong Xue thanks National Natural Science Foundation of China (NO.
60873211) for such generous support.
Part of the work of the fourth author is supported by the 111 Project (Grant No.
B07022) jointly funded by Ministry of Education and State Administration of Foreign Experts A airs, PRC.
