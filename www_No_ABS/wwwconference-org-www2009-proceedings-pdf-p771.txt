Popularity and data volume of modern Web 2.0 content sharing applications originate in their ease of operation for even unexperienced users, suitable mechanisms for supporting collaboration, and attractiveness of shared annotated material (images in Flickr, videos in YouTube, bookmarks in del.icio.us, etc.
).
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
The rapid increase in size of online communities and the availability of large amounts of shared data make discovering relevant content and  nding related users a di cult task.
For instance, thousands of new photos are uploaded to Flickr every minute making e ective automatic content  ltering techniques a necessity.
Flickr photos are accompanied by a variety of meta data such as tags, number of views, user comments, upload date, etc.
The Flickr search interface exploits the explicit and implicit ratings in the meta data to infer rankings.
For instance, the number of views is an indicator for the popularity of a photo, the upload date and the date a photo was taken at is an indicator for the recency of the content, and adding a photo to one s favorite list is probably the most direct positive relevance assignment in Flickr, and is an explicit expression of interest in the photo.
However, for recently uploaded photos community feedback in any form might not yet be available.
Furthermore, many photos are just sparsely annotated which might prevent text-based search and mining methods from retrieving this potentially attractive content.
Visual attractiveness is a highly subjective concept which has received extensive interest from the research community.
Semantic aspects are not critical, as the presence of certain concepts in a picture does not necessarily correlate with its appeal for viewers.
The artistic component has a major role in the perception of the aesthetics of images, and low-level features can provide a better insight on this aspect of photos.
Metrics such as sharpness, an overall value of the granularity of the image, or colorfulness, which measures the diversity of spectrum contained in the image, have been shown to provide high correlation with the human perception of attractiveness ([24]).
In this paper we focus on a methodology for automatically classifying and ranking photos according to their attractiveness.
We exploit the vast amount of social feedback available in Web 2.0 applications, more speci cally in Flickr, to obtain a training set of photos considered as more or less attractive by the community.
This allows us to build classi cation and regression models based on multi-modal visual and textual features, and to apply them to identify new attractive content.
In a wider system context, such techniques can be useful to enhance ranking functions for photo search, and, more generally, to complement mining and retrieval methods based on text, other meta data and social dimensions.
The rest of this paper is organized as follows: In Section 2 we discuss related work on image features, visual attractiveness, folksonomy mining, and machine learning.
Section 3 provides an overview of image attributes commonly asso-views methods to compute their values.
We provide a short overview of classi cation and regression techniques in Section 4, and explain how we can apply these techniques in the context of photo attractiveness detection.
In Section 5 we provide the results of the evaluation of our automatic attractiveness detection methods for classi cation and ranking of photos in Flickr.
We conclude and show directions of our future work in Section 6.
The determination of image quality metrics has received signi cant research interest under very diverse lines of work.
The analysis of human perception of color signals is the basis for an important number of image transformation techniques, as it provides a mechanism to assess visual quality at a perceptual level, i.e. as it is perceived by a human observer.
This is a common requirement for the evaluation of image compression algorithms [32], but also has applications in image enhancement techniques [12] and unsupervised calibration systems for image capturing gear (e.g.
auto-focus systems [30]).
Classic metrics such as PSNR and MSE [33] model quality degradation as a measure of the di erence between a baseline image and a variation.
They perform poorly as objective quality metrics, as they neglect the perceptual impairments associated to the absolute changes in signal values.
In the scope of this paper we are interested in quantitative metrics of perceived image quality rather than visual  delity.
Savakis et al. present a subjective evaluation of the sig-ni cance of di erent visual aspects for the determination of the overall appeal of natural images, in this case consumer photographs [24].
Their results show that, while the main factors are related to the presence of determinate concepts (e.g.
people) and artistic value (e.g.
composition), some speci c objective measures of visual features provide significant correlation to human judgements.
These results are supported by the work of Winkler [31] and Wee et al. [30], who propose ways to quantify sharpness and colorfulness of images and conduct extensive subjective experiments showing the properties of these features as e ective indicators of image appeal.
Additional metrics such as exposure [25], contrast [33, 22], or texture features [20] have also been used with varying levels of success to provide metrics of image appeal.
These metrics have been exploited for image retrieval and management applications, e.g.
detection and removal of undesirable images from photo collections [25].
However, few works have focused on providing accurate statistical models combining multiple features to predict the attractiveness of images.
Kalenova et al.
[16] propose an unsupervised model for spectral image quality characterization, and considers a very restricted set of only 5 images for its evaluation.
In [26], a method for classi cation using visual features is presented but its e ectiveness is not shown as evaluation is omitted.
In contrast, our work considers a large set of images from the popular Web 2.0 site Flickr, allowing to build robust classi ers and ranking models, and combines two di erent modalities: visual (content-based) and text (meta data) features.
In addition, we conduct a large scale evaluation to assess the viability of our approach.
Schmitz et al. have formalized folksonomies and discuss the use of association rule mining for analyzing and structuring them in [27].
Work on folksonomy-based web collaboration systems includes [5], [9], and [18] which provide good overviews of social bookmarking tools with special emphasis on folksonomies.
A node ranking procedure for folk-sonomies, the FolkRank algorithm, has been introduced in [11].
FolkRank operates on a tripartite graph of users, resources and items, and generates a ranking of tags for a given user.
Another procedure is the Markov Clustering algorithm (MCL) in which a renormalization-like scheme is used in order to detect communities of nodes in weighted networks [29].
A PageRank-like algorithm based on visual links between images is used to improve the ranking function for photo search in [13].
However, none of these these articles are using a combination of community feedback and visual features to classify and rank attractiveness.
There is a plethora of work on classi cation using a variety of probabilistic and discriminative models [4] and learning regression and ranking functions is well known in the literature [28, 23, 2].
The popular SVM Light software package [15] provides various kinds of parameterizations and variations of SVM training (e.g., binary classi cation, SVM regression and ranking, transductive SVMs, etc.
).
In this paper we will apply these techniques, in what is a novel context, to automatic image attractiveness assignment.
To the best of our knowledge, our paper is the  rst to apply and evaluate automatic classi cation and ranking methods for photo attractiveness based on visual features and textual meta data.
Furthermore, we are the  rst to propose gathering large training and evaluation sets for photo attractiveness based on community feedback in a Web 2.0 content sharing environment.
Image attractiveness is a very subjective concept in u-enced by a wide number of factors.
In previous studies, it has been shown that high level semantic attributes, such as people expressions or picture composition, are the most relevant when determining the overall appeal of a photo [24].
The current limitations in semantic understanding of images prevent automatic methods from taking advantage of them for the establishment of models.
However, there are a number of other attributes which also in uence the perception of image attractiveness and that can be measured.
This is illustrated in Figure 1, where pairs of semantically a ne pictures with varying appeal levels are depicted, showing how semantic properties could just be insu cient for the correct classi cation of pictures in terms of their attractiveness.
In this section we introduce image features available from the content and its associated meta data that we use later for the training of models for image attractiveness classi cation.
It is widely accepted that human perception of images is mainly in uenced by two factors, namely color distribution and coarseness of the patterns contained [12].
These are complex concepts which convey multiple orthogonal aspects that have to be considered individually.
Figure 1 shows several examples.
For the same semantic concepts (columns in the  gure), very di erent perceptions of image quality can be perceived.
The images in the upper row are generally perceived as more appealing, mainly because of their higher semantic concept (animal, landscape, portrait,  ower) but di erences in appeal-related visual attributes.
artistic value.
Even though artistic quality cannot be quantitatively computed, it correlates to certain visual features of images, assigning more optimal values to them.
For instance, appealing images tend to have higher colorfulness (column 2), increased contrast (column 3) and sharpness.
In this section, we review some of the most relevant visual aspects which we intend to use as image quality indicators.
Color is the pillar of the human vision system.
It can be expressed in absolute terms as coordinates in a speci c color space.
Di erent color spaces have been de ned to suit the requirements of di erent color-dependent applications.
In this section we make use of some of them to establish color attributes of the image.
In addition to the well-known sRGB, we also refer to the HSV (Hue-Saturation-Value) and HSL (Hue-Saturation-Lightness) color spaces, which provide a more intuitive representation of colors for humans [1].
The YUV (Luma-Chrominance) color space is also used as it maps luminance intensity (brightness) directly as the Y coordinate.
Finally, the CIEL u v  color space [1] is the most comprehensive color model, capable of describing the complete visible spectrum.
It provides an interesting color decomposition with two chromaticity components, u and v.
The following attributes are commonly used to characterize the color present in images: Brightness The brightness of a color is a measure of the amplitude of its light wave, or intensity.
Even though it is a very simple attribute, it has been e ectively used for  ltering poorly exposed photos [25].
For images in the YUV color space, it can be straightforwardly determined as the average of the luminance values, Y , of the complete sequence of pixels,

 N Xx,y Yxy (1) where Yxy denotes the luminance value of pixel (x, y) and N denotes the size of the image.
Saturation: The saturation of a color is a measure of its vividness.
It is de ned as the di erence of intensity of the di erent light wavelengths that compose the color.
In the CIEL u v  space, saturation is de ned by the expression Suv = 13p(u    u  0)2 + (v    v 
 (2) 0 and v  where u  and v  are the chromaticity coordinates of the con-0 are the corresponding (u , v ) sidered color, and u  coordinates for the white reference color chosen.
In other color spaces, including HSV and HSL, various correlates of saturation are directly mapped into their coordinates.
According to the de nition of HSV, saturation can be established using S = max(R, G, B)   min(R, G, B) (3) where R, G and B are the coordinates of the color the sRGB color space.
Colorfulness: The colorfulness of a color is a measure of its di erence against grey.
When considering the pixels of an image altogether, the individual distance between pixel colors is also taken into account.
Winkler [31] proposes to compute the colorfulness index using the distribution of chroma values.
A more e cient method for images coded in the sRGB color space is described by Hasler [10].
The opponent color space is de ned as rg = R   G, yb =


 and colorfulness can be obtained using Cf =  rgyb + 0.3    rgyb,  rgyb = q 2  rgyb = q 2 rg +  2 yb, rg +  2 yb (4) (5) (6) Naturalness: This highly subjective concept aims at providing a measure of the degree of correspondence between aspects of perceived color attributes, such as colorfulness or dynamic range.
Huang et al. propose a method to obtain a quantitative value [12].
Considering colors are in the HSL color space, they use pixels with 20   L   80 and S > 0.1.
These are grouped according to their hue (H coordinate) value in three sets:  A - Skin ,  B - Grass  and  C - Sky .
Average saturation values for each group,  S, are used to compute local naturalness indexes using the following expressions: NSkin = e NGrass = e NSky = e
  0.5  A  0.5  B  0.5  C




 , if 25   hue   70 , if 95   hue   135 , if 185   hue   260 The  nal naturalness index is given by the expression: N = Xi  iNi, i   { Skin ,  Grass ,  Sky } (7) where  i denotes the proportion of pixels of group i in the image.
Contrast: As introduced above, color perception depends heavily on the relation of local luminance variations to the surrounding luminance.
Contrast measures this relative variation of luminance.
Multiple de nitions for computing the contrast index have been proposed.
Weber s de nition provides a simple way to obtain contrast for simple periodic patterns as:


 (8) The RMS-contrast is commonly used to determine contrast in a way which allows to be compared between independent images: C rms =
 n   1 n Xi=1 (xi    x)2 (9)
 Coarseness, on the other hand, represents the degree of detail contained in an image.
It mainly depends on the quality of the capturing gear and the photographer, and closely relates to the notions of resolution (number of pixels per inch) and acutance (maximum color change ratio per inch).
The most commonly used metric to determine the coarseness of images is sharpness.
Sharpness measures the clarity and level of detail of an image.
Its importance in the  nal appearance of a photo has been repeatedly emphasized by professional photographers and studies on image appeal [24].
Sharpness can be determined as a function of its Laplacian, normalized by the local average luminance in the surroundings of each pixel: Sh = Xx,y L(x, y)  xy , with L(x, y) =
  x2 +
  y2 (10) where  xy denotes the average luminance around pixel (x,y).
In addition to visual features, the textual annotation of images available in Web 2.0 folksonomies such as Flickr can provide additional clues on the attractiveness of photos.
This holds partly due to correlations of topics with appealing image content.
As an illustrative example we computed a ranked list of tags from a set of 12,000 photos with more than 5 favorite assignments ( attractive ) and another set of the same size containing photos without any favorite assignments ( unattractive ).
For ranking the tags, we used the Mutual Information (MI) measure [21, 19] from information theory which can be interpreted as a measure of how much the joint distribution of features Xi (terms in our case) deviate from a hypothetical distribution in which features and categories ( attractive  and  unattractive ) are independent of each other.
Table 1 shows the top-50 terms extracted for each category.
Obviously many of the  attractive  photos contain nature motives (e.g., sunset,  ower, animals), have tags relating to photo technology (canon, nikon, hdr), emphasize artistic aspects and colors (blackandwhite, green, red, etc.)
and contain positive statements (supershot, colorpho-toaward).
 Unattractive  photos, on the other hand, are often about family occasions (e.g., birthday, wedding, family, dad) as well as other private events and activities (graduation, party, weekend, trip, camping) which are of importance for a small circle of friends and family members but less interesting for a larger community of Flickr users; furthermore, the technical quality of some of these photos might be a ected by their amateur character.
In the previous section, we have seen how feature representations of photos can be obtained using analysis of visual content and textual annotations.
In this section, we provide a short review of speci c classi cation and regression techniques known from the machine learning literature, and show how these techniques can be applied to our scenario.
We use classi cation models to automatically categorize photos as attractive or unattractive, and regression models to obtain lists of photos ranked by their attractiveness.
In order to classify photos into categories  attractive  or  unattractive  we use a supervised learning paradigm which is based on training items (photos in our case) that need to be provided for each category.
Both training and test items, which are later given to the classi er, are represented as multi dimensional feature vectors.
These vectors can be constructed using tf or tf   idf weights of tags and the visual features described in Section 3).
Photos labeled as  attractive  or  unattractive  are used to train a classi cation model, using probabilistic (e.g., Naive Bayes) or discrimina-tive models (e.g., SVMs).
How can we obtain su ciently large training sets of  attractive  or  unattractive  photos?
We are aware that the concept of appeal lies in the eye of the beholder, and is highly subjective and problematic.
However, the amount of community feedback in Flickr results in large annotated photo sets which hopefully helps to average out noise in various Terms for Attractive photos green abigfave impressedbeauty natures nest red nature sky  ower supershot macro aplusphoto blue bw canon anawesomeshot water white nikon portrait girl diamondclassphotographer sunset  owers art light explore  ickrdiamond blackandwhite color yellow clouds blueribbonwinner pink black woman soe night landscape bravo superbmasterpiece colorphotoaward  lm sea hdr coolest street beach animal sun garden Terms for Unattractive photos
 wedding graduation
 party trip honeymoon vacation

 camping festival may canyon ubc ubcaagrad07s tour family bbq softball madagascar memorialday prague china cycling cruise kollegstufenfahrt birthday drinking vegas memorial matt pics vietnam
 urlaubvacation kreuzfahrtcruise commencement mvmarcopolo grand race mt dad
 weekend kenya part ian regatta bermuda forms and, thus, re ects to a certain degree the  democratic  view of a community.
To this end we considered distinct thresholds for the minimum number of favorite assignments NumFav for photos; in Section 5 we will see that favorites are highly correlated with other kinds of community feedback such as number of comments or views.
Formally, we obtain a set {( ~p1, l1), .
.
.
( ~pn, ln)} of photo vectors ~pi labeled by li with li = 1 if N umF av lies above a threshold ( positive  examples), li =  1 otherwise ( negative  examples).
Linear support vector machines (SVMs) construct a hy-perplane ~w  ~x+ b = 0 that separates the set of positive training examples from a set of negative examples with maximum margin.
This training requires solving a quadratic optimization problem whose empirical performance is somewhere between linear and quadratic in the number of training items [3].
In real life, the classes in the training data are not always separable.
To handle the general case where a single hyperplane may not be able to correctly separate all training points, slack variables are introduced in order to relax the constraints of the optimization problem.
For a new, previously unseen, photo ~p the SVM merely needs to test whether it lies on the  positive  side or the  negative  side of the separating hyperplane.
The decision simply requires computing a scalar product of the vectors ~w and ~p.
SVMs have been shown to perform very well for various classi cation tasks (see, e.g., [7, 14]).
Other discriminative classi ers (e.g., based on Fisher discriminants) trade o  some accuracy for speed [6], but we restrict ourselves to linear SVMs.
SV-  regression [28] computes a function f (~x) that has a deviation     from the target relevance values ri of the training data with a minimum value for   and at the same time is as  at  as possible.
For a family of linear functions ~w ~x+b  atness  means that || ~w|| is minimized which results in the following optimization problem: minimize || ~w||2

 subject to   ri   ~w ~pi   b     ~w ~pi + b   ri     (11) (12) Similar to the classi cation scenario, slack variables can be introduced if the constraints of the optimization problem cannot be met.
By means of the learned regression function f , relevance values f (~p) can be assigned to vector representations ~p of new test photos, resulting in a list of photos ranked according to their attractiveness.
In this section, we present the results of our evaluation for automatic detection of photo attractiveness.
First, we describe our strategy for gathering a photo collection from Flickr, and elaborate on the characteristics of our data set.
Then, we present the outcome of our twofold evaluation methodology: 1) We examine the in uence of the enhanced photo representations on automatic classi cation of photo attractiveness.
2) We apply regression models to obtain rankings of photos according to their attractiveness.
To learn a regression model we consider training sets {( ~p1, r1), .
.
.
, ( ~pn, rn)} of photo vectors ~pi along with relevance values ri   R instead of the category labels used for classi cation.
We are considering the number of favorite assignments N umF av for a photo pi as relevance value, and feature vector representations of photos as described in the previous subsection on classi cation.
We gathered a sample of photos from Flickr uploaded in the time between June 1 and 7, 2007.
We used the Flickr API to query for photos uploaded in 20 minutes time intervals.
In this way, we obtained a total of 2.2 M photos in medium size from 185 k users (note that this is just the subset of photos provided by the Flickr API, the actual amount of uploaded photos during that time is larger).
t s o o h
 f o o
 1e+07 1e+06














 >20 Favorite Assignments Figure 2: Distribution of favorite assignments The relatively short time frame of one week (compared to the existence of Flickr) guarantees that for all photos, there was roughly the same chance to obtain community feedback.
For each photo, we extracted the number of times the photo was assigned to favorite lists.
Figure 2 shows the distribution of the number favorite assignments for these photos.
Since adding a photo to one s favorite list is probably the most direct positive assessment, we used the number of favorites as relevance values for building and testing machine learning models.
This is also justi ed by the high correlation of the number of favorite assignments with other important indicators of community interest.
We computed the correlation with the number of views/comments and obtained the following values for Kendall s Tau-b: 0.688 for views, 0.767 for comments.
Positive examples were selected using all the photos with at least 2 favorite assignments.
We deliberately dismissed photos with just 1 favorite assignment as they do not provide su cient evidence of social agreement.
This resulted in a set of 35,000 photos.
In addition we chose a random sample of 40,000 photos without any favorite assignments as the set of negative examples.
In Section 3, we have presented di erent methods for extracting visual features and textual features, resulting in enhanced combined feature representations of photos.
Machine learning algorithms described in Section 4 make use of this feature information to generate models, and to automatically organize the data.
In this section, we show results for classi cation as well as ranking.
Classifying data into thematic categories usually follows a supervised learning paradigm and is based on training items that need to be provided for each topic.
We used the SVM-light [15] implementation of linear support vector machines (SVMs) with standard parameterization in our experiments, as this has been shown to perform well for various classi cation tasks (see, e.g.,[8, 14]).
We performed di erent series of binary classi cation experiments of Flickr photos into the classes  attractive  and  unattractive .
We are aware that the concept of appeal is highly subjective and di cult to capture.
However, the large amount of community feedback in Flickr allows for a large scale evaluation which hopefully helps to average out noise and re ects, to a certain degree, the view of the community.
For our classi cation experiments, we considered di erent levels of restrictiveness for the class  attractive ; to this end we considered distinct thresholds for the minimum number of favorite assignments for a photos (NumFav   2, 5, 10 and 20) to be considered as  attractive ; photos without any favorite assignments were considered to belong to the category  unattractive .
We considered di erent amounts of randomly chosen  attractive  training photos (T = 500, 2000, 8000, 20000) as positive examples (where that number of training photos and at least 1000 test photos where available), and the same amount of randomly chosen  unattractive  photos as negative samples.
For testing the models based on these training sets we used the disjoint sets of remaining  attractive  photos with same minimum number of assigned favorites and a randomly selected disjoint subset of negative samples of the same size.
We compared the following methods for producing visual features from photos, introduced in Section 3, and build (1-dimensional) feature vectors for classi cation: 1. brightness: computed using equation (1).
2. contrast: computed using equation (9)
 extension of equation (9) into the three-dimensional RGB color space.
4. saturation: computed as the average of saturation values across the complete sequence of pixels as de ned by equation (3) 5. saturation variation: computed as the standard de- viation of the distribution of values used for saturation.
6. colorfulness: computed using equation (4) 7. sharpness: computed using equation (10) 8. sharpness variation: computed as the standard de- viation of the distribution of values used for sharpness.
9. naturalness: computed using equation (7) In addition, we studied the following higher dimensional combined feature vectors: 1. text: feature vectors based on the tag representation of the photos using tf weighting 2. visual: 9-dimensional feature vectors combining the visual features described above 3. text+visual: combination vector obtained from the textual and visual features Our quality measures are the precision-recall curves as well as the precision-recall break-even points (BEPs) for these curves (i.e. precision/recall at the point where precision equals recall which is also equal to the F1 measure, the harmonic mean of precision and recall in that case).
The results for the BEP values are shown in Tables 2 through 5.
The detailed precision-recall curves for the example case of T=8,000 training photos and minimum number of favorite assignments N umF av=5 are shown in Figure 3.
The main observations are:   The combination vectors obtained from textual and visual features (text+visual) provide the best performance.
For instance, the con guration with T=8000 positive/negative training photos and minimum Num-Fav =5, leads to a BEP of 0.8363.
Consistently, similar observations can be made for all examined con gura-tions.
  Attractiveness classi cation based just on textual features (text) performs surprisingly well, e.g., BEP = explained by a higher interest in certain topics (e.g.
woman, car), topic correlation with high technical quality of the pictures (e.g.
nature motives, photos annotated by camera-related terms), or, in some cases, quality assignments in tags (e.g.
 awesomephoto ).
  Although classi cation using a combination of all visual features (visual) is outperformed by classi cation with textual features (BEP = 0.6664 for T=8000 and NumFav   5) trading recall against precision still leads to applicable results.
For instance, we obtain prec=0.7975 for recall=0.3, and prec=0.8472 for re-call=0.1; this is useful for  nding candidates of attractive photos in large photo sets.
Furthermore, classi ers based on visual features have the additional advantage that they can be applied in a more  exible way and in a broader context, e.g., in the absence of textual annotations or in personal photo collections.
We have also studied each of the visual features individually.
As expected, each of these features alone proves less powerful than their combination.
BEPs are typically around 0.5; however, the precision-recall curves reveal in most cases a clear increase of precision with decreasing recall and thus show that these features are indeed indicators of photo attractiveness.
The much higher performance of the combined visual features indicates more complex patterns and relationships between the visual dimensions.
Classi cation results tend to improve, as expected, with increasing number of training photos.
Furthermore, the classi cation performance increases with higher thresholds for the number of favorite assignments for which a photo is considered as  attractive .
Ranking algorithms order a set of objects, Flickr photos in our case, according to their relevance values.
For our experiments we chose SVM Regression using the SVMlight [15] implementation with standard parameterization for regression.
For training the regression model, we randomly selected 20,000 photos with more than 2 favorites and the same number of photos with 0 favorites.
We tested the model on the remaining (disjoint) set of photos with NumFav 2 and on a disjoint set of the same size containing photos with no favorite assignments.
The list of test photos in descending order of their number of favorite assignments was considered as ground truth for our experiments.
We compared the order of the automatically generated rankings using Kendall s Tau-b [17]:  b =
 (13) p(P + Q + T1)(P + Q + T2) where P is the number of concordant pairs, Q is the number of discordant pairs in the lists, T1 is the number of pairs tied in the  rst but not in the second list, and T2 is the number of pairs tied in the second but not in the  rst list.
Values for  b can range from 1 to 1.
We have chosen the Tau-b version in order to avoid a systematic advantage of our methods due to many ties produced by the high number of photos with same numFav value.
We constructed feature vectors based on tags (text), single visual features, all visual features (visual) and their combination (text+visual) in the same way as for the classi cation experiments described in the previous Section 5.2.1.
Table 6: Ranking using Regression (Kendall s Tau-b): 40000 training photos Method brightness contrast RGB contrast saturation saturation variation colorfulness sharpness sharpness variation naturalness text visual text+visual Kendall s Tau-b
 -0.0172


 -0.0497
 -0.0914



 The results of the comparison are shown in Table 6.
The main observations are very similar to the ones obtained for the classi cation scenario:   The combination vectors obtained from textual and visual features (text+visual) provide the best ranking performance ( b = 0.4841).
This value illustrates a remarkable correlation of our model with the actual community feedback, proving the viability of our proposed multi-modal approach.
  Ranking using a combination of all visual features (visual) is outperformed by ranking with textual features.
However, ranking with only visual features still produces promising results and can be useful for cases and applications where no or insu cient textual photo annotation is available.
In this paper, we have shown that community feedback in social sharing systems in combination with a multi-modal image representation based on textual annotation and visual features can be used for automatic assignment of photo attractiveness values.
More speci cally, in what is a novel approach, we have used favorite assignments in the photo sharing environment Flickr to obtain training data and a ground truth for a community-based notion of  attractiveness .
We used textual as well as various visual features for constructing vector representation of photos and for building classi cation and regression models.
Our classi cation and ranking experiments show the best performance for a hybrid combination of tags and visual information.
However, the approach of using only visual features shows applicable results as well, and has the advantage of a higher  exibility in the sense that it can be applied in scenarios where no textual meta annotation is available (e.g.
personal photo collections or photos without annotations on the web).
We plan to extend and generalize this work to consider various kinds of resources in folksonomies such as still and moving pictures (Youtube) or text (del.icio.us), and use different content and meta data analysis techniques to obtain appropriate object representations.
The extension of this work to moving pictures presents several challenges.
Perceived quality in this scenario cannot be directly obtained from the analysis of independent frames, as the inherent redundant nature of videos is used by the human brain to Method brightness contrast RGB contrast saturation saturation variation colorfulness sharpness sharpness variation naturalness text visual text+visual NumFav   2 NumFav   5 NumFav   10 NumFav   20















































 Table 3: Classi cation Results (BEP): 2000  attractive / unattractive  training photos Method brightness contrast RGB contrast saturation saturation variation colorfulness sharpness sharpness variation naturalness text visual text+visual NumFav   2 NumFav   5 NumFav   10



































 Table 4: Classi cation Results (BEP): 8000  attractive / unattractive  training photos Method brightness contrast RGB contrast saturation saturation variation colorfulness sharpness sharpness variation naturalness text visual text+visual NumFav   2 NumFav   5























 Table 5: Classi cation Results (BEP): 20000  attractive / unattractive  training photos Method brightness contrast RGB contrast saturation saturation variation colorfulness sharpness sharpness variation naturalness text visual text+visual NumFav   2











 i n o s c e r
 i i n o s c e r
 i i n o s c e r
 i i n o s c e r




























 brightness





 Recall saturation





 Recall sharpness





 Recall text





 Recall i i n o s c e r
 i i n o s c e r
 i i n o s c e r
 i i n o s c e r




























 contrast





 Recall saturation variation





 Recall sharpness variation





 Recall visual





 Recall i i n o s c e r
 i i n o s c e r
 i i n o s c e r
 i i n o s c e r




























 RGB contrast





 Recall colorfulness





 Recall naturalness





 Recall text+visual





 Recall Figure 3: Precision-recall curves for visual and textual dimensions and their combination (8000 training photos per class, numFav 5) produce an improved version which is what we ultimately sense.
For the case of text documents, besides a simple Bag-of-Words approach which would capture correlations with attractive topics, stylometric features based on sentence structure, vocabulary distributions and linguistic constituents might provide additional clues about the attractiveness of longer texts.
Furthermore, we intend to introduce more accurate computations of the described visual attributes, e.g.
eigenvalues-based sharpness detection, as well as additional features, such as texture descriptors.
In addition, besides an aggregated community-based perception of attractiveness, we would like to study recommender mechanisms taking individual user contexts and preferences into account to provide personalized results.
We think that the proposed techniques have direct applications to search improvement, where automatically computed  attractiveness  can, besides other criteria, be taken into account to improve relevance assignments and rankings.
wider system context and encompassing additional complementary retrieval and mining methods is of high practical importance.
This research was partially funded by the EU Marie Curie ToK Grant Memoir (MTKD-CT-2005-030008), and the Large-Scale Integrating EU Project LivingKnowledge.
