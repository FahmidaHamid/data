Weblogs, also known as blogs, are becoming important forms of individual-driven media outlets due to their simplicity, availability, and  exibility.
According to [1], the bl-ogosphere is doubling once every 5 months and upto 40,000 new blogs are being created each day.
While some of these blogs are personal journals, a signi cant portion of the blo-gosphere consists of  lters and notebooks, which are tracking real-world events, such as political news, sports, technology, or entertainment [13].
As more individuals start relying on the Web as their primary information source (and outlet), a parallel development is also observed in more traditional (print and broadcast) media outlets: in order to reach consumers through alternative venues (and thus increasing their advertisement revenues), the number of news sites on the Web is also continuously increasing.
While the numbers of these sites grow, the same cannot be said about the unique content: as it can be attested by anyone who has looked through a list of news articles grouped together using automatic generated news sites (such as Google News websites [3] and Yahoo News [5]), content-reuse (whether in the form of extensive quotations or content borrowing across media outlets) is very common in blogs and news outlets tracking the same real-world event.
During the past few years, there has been growing research on analysis of information  ow within the blogo-sphere.
[19] for example introduces and analyzes community graphs, where edges indicate how one blog communicates with another.
Adar et al. [8] also considers information propagation across blog entries, but relies on the embedded hyperlinks.
However, while explicit hyperlinks provide a fairly good indicator of content-wise relatedness in the more gen-news entries and further help the user to pick among such related entries in an informed manner eral Web content, the same cannot be said for blog and news entries [28, 29].
In fact, knowledge about which web entries reused content or quoted from which others must also be leveraged when organizing (e.g., assessing, ranking, and linking) these entries for presentation (Figure 1).
For example, Figure 2 depicts a hypothetical news site, designed to help readers to observe quotations and content-borrowings across news entries and further help the user to pick among such related entries in an informed manner.
Quotation-based links also can be used for navigating between related books.
For example, [30] presents a novel user interface for navigating between books by using links based on quotations, which is currently used in Google Book Search [4].
While we are not focussing on how to leverage reuse information for e ective presentation, in this paper we concentrate on the problem of e cient detection of content reuse.
Reuse detection metric: Reuse detection is an important problem that has implications in various application domains, including copy (plagiarism) detection and biological sequence mining.
A particular challenge in reuse detection is that reuse can happen at di erent levels and detecting di erent types of reuses can require di erent techniques.
Established techniques include information retrieval measures [33, 18, 36, 20] and  ngerprinting [12, 34, 31, 24, 14, 26, 32], where the document is treated as a sequence of symbols and substring (q-gram) based  nger-prints are extracted from the documents.
In particular, COPS [15] and SCAM [36] focus on the problem of copy prevention.
[36] identi es four di erent types of reuse: plagiarism, subset, copies, and related and shows that di erent test are applicable for di erent types of reuse.
While COPS relies on sentence chunking, SCAM applies word chunking for more precise detection.
In this paper, our focus is not on developing better reuse metrics, but on the e cient identi cation of reuse in large collections.
Thus, we develop a mechanism for e cient word-overlap based reuse [33] by mapping sentence domain context to a multidimensional signature space and leveraging range searches in this space.
Index schemes: There have been a number of proposals for  nding near-duplicate documents in the database and web-search communities [21, 37, 10].
The near-duplicate detection have been used in diverse applications, including data cleaning and integration in DBMS as well as copy detection in web documents.
[38] proposes instance-level constrained clustering approach to detect near-duplicated documents.
Similarity join algorithms in the database are used to  nd pairs of records whose similarity scores are above a given threshold [23, 9, 40, 17].
[9] presents algorithms for computing exact set-similarity joins by converting the similarity threshold to a threshold on Hamming distance.
[23] exploited q-grams to  nd similar string pairs in the database where edit distance between pairs of string is measured based on the overlap constraints on q-grams, such as count, position, and length  ltering.
In [17], the string similarity functions are measured as the overlap constraint between q-grams.
From an indexing perspective, one possible approach is to apply near neighbor searches in high dimensional spaces to the problem of duplicate identi cation [22, 35].
Locality-Sensitive Hashing [22], which uses several hash functions so that similar data are mapped to the same buckets with high probability, is one alternative.
Another approach recently applied to duplicate detection is to use inverted list based algorithms to  nd all pairs of documents whose similarity scores are above a given threshold [10].
We note that one disadvantage of inverted index-based schemes is that these do not lend themselves to e cient incremental implementations, where newly arriving blog and news entries are mapped incrementally against existing entries in the collection.
Another constraint (due to the size of blo-gosphere and  ne-grained, sentence-level reuse detection) is that, unlike [22], [10], and many others, the underlying index structure of qSign is implemented on secondary storage rather than the main memory.
Knowledge about content-reuse and overlaps is not cheap to acquire: considering the size of the blog and news space, the cost of identifying content-overlaps (even within entries that can broadly be categorized to be similar) can be expensive.
In fact, while quotation and reuse detection on digital documents is not new problem (see Section 1.1), existing techniques are not suitable for the size and dynamicity of the blogosphere and online news sites.
Thus, it is essential that the techniques used for identifying reuse are fast and scalable: Desideratum 1.1 (Efficient).
Considering the size of the blogosphere, it is essential that the techniques developed for content-reuse and overlaps are fast and scalable.
More importantly, the dynamic nature of blog and news entries necessitates incremental processing for reuse detection.
Desideratum 1.2 (Incremental).
Reuse detection should be an incremental process in that whenever a new contents is provided, it is compared to entries in the data collection to  nd reuses.
Thus, in this paper, we develop a novel qSign algorithm that e ciently and incrementally analyze the blogosphere for quotation and reuse identi cation.
In particular, we propose an indexing scheme to prune sentences whose reuse scores are predicted with high probability to be less than a given threshold.
Note that [39] showed that for boolean keyword queries inverted index based schemes outperform signature based schemes.
In this paper, we show that appropriately designed signature based schemes can in fact outperform inverted index based schemes in the context of reuse detection.
Intuitively, this is due the fact that while Boolean query processing necessitates investigation of all signatures which have at least one matching bit with the query, candidate selection for reuse detection is based on a similarity measure which lends itself to indexing and pruning.
Consequently, not all signatures which have a matching bit need to be investigated for candidate selection.
This provides signi cant savings that were not possible in Boolean keyword query processing and helps signature-based scheme we present in this paper to outperform state-of-the-art inverted index based scheme in reuse detection task.
Experiments in Section 3 show that qSign algorithm signi cantly improves the reuse detection speed e ciency and provides high recall and precision.
Processing time gains can be pushed multiple orders of magnitude (from 100X to 1000X) for 70% recall.
Furthermore, when we adapted state-of-the-art duplicate detection technique [10] into sentence-based reuse detection problem, qSign provides time gains from 10X to 100X, while maintaining reuse detection rates of upto 90%.
As described earlier, there are many metrics for reuse detection.
In RECAP [33], Metzler et al. studied various reuse patterns, including (a) identical, (b) su ciently overlaping (common source), and (c) some overlap (common knowledge).
In particular, [33] showed that, when applied at the sentence level (as opposed to identifying general topical similarity at the whole-document level), the word overlap measure was competitive when looking for reuse: given a query File : Motorola also displayed a music phone.
Word Signature of word Motorola 0011 0010 music 0001 1100 phone 0001 0110 Signature of File (bitwise or) 0011 1110 User Query Signature of user query (a) match : Motorola 0011 0010 (b) no match : game 1000 0011 (c) false match : television 0010 1010 Figure 3: Signature generation and use sentence Q and a document sentence R, the degree of word-overlap is de ned as


 (cid:1) ( w Q R log
 dfw ).
See [33] for more detailed description of the algorithm.
Unfortunately, since word-overlap metric requires sentence-level comparison, detecting content-reuse within a large collection, based on word-overlap metric would be very expensive.
In this section, we present an e cient and incremental qSign algorithm which, while not being exact, can guarantee word-overlap lowerbounds for reuse detection.
tal Reuse Detection Consider a naive indexing approach where a sentence le stores keyword vectors for the sentences in the collection.
Naturally, given a sentence (from a new blog entry), identifying the reuse by scanning the entire sentence le to compute the similarity scores for each sentence is likely to be very costly.
One possible solution is to pre-cluster the blog entries based on the recency.
Furthermore, it could also be possible to mine for independent topics in the collection using a content-only scheme (such as latent semantic indexing (LSI)-based method [25]).
Yet, since the number of relevant blog entries can still be large after clustering, it is crucial to reduce the quotation and reuse computation costs and render the detection process incremental.
While developing the word overlap measure, [33] observes that when two sentences have the same source or one sentence is the reuse of the other sentence, two sentences contain a great number of common.
In this section, we further leverage the observation for developing an index based  ltering scheme (qSign) to eliminate those sentences that are guaranteed not to produce a high reuse score with a given query sentence.
In particular, we propose a sentence-signature based mechanism for mapping from the sentence domain to a multidimensional space such that word-overlap searches can be reposed as range searches in this space.
Signature  les have been used in query processing in large text databases to screen out most unquali ed data [39].
In a signature  le, each word is assigned a  xed-width bit string, generated by a hash function.
File signature is then created by taking the bitwise-or of all signatures of words that appear in the  le.
Figure 3 shows the process of  le signature generation and querying: (a) the  le signature matches the signature is identical to the query signature; (b) the  le signature does not match the query if the bitwise-and results in a loss of bits.
Note that as shown in Figure 3(c),  le signatures may return false matches: signature comparison indicates a match, but in fact there is no keyword match between the  le and the query.
Therefore, query processing with  le signatures need three steps: (1) transforming of the query into a query signature, (2) searching for the query signature in  le signatures, and (3) elimination of the false matches.
Generally, the original signature schemes are implemented in three di erent ways: bitstring, bitslice and blocked signature  les [39].
In bitstring signature  les, each document is represented as a bitstring of the  xed-width, while in bitslice signature  les, the signature  les are decomposed into one  xed-width slice for each bit position.
On the other hand, in blocked signature  les, a sequence of text document is divided into several blocks containing the  xed number of distinct words and for each block, a bit mask of the  xed-width is assigned.
Since these index schemes are designed for a containment query, they are not applicable for reuse detection problem in which the goal is to discover matches with high word-overlaps.
Thus, in this paper, we propose qSign algorithm which exploits an e cient index scheme to support word overlap identi cation.
on Bit Differences Since signatures are randomized in nature, a signature-based index structure cannot always achieve 100% precision and recall.
Thus, the qSign indexing scheme presented in this section aims to maximize the recall rate,  rec, given a user supplied upper bound,  f p, on the rate of false positives.
In this subsection, we discuss how to compute an upper bound on bit di erences given an upper bound on false positive rate.
Let us be given a sentence, s1, composed of n words, and a corresponding signature, Sigs1 , of m-bits, where exactly l-bits are randomly set (l   m).
As described earlier, the sentence signature is formed by bitwise-or of the signatures of the words appearing in the sentence.
Thus, the probability of a given bit being set to 1 in this sentence signature can be computed as follows [39]:
 m )nl   1   e   nl m .
Let us now consider a second sentence, s2 which contains the same n words plus k additional words1.
Since the set of words of s1 is a subset of the set of words of s2, the bits set for the signature, Sigs2 , of the sentence s2 will also be a superset of the bits set to 1 in Sigs1 .
Some of the bits that are 0 in Sigs1 will, however, switch to 1 due to the additional k words.
The probability of a given bit switching from 0 to 1 due to the addition of these k new words can be computed as follows: Pbitswitch(m, l, n, k) = c(1   1 m )nl   (1   (1   1 m )kl)   e m   (1   e   nl   kl m ).
words can be modeled as combinations of  addition  and  deletion  operations.
reuse sentences Perform hash join to P f compute reuse score h h j t t i candidate







  .
Kj Kj F_Kj F Kj
 query q y







  .
Kn F_Kn



 Update sentence index Post-processing Post processing

 Search sentence-index and create a candidate hash table sentence-index (Doci, Senj)

    ..
 ..
candidate queue candidate queue Candidate-C did t selection ti l 2.a Create a query hash table and Create a query hash table and a sentence signature a sentence signature 2.c Perform Perform range-search range-search

 Extract sentences Extract sentences s2 sn  ..
sentence queue sentence queue s1   stream of stream of new entries new entries

 ..
..
sentence signature t t i Signature-index 2.b Map a signature Map a signature into a query point into a query point (1,0, .,1) Update
 2.d signature-index signature index Figure 4: An overview of the qSign reuse detection with the use of sentence signatures.
Given this, we can formulate the probability, Pexact bit dif f , that there will be exactly t bit di erences between signatures Sigs1 and Sigs2 due to these k words as follows: Pexact bit dif f (m, l, n, k, t) = (cid:2) (cid:3) m t Pbitswitch(m, l, n, k)t(1   Pbitswitch(m, l, n, k))m t.
Similarly, we can compute the probability, Pmax bit dif f , that there will be at most d bit di erences between signatures Sigs1 and Sigs2 due to these k words as (cid:1) Pmax bit dif f (m, l, n, k, d) = Pexact bit dif f (m, l, n, k, t).
0 t d Let us assume that the user allows up to k-words  exibility in the detection of word-overlaps between sentences.
Under this condition, s1 and s2 should be returned as a matching pair by the index structure with high probability.
In other words, under k-words  exibility, for the given values of m, l, n and k, and an acceptable false hit rate,  f p, we need to pick the largest bit-di erence value, d, such that Pmax bit dif f (m, l, n, k + 1, d)    f p.
  nl In other words, for any sentence with more than k additional words, the probability of being returned within d bit di erences will be at most  f p.
Note that Pmax bit dif f is small when e m is close to either 0 or 1 (i.e., when l is close to 0 or m is close to in nity).
Of course, arbitrarily increasing the number of bits used by the signature (that is, m) would negatively e ect the cost of range searches on the index structure (this is referred to as the dimensionality curse in multidimensional indexes [11,
 picks the number, l, of bits set to 1 in the signature low.
In particular, given a dictionary of w words, l is selected as the smallest value such that all the words in the dictionary can (cid:2) (cid:3) be uniquely represented; i.e., m l   w.
  Input : a query sentence, sq, and a distance, Output : a set of identi es of document and sentence, Os d Step 1 : Candidate-selection
 a hash table, Hsq for a sq.
d, and add (Doci, Senj) of results into candidate queue, Cs.
  Step 2 : Post-processing (Reuse Detection with Candidate Sentence)


 Search a sentence-index using key (Doci, Senj), extract a word and frequency list, sc, and create a hash table, Hsc .
Perform a hash join between Hsq and Hsc , and compute a similarity score by using word-overlap measure.
If score is greater than a threshold, store (Doci, Senj) into Os.
In Section 3.2, we will evaluate the impact of m, n and d on the recall as well as reuse detection time for blogs and news articles.
qSign: Supporting Range Searches based on Signature Bit Difference The sentence signature can be mapped into a point in a high dimensional space.
Let us be given a sentence s1 and a corresponding signature of m-bits, Sigs1 = vs1,1 vs1,2 ...vs1,m where the value of vs1,i is a zero or one.
This signature can be thought as a point, ps1 = (vs1,1 , vs1,2 , ..., vs1,m ), in m-dimensional space.
Then, a Euclidean distance between two points, ps1 and ps2 whose corresponding signatures are t bit di erent with each other, can be computed as a follow (cid:4)(cid:5)(cid:5)(cid:6) m(cid:1) Dist(ps1 , ps2 ) =   vs2,i )2 = (vs1,i   t.
i=1 Thus, given a d-bits upperbound on the di erence between the query signature and the sentence signatures in the database, the qSign algorithm identi es candidate sentences as those that lie within a Euclidean distance d. Figure 4 provides an overview of the proposed signature-based reuse detection process, based on the above signature and query range identi cation schemes:  
 candidate-selection step extracts sentences from the incoming entries and inserts them into a sentence queue.
(a) First a query word hash table and a sentence signature are created.
(b) The sentence signature is mapped into a query point in a high dimensional space.
Table 1: Data sets and reuse detection queries name #docs.
#sent.
average words/sent.
data Google 1000 Google 10000 query Google query Blog 100000 data query Blog query














 (c) Candidate sentences are identi ed (and inserted into a candidate queue) by searching points that lie within a search range.
Here, the appropriate search distance, d, is computed based on the equation described in Section 2.1.2.
  (d) The new entry is then inserted into the signature-index to support future lookups.
post-processing step identi es a word-and-frequency list using a sentence-index.
These are inserted into a candidate hash table for quick lookup.
date hash table are matched using a symmetric, non-blocking hash join.
index to support future lookups.
  Figure 5 presents the pseudo-code for the qSign algorithm.
Given a database of sentences (signature-index) and a query sentence, q, in the candidate-selection step, a set (Cs) of candidate sentences is identi ed by searching points that lie d from the point, psq , cor-within a Euclidean distance, responding to the signature of the query sentence (Step 1.1 - 1.3).
Then, a point, psq , is inserted into signature-index (Step 1.4).
If Cs is not empty, for each candidate sentence in Cs, we extract a word and frequency list indexed by the sentence-index and made a hash table, Hsc (Step 2.3).
Both Hsq and Hsc are used for performing a hash join between the query and the candidate sentences when computing similarity scores (Step 2.4).
Then, if the reuse score is greater than the given threshold value, the identi er of the document and the corresponding sentence is added to the list, Os, of matches (Step 2.5).
Finally, a sentence, sq, is inserted into sentence-index (Step 2.6).
Note that reuse detection is a incremental process in that whenever a new blog entry is provided, it is compared to entries in the data collection to  nd reuses.
Furthermore, qSign algorithm incrementally updates sentence-index and signature-index data structures to compute reuse score (Steps
 computation of reuse score as new blog entries are entered to system and avoids frequent rebuilding of index structures.
In this section, we experimentally evaluate the e ective-ness and e ciency of the signature-based reuse detection   scheme (qSign) discussed in Section 2.
We study the impact of the signature length (m) and the search range ( d) on the reuse detection time and we show that indexing-based  l-tering for reuse detection enables scalable operation of reuse detection schemes for large collections of blog and news entries.
Most importantly, we show that signature-based reuse detection does not cause a signi cant reduction in recall or precision.
First we describe the experimental setup and then we will discuss the results.
corresponds the Eucledian query range of   d).
level threshold( ) #reuse 0.9 <     1 0.8 <     0.9 0.7 <     0.8 0.6 <     0.7 0.5       0.6 sen.
total
 #matches per sen.
threshold( ) #reuse 0.9 <     1 0.8 <     0.9 0.7 <     0.8 0.6 <     0.7 0.5       0.6 sen.
level total #matches per sen.
Precision(cid:3)vs.(cid:3)Recall n o o i s i c e r












 d=0






 d=0






 d=1






 d=1






 16 bit signatures d=2






 d=3






 d=4






 d=5






 (a) Google 1000 16 bit signatures d=2






 d=3






 d=4






 d=5






 (b) Google 10000 Precision(cid:3)vs.(cid:3)Recall d=0






 d=0






 d=1






 d=1






 32-bit signature d=2






 d=3






 32-bit signature d=2






 d=3






 d=4






 d=4






 d=5






 d=5






 Precision(cid:3)vs.(cid:3)Recall n o i s s i c e r












 n o o i s i c e r


























 Recall






 Recall 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures (a) Google 1000 (b) Google 10000 Recall 16(cid:882)bits(cid:3)signature 32(cid:882)bits(cid:3)signature (c) Blog 100000 Figure 6: Precision/Recall graphs shows that 32-bit (with l = 2) outperforms 16-bit (with l = 1)
 We experimented with two di erent signature lengths (m):
 the mask for each word using hash function, MD5 [6] (l = 2).
To maintain a similar m/l ratio, for 16-bit signatures, only one bit was set (l = 1).
Note that with 16-bit signatures and l = 1, there is signi cant overlap in the keyword signatures: there are only 16 unique keyword signatures.
For disk-based multidimensional indexing of the signatures, we used a Hybrid-tree [16].
For reuse detection, we used word overlap measure and set the word-overlap threshold,  , to
 machine with 512M of RAM.
In this section, we report two kinds of experiments: non-incremental and incremental update experiments.
In non-incremental setting, the data collection is pre-processed and indexed o line and is not updated in the course of reuse detection.
On the other hand, in incremental update experiments, whenever a query blog entry is provided, it is compared to entries in the data collection to  nd reuses, and then used to incrementally update the data collection to support further lookups.
For non-incremental update experiments, we used three data sets: Google data (blogs [2] and news [3]) were crawled from April 10th to April 18th, 2007 from diverse categories, such as politics, business, science and sport.
We randomly selected 1000 and 10000 news articles and blog entries from the crawled data.
We also selected 100000 entries from the Blog data from the benchmark collection distributed in [7].
As reuse detection queries, we randomly selected 30 entries from the Google data set and 20 entries from data distributed in [7].
The duplicates we are reporting are the sentence duplicates between query documents and data set.
Table 1 summarizes the number of sentences and keywords for the data sets and queries for reuse detection used in our experiments.
The experiments in this subsection are divided into two parts.
First, we measure the precision and recall of the proposed method to quantify the e ectiveness of signature-based  ltering.
Then, we evaluate the scalability of the approach.
Table 2 illustrates the impact of signature-based  ltering in reuse detection.
Figures 6 summarizes the precision versus recall behavior for the results presented in Table 2.
The ground truth (#reuse sen. column of Table 2) for these experiments is obtained by a naive approach which compares query sentences with all sentences in the database.
In this table, d represents d bits di erence with the query signature (and corresponds the Eucledian query range of d).
The table includes d values upto 5 since this value is su -cient for high recall.
For this experiment, various levels of word-overlap threshold ( ) are used for reuse detection.
The observations based on Table 2 can be summarized as follows:   Predicted(cid:3)vs.(cid:3)Observed(cid:3)Recall(cid:3)(Google_1000) l l a c e











 Observed(cid:3)(0.8(cid:3)(cid:882) 0.9) Predicted(cid:3)(0.8) Observed(cid:3)(0.7(cid:3)(cid:882) 0.8) Predicted(cid:3)(0.7) Observed(cid:3)(0.6(cid:3)(cid:882) 0.7) Predicted(cid:3)(0.6) Observed(cid:3)(0.5(cid:3)(cid:882) 0.6) l l a a c e













 Observed(cid:3)(0.8(cid:3)(cid:882) 0.9) Predicted(cid:3)(0.8) Observed(cid:3)(0.7(cid:3)(cid:882) 0.8) Predicted(cid:3)(0.7) Observed(cid:3)(0.6(cid:3)(cid:882) 0.7) Predicted(cid:3)(0.6) Observed(cid:3)(0.5(cid:3)(cid:882) 0.6) d=1 d=2 d=3 d=4 d=5 Bit(cid:3)difference(cid:3)(d) (a) 16 bit signatures d=1 d=2 d=3 d=4 d=5 Bit difference (d) Bit(cid:3)difference(cid:3)(d) (b) 32 bit signatures Figure 7: Predicted values using Pexact bit dif f lie between observed recall values.
For example, when the word-overlap threshold is set to 0.7, predicted values lie between observed recall values of 0.6 <     0.7 and 0.7 <     0.8. e m m i t (cid:3) n u r (cid:3) (cid:3) e v i t a e
 l





 Relative(cid:3)run(cid:3)time(cid:3)vs.(cid:3)Recall




 Relative(cid:3)run(cid:3)time(cid:3)vs.(cid:3)Recall






 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index e m m i t (cid:3) n u r (cid:3) (cid:3) e v i t a l e




 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index Recall (b) Blog 100000 Recall (a) Google 10000 Figure 8: Comparison of run times for relative to naive full scan   The  rst thing to note in this table is that there exist a great deal of reuse in the high word-overlap threshold level (0.9 <     1).
This is because in many cases news articles and blog entries contain a large number of exact quotations.
  This table also lists the number of candidate sentences found matching the query sentence (#matches per sen.
row).
On a 32-bit signature (with l = 2), the number of candidate matches shows a relatively stable growth as the search radius increases.
However, for 16-bit signatures (with l = 1), the number of candidate matches grows almost exponentially as the search radius increases.
This is mainly due to the negative e ect of word signature overlaps (due to the existence of only a small number of unique word signatures).
Precision versus recall plots in Figure 6 verify that 32-bit signatures (with l=2) achieve better precision than 16-bit signatures (with l=1).
These results also indicate that the performance gap between 16- and 32-bit signatures sharply increases with increasing co-indexed documents.
This is because on the 32-bit scheme, the number of candidate matches grows stably as the number of co-indexed documents increases, while on 16-bit scheme, the number of candidate matches shows an exponential growth with increasing co-indexed documents.
Furthermore, Figure 6 shows that using 32-bit signatures it is possible to achieve a high reuse detec- tion rate (  75%) with almost perfect precision.
The recall rate can be further improved by relaxing the radius of the query range, but this has a drastic impact on the precision, thus may require costly post-processing to eliminate false matches.
In Section 3.2.2, we study the query processing time for reuse detection in more detail.
Predicted vs.
Observed Recall: Before we proceed to the experiments for reuse detection performance, we further verify that the Pexact bit dif f in Section 2.1.2 can indeed be used to predict the search radius.
Figure 7 shows the Pexact bit dif f based recall prediction when the word-overlap threshold ( ) is set to 0.7, 0.8, or 0.9 for 32-bit signatures (m = 32 and l = 2) as well as 16-bit signatures (m = 16 and l = 1).
The  gure also includes the observed values (Table 2(a)) for the cases of 0.5 <     0.6, 0.6 <     0.7,
 to 5.
In Figure 7, the solid lines are predicted values based on the equation presented in Section 2.1.2 each for a  xed threshold,  .
The dashed lines correspond to average recall for di erent ranges of   values.
We expect that each pair of two consecutive dashed lines will behave as upper and lower-bounds for a solid line.
As expected, the curves corresponding to the observations are lower and upper-bounds of the predicted curve.
For example, the lines of 0.6 <     0.7 and 0.7 <     0.8 are respectively lower and upper-bounds of the predicted recall when the word-overlap threshold ( ) is set to 0.7.
The shapes of the observed curves match the shape of the predicted curve well.
In this section, we test the scalability of qSign on real data sets.
In these experiments, we used data sets of 10,000 and 100,000 entries, as in most systems, the blog entries are pre-clustered based on recency or based on high-level topic analysis.
We estimate that 100,000 blog entries is a reasonable collection size for reuse-detection.
Note, however, that there may be multiple such clusters, rendering in memory-based solutions to reuse-detection highly impractical.
Post(cid:882)processing(cid:3)time(cid:3)vs.(cid:3)Recall Total(cid:3)time(cid:3)vs.(cid:3)Recall ) c e s m m ( (cid:3) e m
 i








 ) c e s m m ( (cid:3) e m
 i








 ) c e s m m ( (cid:3) e m
 i












 Recall













 Recall Recall 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index (a) Candidate-selection of Google 10000 (b) Post-processing time of Google 10000 (c) Total time of Google 10000 Candidate(cid:882)selection(cid:3)time(cid:3)vs.(cid:3)Recall Post(cid:882)processing(cid:3)time(cid:3)vs.(cid:3)Recall Total(cid:3)time(cid:3)vs.(cid:3)Recall











 ) c e s m m ( (cid:3) e m
 i





 ) c e s m m ( (cid:3) e m
 i ) c e s m m ( (cid:3) e m
 i




 Recall






 Recall






 Recall

 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index 16(cid:882)bit(cid:3)signatures 32(cid:882)bit(cid:3)signatures Inverted(cid:882)index (d) Candidate-selection of Blog 100000 (e) Post-processing time of Blog 100000 (f) Total time of Blog 100000 Figure 9: Comparison of (a,d) candidate-selection times, (b,e) post-processing times, and (c,f ) total times for 10,000 and 100,000 entries In this subsection, we compare our qSign algorithm with an inverted-index based scheme used in duplicate detection.
As stated in Section 1.1, [10] is an inverted-index based algorithm and  nds all pairs of documents in a collection, where similarity score is above a given threshold.
For our evaluations, we adapted the underlying inverted-index based technique in [10] into sentence-based reuse detection problem:
 dividual sentences in the documents.
set to identify all matching pairs.
In this set of experiments, however, (while the underlying index structures and matching algorithm are the same as the original one), we only need to iterate over 30 entries for Google data and 20 entries for the Blog data, and match them to the complete data set.
threshold and post-processing needs to identify actual reuses among candidate sentences.
Note that in experiments, we varied similarity threshold in [10] such that the recall ranges obtained by the inverted-index based technique are similar to those by qSign.
Figure 8 plot the index-based reuse detection time as a ratio of a naive reuse detection time approach (which would scan the entire database for detecting reuse): the relative run time = run time of f iltering approach run time of naive approach .
The relative run time being less than 1 means that the  ltering-based approach outperforms the naive method.
Note that the run time for reuse detection can be split into two individual steps of the algorithms in Subsection 2.1.3:   candidate-selection which includes the time to create the signature for each sentence, to perform range search on a multidimensional index to generate a set of candidate reuse matches, and to incrementally update a multidimensional index, and   post-processing step which includes the execution times to perform a hash-join based post-processing to compute a reuse score and to update sentence-index incrementally.
Figure 9 shows the way that the execution time is split between candidate-selection and post-processing step.
Note that data in this section are obtained by the disk-based implementation of both qSign and the underlying inverted-index based technique in [10].
The observations from Figure 8 and 9 can be summarized as follows:   Depending on the permissible recall rate, qSign sig-ni cantly improves the reuse detection performance.
The processing time gains against a naive solution can be 100X to 1000X, with recall rates of upto 70% (Figure 8).
  Furthermore, at all recall rates, qSign outperforms the inverted-index based scheme (Figure 8 and 9 (c,f)).
As can be seen in Figure 9 (a,d), qSign signi cantly outperforms the inverted-index based scheme in candidate-selection step.
This is because in the candidate-selection, qSign leverages an e cient index scheme, based on range searches in a multidimensional space, for pruning sentences whose similarity scores are less than a given threshold with high probability.
On the other hand, in post-processing step, the inverted-index based algorithm shows a slightly better performance than qSign (Figure 9 (b,e)).
The major performance gain of qSign algorithm is achieved by the e cient scheme for

 ) ) c e s m ( (cid:3) e m
 i


 Recall vs. Time Recall(cid:3)vs.(cid:3)Time






 Recall Google 1000 Google_1000 Google 10000 Google_10000 Blog 100000 Blog_100000







 ) c e s ( (cid:3) (cid:3) e m
 i

 y(cid:3)=(cid:3)1E(cid:882)08x2 (cid:882) 0.004x(cid:3)+(cid:3)570.9 y(cid:3)=(cid:3)3E(cid:882)09x2 +(cid:3)0.001x(cid:3)(cid:882) 122.4 y(cid:3)=(cid:3)4E(cid:882)09x2 (cid:882) 0.001x(cid:3)+(cid:3)272.4 y(cid:3)=(cid:3)2E(cid:882)10x2 +(cid:3)0.000x(cid:3)(cid:882) 21.55




 #(cid:3)(cid:3)New(cid:3)sentences(cid:3) 32(cid:882)bit(cid:3)signatures,(cid:3)d=1 16(cid:882)bit(cid:3)signatures,(cid:3)d=1 Poly.
(cid:3)(32(cid:882)bit(cid:3)signatures,(cid:3)d=1) Poly (16(cid:882)bit signatures d=1) Poly.
(cid:3)(16 bit(cid:3)signatures,(cid:3)d 1) 32(cid:882)bit(cid:3)signatures,(cid:3)d=2 16(cid:882)bit(cid:3)signatures,(cid:3)d=2 Poly.
(cid:3)(32(cid:882)bit(cid:3)signatures,(cid:3)d=2) Poly (16(cid:882)bit signatures d=2) Poly.
(cid:3)(16 bit(cid:3)signatures,(cid:3)d 2) Figure 10: Recall versus run time (32-bit signatures) identifying candidate sentences using the proposed index structure.
Especially, on a 32-bit signature, qSign signi cantly outperforms the inverted-index based algorithm in candidate-selection, while qSign and the inverted-index based algorithm show a similar performance in post-processing.
  When lower recall rates are acceptable, the 16-bit signature-based  ltering can be slightly faster than 32-bits.
Note that when the data set is small and a recall rate is low, the number of candidate matches between 16- and 32-bit signatures is almost similar to each other (as shown in Table 2), which causes candidate-selection step to be a major contribution on the performance gap between 16- and 32-bit signature-based  ltering.
On the other hand, when high recall is required, the 32-bit signature-based  ltering outperforms the 16-bit scheme.
The reason for this is that the number of candidate matches that need to be post-processed on the 16-bit scheme grows almost exponentially (as shown in Table 2), which results in signi cant amounts of false matches to be eliminated during post-processing.
Figure 10 plots the average reuse detection time per query sentence.
As can be shown in this  gure, given a query sentence, it takes less than 1 second to detect reuse sentences in 100,000 co-indexed documents by using qSign.
For incremental update experiments, we collected 1 million sentences from the Blog data [7], treated the data as a sentence stream, and found reuses for each sentence in this stream.
E ect of candidate-selection and post-processing on execution times: The next set of experiments is about the e ect of candidate-selection and post-processing on execution times.
Figure 11 shows how the execution times for reuse detection increase as the number of sentences inserted into the database increases.
Notice that Figure 11 plots the case where the bit di erence between signatures, d, is set to 1 and 2 (and corresponds the search range of 1 and 2) and 16-bit signatures and 32-bit signatures are used.
  As shown in  gure, the execution times increase with both the number of sentences and the search radius used in performing range search on a multidimensional index.
For a given search radius is used, the 32-bit scheme outperforms
 ure 11 and can see that the execution times are quadratic in the number of sentences that are being compared.
Figure 11: The performance of reuse detection: the execution times of qSign are quadratic.
Figure 12 shows the way that the execution time is split between candidate-selection and post-processing steps.
The observations based on Figure 12 can be summarized as follows:  In the case of the 16-bit scheme, when the number of sentences to be inserted increases, the major contributor to the corresponding increase in the execution times is the post-processing step.
This is expected as for 16-bit signatures, the number of false matches that need to be eliminated during post-processing grows quickly as the number of sentences increases.
  For the 32-bit scheme, we see that while the execution time of post-processing increases, the major contributor to the increase in overall execution times is due to candidate-selection step.
This is because a large search distance greatly e ects the cost of range searches using the multidimensional index structures.
Performance evaluation of incremental processing: qSign is incremental in that whenever a new blog entry is available, it is compared to entries in the collection to  nd reuses and then is used to update the collection.
In this experiment, we vary the number of sentences originally stored in database from 500K to 800K, while the number of new sentences is  xed as 100K.
Figure 13 plots the case where the bit di erence between signatures, d, is set to 1.
For comparison purposes, we also implemented a non-incremental reuse detection scheme (i.e., full re-computation).
In the non-incremental reuse detection case, when a collection of new sentences is generated, we fully reprocess all sentences in databases and queries to detect reuses, while in the incremental reuse detection case, new sentences are processed to identify reuses and to update the existing database.
As shown in this  gure, the time gain of the incremental processing is up to 5x and 7x for 16-bit and 32-bit scheme respectively.
Furthermore, the performance gaps between incremental and full re-computation reuse detection schemes get larger as the number of sentences already in the databases increases.
In this paper, we observed that frequent content reuse is very common in blogs and news entries outlets tracking the same real-world event.
In order to enable scalable operation of reuse-detection we presented an e cient and incremental  ltering-based qSign algorithm.
We also investigated an e ective scheme to compute the appropriate search distance given an acceptable recall rate.
Our experiments show



 #(cid:3)New(cid:3)sentences









 # New sentences #(cid:3)New(cid:3)sentences



 ) c c e s 6000e
 ( e m

 i



 ) c c e s 1500e ( e m

 i





 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits 16 bit 16(cid:882)bits 32 bit 32(cid:882)bits d(cid:3)=1 Post(cid:882)processing Candidate(cid:882)selection (a) Bit di erence (d = 1) d 2d(cid:3)=2 Post(cid:882)processing i Candidate(cid:882)selection did l i (b) Bit di erence (d = 2) Figure 12: The way that the execution time is split between candidate-selection and post-processing steps.
) c e s s ( (cid:3) e m
 i

 Incremental computation vs full recomputation Incremental(cid:3)(cid:3)computation(cid:3)vs.(cid:3)(cid:3)full(cid:3)recomputation(cid:3) (100K(cid:3)(cid:3)new(cid:3)sentences)
 #(cid:3)Sentences(cid:3)(cid:3)originally(cid:3)in(cid:3)the(cid:3)(cid:3)DB

 Full(cid:3)re(cid:882)computation(cid:3)(m=32,(cid:3)l=2,(cid:3)d=1) Full(cid:3)re(cid:882)computation(cid:3)(m=16,(cid:3)l=1,(cid:3)d=1) incremental(cid:3)(m=32,(cid:3)l=2,(cid:3)d=1) incremental(cid:3)(m=16,(cid:3)l=1,(cid:3)d=1) Figure 13: The performance comparison between incremental processing and full re-computation for reuse detection.
that qSign algorithm signi cantly improves the reuse detection speed e ciency without causing signi cant reductions in recall and precision.
Furthermore, we veri ed that the Pexact bit dif f based recall prediction can be e ectively used to predict the search radius through experiments.
Future work will include investigation of reuse detection techniques for even larger data sets.
