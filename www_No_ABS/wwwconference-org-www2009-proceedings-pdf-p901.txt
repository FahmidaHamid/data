Industrial leaders such as IBM, Microsoft, Oracle, and BEA promote the use of service-oriented business processes to build their enterprise applications.
Process engineers may develop such applications using orchestration languages like the Web Service for Business Process Execution Language (WS-BPEL) [22] and Business Process Modeling Language (BPML).
In a typical application, a business workflow (say, coded in BPEL) may use external web services to implement individual workflow steps.
To transfer type-safe XML messages [22] among individual workflow (+852) 2788 9684.
Fax: * This research is supported in part by GRF grants of the Research Grants Council of Hong Kong (project nos.
111107, 123207, 717308, and 717506).
 All correspondence should be addressed to Dr. W. K. Chan at Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong.
Tel: (+852) 2788 8614.
Email: wkchan@cs.cityu.edu.hk.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
T. H. Tse The University of Hong Kong Pokfulam Hong Kong thtse@cs.hku.hk wkchan@cs.cityu.edu.hk steps and web services, process engineers write diverse specifications in Web Service Description Language (WSDL) [23] (dubbed WSDL specifications) to interpret different portions of the same or different XML documents for various workflow steps.
Since a workflow step may use part of the content kept in an XML document, process engineers may define XPath expressions [25], which pairs with WSDL specifications, to extract the required contents from the document.
To cope with changing business requirements, process engineers may modify the service-oriented business process [11][14][26].
Testers should assure the quality of such revised applications.
Regression testing, aimed at detecting potential faults caused by software changes, is the de facto approach [8][20].
It reruns test cases from existing test suites to ensure that no previously working function has failed as a result of the modification [8].
Although many researchers point out that frequent executions of regression test are crucial in successful application development [8][15], rerunning the regression test suite for large and complex systems may take days and even weeks, which is time-consuming.
In service-oriented computing, a business process may invoke external web services (such as viewing an article in Economist.com), which may incur charges.
To reduce costs, it is desirable to detect failures as soon as possible when executing the test suite.
The use of effective regression testing techniques is, therefore, crucial.
Thus, test case prioritization [19] is important in regression testing [9][15].
It schedules the test cases in a regression test suite with a view to maximizing certain objectives (such as revealing faults earlier), which help reduce the time and cost required to maintain service-oriented business applications.
Existing regression testing techniques for such applications focus on testing individual services [20] or workflow programs [6].
Surprisingly, to the best of our knowledge, the integration complexity raised by non-imperative artifacts such as XPath and WSDL among workflow steps has been inadequately addressed in regression testing research.
Let us consider a simple example.
Suppose an application aims to implement an XPath query to select (from a list of available hotel rooms kept in an XML document) all  single rooms  priced less than $100.
Suppose also that the XPath expression has been implemented erroneously as selecting either  single rooms  or rooms priced less than $100.
Using this incorrect XPath query, the application may select a  single room  priced at $100 or above.
In general, an XPath query in a workflow step may introduce additional (conceptual) branch decisions (such as deciding whether a room can be selected), and thus affect the workflow logic.
Furthermore, different XML messages that conform to the same WSDL specification may contain different sets of XML elements (including tags and attribute names).
We refer to an XML element defined by at least one XML schema in a WSDL specification as a WSDL element.
Incorrectly defining a WSDL element or failing to provide a definition may result in an integration error.
and WSDL) in a service-oriented business application.
To the best of our knowledge, however, prioritization techniques to effectively find test cases to reveal such implementation problems earlier during maintenance has not been studied.
This paper tackles the problem.
Following our previous work [12], we model an XPath query (in the presence of a WSDL specification) as an XPath Rewriting Graph (XRG).
An XRG represents potential scenarios of content selections from XML messages.
Each content selection scenario is captured as an XRG branch (see Section 2.2.2).
We note that XRG branches for different XML messages that the XPath expression is querying on may be different.
To account for the WSDL artifact, we say that a test case t has covered a WSDL element e if t contains an XML message m as input, or t causes the application to generate an XML message m, such that m has e as its entity tag.
In a changing business application, every artifact (workflow, XPath, or WSDL) may be modified.
As a result, fault(s) may be introduced to the artifacts.
The use of workflow coverage data to prioritize test cases may be effective for detecting faults in the workflow program, such as wrong predicates.
However, such prioritizations may be ineffective for handling faults in other artifacts.
More examples will be given in Section 3.
We propose a multilevel coverage model to capture the coverage requirements of these artifacts.
Level 1 covers only the workflow, level 2 covers both workflow and XPath, and level 3 covers workflow, XPath, and WSDL.
Through the level-by-level use of coverage data for test cases, we propose a new family of test case prioritization techniques.
To handle multiple types of artifact in the family of test case prioritization techniques, we use two strategies.
The first strategy is to treat different artifacts homogenously, which is akin to enlarging the coverage space from pure workflow-oriented coverage space to a space linked up to the coverage space of other artifact types.
We call it a summation strategy.
On the other hand, we appreciate that such a homogenous treatment of artifacts may not reflect the different roles of these artifacts in a service-oriented business application.
For instance, from the perspective of process engineers who write such applications, a workflow program is more important than XPath expressions or WSDL specifications.
Therefore, we propose another strategy called a refinement strategy.
This strategy would refer to another type of artifact (such as WSDL) only if using the artifacts already referred to (such as workflow and XPath) cannot help a prioritization technique to select a test case.
We develop a family of techniques using the above model and strategies.
With the inclusion of more artifacts, our techniques can intuitively be more effective in detecting faults residing across various artifacts.
Our experiment further shows that the family of techniques is effective to reveal regression faults in modified programs, and the techniques at a higher level is generally more effective than those at a lower level.
(including workflow, XPath, The main contribution of this paper is threefold.
(i) Through a multilevel coverage model, we propose a family of test case prioritization techniques that consider imperative and non-imperative artifacts in service-oriented business applications.
(ii) We analyze the proposed prioritization techniques and present a hierarchy to capture their relations.
To our best knowledge, this is the first logical hierarchy to relate test case prioritization techniques in the literature.
(iii) We report an experimental study to verify the effectiveness of our proposal.
and WSDL) The rest of the paper is organized as follows: Section 2 gives the preliminaries.
Section 3 shows a motivating example to discuss the challenges.
Section 4 presents our prioritization techniques.
Section 5 presents an experiment to validate our proposal, followed by discussions and related work in Sections 6 and 7, respectively.
Finally, Section 8 concludes the paper.
Test case prioritization [5][19] is an important kind of regression testing technique [9][18].
With the information gained in the previous software evaluation, we may design techniques to run the test cases to achieve a certain goal in the regression testing.
For example, proper test case prioritization techniques increase the fault detection rate of a test suite and the chance of executing test cases with higher rates of fault detection earlier [5].
We adopt the test case permutation problem from [19] as follows: Given: T, a test suite; PT, the set of permutations of T; and f, a function from PT to real numbers.
(For example, f may calculate the fault detection rate of a permutation of T.) Problem: To find T PT such that,  T PT, T    T    f (T )   f (T ).
The metric of Average Percentage of Faults Detected (APFD) [5] is widely adopted in evaluating test case prioritization techniques [6][19].
A higher APFD value indicates faster (or better) fault detection rate [5].
Let T be a test suite containing n test cases, F be a set of m faults revealed by T, and TFi be the first test case index in ordering T  of T that reveals fault i.
The following equation gives the APFD value for ordering T  [5].
+


 m  =

 ++
 ...
mn +
 n
 We provide an example to show how APFD measures the fault detection rate of different test suite ordering.
Fault f4 f5 Test Case f2 f1   f3   Example on test suite and faults exposed             tA tB tC tD tE f8     f6 f7    
 s t l u a
 d e t c e t e
 t n e c r e












 s t l u a
 d e t c e t e
 t n e c r e















 Test Suite Fraction





 Test Suite Fraction
 (a) APFD for test suite T1 (b) APFD for test suite T2 Figure 1.
Example illustrating the APFD measure.
A program may have multiple faults.
A test case sometimes can detect zero, one, or more faults, however, it can hardly find out all faults.
Suppose the faults that test cases tA to tE can detect are shown in Figure 1.
Let the two permutations for tA to tE be T1  tB, tA, tD, tC, tE  and T2  tC, tD, tE, tA, tB .
The APFD measures on T1 and T2 are also given in Figure 1.
Other metrics [9] can also be used to measure these techniques.
Owing to space limit, we will report such results in future work.
We adopt the definition of XPath expression in [13].
An XPath expression is defined using the following grammar: q   n |.|*| qqq / | // qqq ][ | The operators include the following: n     is any label, * denotes a label wildcard, and .
(the dot operator) denotes the current node.
The constructions / and // mean child and descendant navigations, while {y | (x, y)  EDGES(t)} {z | y q1(x), z q2(y)} {z|y q1(x), (y, u) EDGES*(t), z q2(u)} {y | y q1(x), q2(y) } {x} = = = = = = Rule            





 n(x) *(x) .
(x) (q1/q2)(x) (q1//q2)(x) (q1[q2])(x) the square brackets [ ] enclose a predicate.
The symbols in   represent the element labels and attribute labels that can occur in XML documents.
The set of all trees are denoted by T , and each tree represents an XML document satisfying the XML schema  .
We also use   to represent the set of labels that can occur in the XML schema  .
For a tree t   T , an XPath query q(t) is a query on t using an XPath expression q, and returns a set of nodes of t. NODES (t) and EDGES(t) denote the sets of nodes and edges, respectively.
LABEL(x) is the label on node x, LABEL (x)    .
The transitive closure of + EDGES(t) is denoted by EDGES (t), and the reflexive and transitive closure of EDGES(t) is denoted by EDGES*(t).
Reference [13] gives the following definitions to represent a decidable fragment of XPath in Figure 2.
According to [13], this fragment has provided representative XPath syntaxes and is sufficient to be the basis of studying XPath.
Figure 2.
Syntax of a decidable fragment of XPath [13].
XPath queries are used to locate contents from an XML document.
We have proposed in [12] an XPath Rewriting Graph (XRG) to represent an XPath with a document model   of XML documents.
We revisit XRG here to facilitate the description of our techniques.
An XRG is built on the definitions of XPath syntactic constructs [13].
We treat these definitions (Figure 2) as  left-to-right  rewriting rules, and through a series of rewriting [3], transform an XPath into a normal form or a fixed point.
An XRG also records the intermediate rewriting steps, and links every two consecutive steps in the graph.
To capture the notion of rewriting [3], there are two types of node in an XRG, namely rewriting node  q, Lc, rule  and rewritten node  q, Lc, Ln, S .
q is an XPath expression.
Lc and Ln are sets of nodes (Lc, Ln   NODES( )).
They represent the sets of tags relevant to q. Lc is the set of nodes located by the previous rewriting step.
Ln is the set of nodes that can be located by q starting from some node in Lc.
S is a set-theoretic representation of the result of q.
Besides, rule denotes the rewriting rule used to generate the sub-terms in this node.
Initially, Lc is assigned to {ROOT}, where ROOT is the unique root node of  .
<xsd:element name="roomno" type="xsd:int" /> <xsd:element name="price" type="xsd:int"/> <xsd:element name="persons" type="xsd:int"/> <xsd:element name="name" type="xsd:string"/> <xsd:element name="room" type="xsd:RoomType"/> <xsd:element name="error" type="xsd:string"/>









 Figure 3.
Part of WSDL document: XML schema of hotel.
Let us show an example of an XRG.
Suppose, during the reservation of a hotel room (see the example in Section 3), the booking information (in XML format) is kept in a BPEL variable HotelInformation.
Figure 3 shows a simplified XML schema hotel for HotelInformation.
(We have omitted relevant details from the schema to ease the discussion of the example in Section 3.)
A room has three attributes (lines 7 9): roomno, price, and persons (indicating the maximum number of persons allowed).
Consider an XPath query on HotelInformation, denoted by XQ(HotelInformation, q), where q is //room[@price Price  and @persons= Num ]/price/.
Informally, q finds a room within the requested price that can accommodate the requested number of persons.
The corresponding XRG is shown in Figure 4.
and q2 = We use the algorithm Compute_XRG from [12] to construct XRGs.
We show the first rewriting step to illustrate how an XRG is computed.
XQ (HotelInformation, q) is first identified by Rule 5 (q1 = * room[precondition]/price/*), where precondition is  @price Price  and @persons= Num   .
Rewriting node R1 is thus generated.
Next, the algorithm recursively processes three sub-terms: //, q1, and q2.
The middle sub-term // matches Rule 5 (note that // is the same as .//.
), and so R3 is generated.
The left sub-term * matches Rule 2, and hence rewritten node R2 is generated.
The right sub-term q2 matches Rule 4, and rewriting node R4 is generated.
The remaining rewriting steps are similar.
Rewriting Node Rewritten Node XQ(HotelInformation, //room[precondition]/price/)
 < //price/, A={ROOT},(q1//q2)> q1(A), q1= *
 < *, A, B, Rule2>
 < //, B, C, Rule5> q2(C), q2=(room[precondition]/price/*)
 <room[precondition]/price/*, C,(q3/q4)> q3(C),q3=room[precondition])
 <room[precondition], C,(q5[q6])>
 q4(E),q4= price/* <price/*, E,(q7/q8)> q5(C),q5=(precondition)
 <precondition, C, D, Rule 1> q6(D), q6=room
 < room, D, E, Rule 1> q7(E),q7=(price)
 < price, E, F, Rule1> q8(F), q8=*
 < *, F, G, Rule2> precondition: (@price Price  and @persons= Num ) A = {ROOT} B = {hotel} C = {name, room, roomno, price, persons, error} D = {room} E = {room} F = {price} G = {g | g is the price value} Rule 1: n(x) = {y|(x, y) EDGES(t), LABEL(y) = n} Rule 2: *(x) = {y|(x, y) EDGES (t)} Rule 4: (q1/q2)(x) = {z| y q1(x), z q2(y)} Rule 5: (q1//q2)(x) = {z|y q1(x), (y, u) EDGES*(t), z q2(u) } Rule 6: (q1[q2])(x) ={y| y q1(x), q2(y)     } Figure 4.
Example of XPath Rewriting Graph (XRG).
Following [12], we can obtain a conceptual path that models a logical computation of an XPath via an inorder traversal of the XRG with all the rewriting nodes dropped (as illustrated in Figure 8).
Such a path contains implicit predicates that decide on the legitimate branch (called XRG branch) to be taken.
For example, if no element in the XML document can be selected for the set B in R2, B would be empty.
This will result in no more applicable rewriting.
A succeeding rewritten node will be appeared on a conceptual path only if its preceding rewritten node provides a nonempty set of Lc.
Therefore, a branch can be modeled by whether Lc on a node is empty or not.
We adapt the business process HotelBooking from the TripHandling project [21] to introduce the challenges in a typical service-oriented business application.
HotelBooking offers the hotel booking service.
Since showing the actual BPEL code in XML format is quite lengthy, we follow [12] to use an UML activity diagram to depict this business process to ease the illustration (Figure
 failures in Figures 5(b) and 5(c).
We use a node to represent a workflow node, and a link to represent a transition between two activities.
We also annotate the nodes with information extracted from the program, such as the input-output parameters of the activities and XPath.
The nodes are numbered as Ai (for i from 1 to 8) to ease the illustration.
The process HotelBooking in Figure 5(a) is described as follows: (a) A1 receives a user s hotel booking request, and stores it in the variable BookRequest.
Input: BookRequest A1: Receive HotelBookReqest Input: BookRequest A1: Receive HotelBookReqest Input: BookRequest A2: Assign Price Price= XQ(BookRequest, //price/) Num= XQ(BookRequest, //persons/) A2: Assign Price Price= XQ(BookRequest, //price/) Num= XQ(BookRequest, //persons/) A2: Assign Price Price= XQ(BookRequest, //price/) Num= XQ(BookRequest, //persons/) A3: Invoke HotelPriceService Input: Price Output: HotelInformation A3: Invoke HotelPriceService Input: Price Output: HotelInformation A3: Invoke HotelPriceService Input: Price Output: HotelInformation RoomPrice = XQ(HotelInformation, //room[@price Price  or @persons Num ]/price) A4: Assign RoomPrice RoomPrice = XQ(HotelInformation, //room[@price Price  and persons Num ]/price) A4: Assign RoomPrice RoomPrice = XQ(HotelInformation, //room[@price Price  and @persons= Num ]/price)
 Validate Price Yes No if RoomPrice   0 && RoomPrice   Price A7: Invoke HotelBookService A6: Fault Handling A8: Reply BookingResult Input: RoomPrice Output: BookingResult No A6: Fault Handling A4: Assign RoomPrice
 Validate Price if XQ(HotelInformation, //roomno/)   null Yes && RoomPrice   0 && RoomPrice   Price Input: RoomPrice Output: BookingResult A7: Invoke HotelBookService A8: Reply BookingResult No A6: Fault Handling if XQ(HotelInformation, //roomno/)   null && RoomPrice   Price
 Validate Price Yes A7: Invoke HotelBookService A8: Reply BookingResult Input: RoomPrice Output: BookingResult (a) Original Program (b) Changed Program - 1 (c) Changed Program - 2 Figure 5.
Activity diagram of a WS-BPEL application.
(b) A2 extracts the inputted room price and number of persons via XPath //price/ and //persons/ from BookRequest, and stores them in the variables Price and Num, respectively.
(c) A3 invokes the service HotelPriceService to find available hotel rooms with prices within budget (not exceeding Price), and keeps the result in HotelInformation (defined in Figure 3).
(d) A4 assigns RoomPrice using the price extracted via the XPath //room[@price Price  and @persons= Num ]/price/.
(e) A5 further verifies locally that the price in HotelInformation should not exceed the inputted price (the variable Price).
(f) If the verification passes, A7 will execute HotelBookService to book a room, and A8 returns the result to the customer.
(g) If RoomPrice is erroneous or HotelBookService (A7) produces a failure, A6 will invoke a fault handler, i.e.,  A7, A6  is executed.
For ease of understanding, we summarize the artifacts and their relationships in UML class diagram notation (as shown in Figure 6).
The description has been given in Section 1.
SO business application Workflow
 see example in Figure 3 see example in Figure 5 see example in Figure 7 XPath (XRG) XML messages see example in Figure 4 web services Level-1: Workflow Level-2: Workflow, XPath Level-3: Workflow, XPath,
 Figure 6.
Key artifacts and their relationships in typical service-oriented business application.
Suppose a process engineer Rick decides that the precondition at node A4 in Figure 5(a) should be changed to that at node A4 in Figure
 provide accommodation for the requested number of persons.)
However, he wrongly changes the precondition in the XPath (namely, changing  and  to  or ).
While he intends to provide customers more choices, the process does not support his intention (for instance, the process is designed to immediately proceed to book rooms, rather than providing choices for customers to select).
Further, suppose another engineer May wants to correct this fault.
She plans to change node A4 in Figure 5(b) back to that in Figure 5(a).
However, she considers that the precondition at node A5 is redundant (i.e., no need to require RoomPrice   0).
Therefore, she changes the node A5 in Figure 5(b) to become the node A5 in Figure 5(c), and forgets to handle a potential scenario (Price < 0).
Her change thus introduces a regression fault into the original program.
We use a set of test cases (t1 to t6) to illustrate the challenges in test case prioritization.
The inputs to WS-BPEL applications are XML documents.
We simply use the price value of the variable Price to stand for the variable to save space.
Due to page limit, the XML schema that defines BookRequest is not shown.
Let us discuss A4.
Figure 7 shows the messages used at A4 for t1 to t6.
<Price, Num> Test case 1 (t1): <200, 1> Test case 3 (t3): <125, 3> Test case 5 (t5): < 50, 1> <Price, Num> Test case 2 (t2): <150, 2> Test case 4 (t4): <100, 2> Test case 6 (t6): <  1, 1> </room> <room> </room> </hotel > <hotel> <room> </room> </hotel > <hotel> <name>Hilton Hotel</name> <room> <hotel> <name>Hilton Hotel</name> <room> <roomno>R106</roomno> <price>105</Price> <persons>1<persons> <roomno>R106</roomno> <price>105</Price> <persons>1<persons> <roomno>R101</roomno> <price>150</price> <persons>3<persons> <roomno>R101</roomno> <price>150</price> <persons>3<persons> </room> <room> </room> </hotel > <hotel> <name>Hilton Hotel</name> <room> <roomno>R106</roomno> <price>105</Price> <persons>1<persons> </room> </hotel > Test Case 1 Test Case 2 Test Case 3 <roomno></roomno> <price>100</Price> <persons>2<persons> <hotel> </hotel > <hotel> <room> <price>-1</Price> <persons>1<persons> </room> <error>InvalidPrice<error> </hotel > Test Case 4 Figure 7.
XML messages for XQ(HotelInformation, Test Case 6 Test Case 5 //room[@price    Price  and @persons =  Num ]/price/).
When executing t1 to t6 on the program in Figure 5(b), t1 extracts a right room price; t4 to t6 extract no price value; both t2 and t3 extract the price 105 of the single room, however, they actually need to book a double room and a family room, respectively.
Observe that, t2 and t3 can detect the fault in Figure 5(b).
Similarly, for the program in Figure 5(c), t1 and t2 can extract the right room prices; t3 to t5 extract no price value; t6 extracts a room price  1, although it should not extract any price.
Only t6 can detect the fault in Figure 5(c).
Regression testing uses the coverage data achieved from previous execution round over a preceding version of the application to guide the current round of test case prioritization before executing these test cases on the modified application.
Table 1 shows the workflow branch coverage of t1 to t6 on the original program of HotelBooking (i.e., Figure 5 (a)).
We use a   to represent the item covered by test cases in Figure 8 and Tables 1, 2, and 3.
are same (Table 1).
A conventional branch-coverage prioritization technique may simply order them randomly, and thus ignore much useful information that potentially helps prioritize test cases to achieve a higher fault detection rate.
Therefore, we introduce how XPath and WSDL can be used to address the challenges.
Figure 8 shows the XRG nodes covered by t1 to t6 on the XRG (Figure 4) at node A4 of Figure 5(a).
Table 1.
Workflow branch coverage for t1 to t6.
Branch







 Total t1            
 t2          
 t3          






 < *, A, B, Rule2> < //, B, C, Rule 5> <precondition, C, D, Rule 1> < room, D, E, Rule 1> < price, E, F, Rule1> < *, F, G, Rule2> t4            
 t1             t2         t5          
 t3         t4             t6          
 t5     t6             Price = XQ(HotelInformation, q)  
 Figure 8.
Example of XRG conceptual path.
          Different XRG branches may lead to different content selections, and return different values to the workflow (see [12] for how to find out such a path).
For example, the XRG branch of t1 extracts the value 150 from the price tag and assigns the value to the variable Price.
However, for t2 and t5, it will return no value (referred to as the null value for the ease of discussion) to Price.
We further present Table 2 to show how t1 to t6 cover different XRG branches in the above XRG at node A4 of Figure 5(a).
Table 2.
XRG branch coverage for t1 to t6.
XRG branch









 Total t1          
 t2        
 t3        
 t4          
 t5    
 t6          
 We observe that the XRG branches covered by t1 and t4 are identical.
On the other hand, the branches covered by t2 and t3 are different from those covered by t5 and t6.
Let the tuple  top, bottom  denote the theoretical highest (top) and lowest (bottom) priority orders of a test case determined by a potential prioritization technique.
If we use the additional coverage data on the XRG branch, the tuples for both t2 and t3 are  2, 3 .
However, using the additional branch coverage data, the tuples for both t2 and t3 are  2, 6 .
This shows that the use of additional XRG branch coverage may increase the chance of achieving a higher fault detection rate.
To explore the difference between test cases like t1 and t4, we further present Table 3 on how test cases t1 to t6 cover the WSDL elements (the schema is given in Figure 3) at node A4.
Table 3 shows that t1 to t3 cover the same set of WSDL elements, and are different from t4, t5, or t6.
Intuitively, if we use the additional coverage data on WSDL elements, the couple for t6 will be  2, 2 .
However, the couple for t6 is  2, 6  if using the additional branch coverage data, and is  1,
 using WSDL elements has the potential to increase the chance of achieving a higher fault detection rate.
Table 3.
Statistics of WSDL elements for t1 to t6.
XML schema t1                    
 t2                    
 t3                    
 t4              
 hotel name room roomno price persons error val(name) val(roomno) val(price) val(persons) val(error) Total t5  
 t6                
 We have shown that merely using the workflow branch coverage data may not reveal the internal conceptual branches and message types caused by the XPath and WSDL, and thus the performance of test case prioritization has not been fully maximized.
This observation motivates us to present new techniques that take the XRG and WSDL coverage data into consideration.
Given a test suite T for a service-oriented business application, our target is to reorder T according to the coverage data of the test cases in T when P is executed, with a view to effective regression testing of modified versions of P. In this section, we present a family of new test case prioritization techniques for such regression testing.
In view of the presence of heterogeneous artifacts, we propose a new coverage model to facilitate the development of our test case prioritization techniques.
A coverage model for a service-oriented business application P is a 4-tuple  T,  ,  ,  , where (a) T is a regression test suite for P; (b)  ,  , and   represent, respectively, sets of workflow branches, sets of XRG branches, and sets of WSDL elements collected from various executions of P; and (c)  (t),  (t), and  (t) represent, respectively, the set of workflow branches, the set of XRG branches, and the set of WSDL elements covered by the execution of P with respect to a test case t   T.
We propose to utilize the coverage data of the test cases by levels.
Level 1 covers only workflow, which is the basis of a business process.
Next, since workflow may use XPath expressions to manipulate XML messages, level 2 covers both workflow and XPath.
Finally, since XML messages must conform to the WSDL specification, level 3 covers workflow, XPath, and WSDL.
For ease of presentation, we refer to the three levels of coverage data as CM-1, CM-2, and CM-3, respectively, where CM stands for Coverage Model.
Through the level-by-level use of coverage data, we propose a new family of test case prioritization techniques.
This section presents our test case prioritization techniques.
If we considered a workflow program as a conventional program, the first two techniques (M1 and M2) would resemble to the branch coverage technique are shown in Table 4.
M1 (Total-CM1) [19]: This technique sorts the test cases in T in descending order of the total number of workflow branches executed by each test case.
If multiple test cases cover the same number of workflow branches, M1 orders them randomly.
M2 (Addtl-CM1) [19]: This technique iteratively selects a test case t that yields the greatest cumulative workflow branch coverage, and then removes the covered workflow branches,  (t), from all remaining test cases to indicate that those removed workflow branches have been covered by the selected test cases.
Additional iterations will be conducted until all workflow branches have been covered by at least one selected test case.
If multiple test cases cover the same number of workflow branches in the current round of selection, the technique selects one of them randomly.
If no remaining test cases can further improve the cumulative workflow branch coverage, the technique resets the workflow branch covered of each remaining test case to its original value.
It applies the above procedure until all test cases in T have been selected.
Let m and n be the test suite size and the maximum number of workflow branches covered by a test case t, respectively.
Collecting the branch coverage of test cases will take O(mn) time.
Sorting test cases will take O(m log m) time.
Therefore, M1 can be finished in O(mn + m log m) time and M2 can be finished in O(m2 n + m2 log m) time.
M3 (Total-CM2-Sum): This technique is the same as Total-CM1, except that it uses the total number of workflow and XRG branches covered by each test case, rather than only the total number of workflow branches as in Total-CM1.
It treats workflow branches and XRG branches in the same way.
M4 (Addtl-CM2-Sum): This technique is the same as Addtl-CM1, except that it uses the set of workflow and XRG branches covered by each test case, rather than only the set of workflow branches as in Addtl-CM1.
It also treats workflow branches and XRG branches in the same manner.
Another way to prioritize test cases is to reorder test cases using the number of workflow branches, and when encountering a tie, in which multiple test cases have the same number of workflow branches, a technique may use the number of XRG branches to break the tie.
M5 (Total-CM2-Refine): This technique is the same as Total-CM1 except that, if multiple test cases cover the same number of workflow branches, to break the tie, M5 orders them in descending order of the total number of XRG branches covered by each test case involved in the tie.
If there is still a tie, M5 randomly orders the test cases involved.
M6 (Addtl-CM2-Refine): This technique is the same as Addtl-CM1 except three things.
First, in each iteration, M6 removes the covered workflow branches and the XRG branches of the selected test cases from the remaining test cases to indicate that those removed workflow branches and XRG branches have been covered by the selected test cases (despite that M6 still selects test cases based on the workflow branch coverage as in Addtl-CM1).
Second, if multiple test cases cover the same number of workflow branches in the current round of selection, rather than selecting one of them randomly, the technique selects the test case that has the maximum number of uncovered XRG branches.
If there is still a tie, it randomly selects one of the test cases involved.
Third, when resetting is needed, the technique resets each remaining test case to its original workflow branch coverage and XRG branch coverage.
Let m, n, and x be the test suite size, the maximum number of workflow branches, and XRG branches covered by a test case t, respectively.
Collecting the branch coverage and XRG branch coverage of test cases will take O(mn + mx) time.
Sorting test cases will take O(m log m) time.
Therefore, M3 can be finished in O(mn + mx + m log m) time and M4 can be finished in O(m2 n + m2 x + m2 log m) time.
The time complexity of M5 and M6 are the same with those of M3 and M4, respectively.
M7 (Total-CM3-Sum): This technique is the same as Total-CM2, except that it uses the total number of workflow branches, XRG branches, and WSDL elements covered by each test case, rather than only the total number of workflow and XRG branches as in Total-CM2.
It treats workflow branches, XRG branches, and WSDL elements in the same way.
M8 (Addtl-CM3-Sum): This the same as Addtl-CM2-Sum, except that it uses the set of workflow branches, XRG branches, WSDL elements covered by each test case, rather than in Addtl-CM2-Sum.
It also treats workflow branches, XRG branches, and WSDL elements in the same fashion.
the set of workflow and XRG branches as technique just is M9 (Total-CM3-Refine): This technique is the same as Total-CM2-Refine, except that in the case of a tie, M9 arranges the test cases in descending order of the total number of WSDL elements covered by each test case involved.
If it still cannot resolve a tie, the technique randomly orders the test cases involved.
M10 (Addtl-CM3-Refine): This technique is the same as Addtl-CM1, except three things.
First, in the each iteration, M10 removes the covered workflow branches, the covered XRG branches and the covered WSDL elements of the selected test cases from the remaining test cases to indicate that those removed workflow branches, XRG branches and WSDL elements have been covered by the selected test cases (despite that M10 still selects test cases based on the workflow branch coverage as in Addtl-CM1).
Second, if multiple test cases cover the same number of workflow branches in the current round of selection, the technique selects the test case that has the maximum number of uncovered XRG branches.
If there is a tie, it selects the test case that has the maximum number of uncovered WSDL elements.
If there is still a tie, it randomly selects one of the test cases involved.
Third, if resetting is needed, the technique resets each remaining test case to its original workflow branch coverage, XRG branch coverage, and WSDL element coverage.
Let m be the test suite size; n, x, and w be the maximum numbers of workflow branches, XRG branches, and WSDL elements covered by a test case t, respectively.
Collecting the coverage data of workflow branches, XRG branches, and WSDL elements of m test cases takes O(mn + mx + mw) time.
Sorting m test cases takes O(m log m) time.
Hence, M7 can be completed in O(mn + mx+ mw + m log m) time while M8 can be completed in O(m2 n + m2 x + m2 w + m2 log m) time.
The time complexities of M9 and M10 are the same as those of M7 and M8, respectively.
In Section 5, we will follow [5][19] and compare our test case prioritization techniques with two control techniques, namely random and optimal.
For the sake of completeness, we revisit them in this section.
C1: Random prioritization [19].
This technique randomly orders the test cases in a test suite T.
C2: Optimal prioritization [19].
Given a program P and a set of known faults in P, if we know the specific test cases in a test suite T that expose specific faults in P, then we can determine an optimal ordering of the test cases to maximize the fault detection rate of T.
Such a prioritization is an approximation to the optimal case [19].
In total, we have reported 10 techniques.
The acronyms of these techniques are listed in Table 4.
We use the motivating example prioritization results on t1 t6 to help illustrate each technique.
Table 4.
Categories of prioritization techniques and examples.
t6 Category

 (workflow)








 Total-CM1 Addtl-CM1 Total-CM2-Sum Addtl-CM2-Sum Total-CM2-Refine Addtl-CM2-Refine Total-CM3-Sum Addtl-CM3-Sum Total-CM3-Refine Addtl-CM3-Refine Index









 t2









 t1









 t4









 t3









 t5










 (summation)
 (summation)
 (refinement)
 (refinement) Inspired by subsumption relations among the coverage criteria in unit testing, we propose a notion of subsumption for test case prioritization techniques.
Subsumption: Given two test case prioritization techniques X and Y, we say that X subsumes Y if and only if any permutation of any test suite produced by Y can also be produced by X.
is reflexive, Obviously, subsumption transitive, and anti-symmetric.
It is, therefore, an equivalence relation.
We have analyzed the subsumption relations among our techniques, and the result is summarized in Figure 9.
For instance, we have proved that (M1) Total-CM1 subsumes (M5) Total-CM2-Refine, and we use an arrow from M1 to M5 to represent this relation in the figure.
Other arrows can be interpreted similarly.
The proof of the subsumption relations is straightforward and hence we omit the details because of space limit.
The basic idea is that, if random selection in resolving ties in one technique is replaced by a more deterministic procedure in another technique, then the former technique subsumes the latter.
For instance, unlike M1 (which always use the random selection approach to resolve tie cases), M5 references the XRG branch coverage of test cases to resolve tie cases before using a random selection as the last resort.
Because any test case that M5 can pick to resolve a tie may also be selected by chance in M1, any test case permutation produced by M5 must be a permutation that can be produced by M1.
Other subsumption relations shown in Figure 9 can also be reasoned similarly.
Total-CM3-Refine Addtl-CM3-Refine

 Total-CM2-Refine
 Optimal Addtl-CM2-Refine

 Total-CM-1 Addtl-CM-1
 Figure 9.
Hierarchy of test case prioritization techniques.
random



 We choose WS-BPEL [22], a representative type of service-oriented business application [1][16][24], to evaluate our approach.
The Software Engineering community also uses these applications to evaluate approaches related to service-oriented business applications (e.g., see [12]).
We adopt the set of applications evaluated in [12] as our subject.
Table 5 shows the descriptive statistics of the subject applications.
For example, the size of each application is described using the number of XML elements ( Element ) and the lines of code ( LOC ).
Table 5.
Subject programs and their descriptive statistics.
Ref.
Applications A atm [1] B buybook [16] C dslservice [24] D gymlocker [1] loanapproval[1]
 F marketplace [1] G purchase [1]
 Total triphandling [1] d e i f i d o
 s n o i s r e









 t n e m e l












 h t a





















 h c n a r




 t n e m e l
 d e s
 s n o i s r e



























 We use the known faults and associated test suites to measure the effectiveness of different prioritization techniques.
The faults in the modified versions have been reported by [12] (in which faults are created following the methodology presented in [7]).
We then follow [4][5][7] and discard any fault version if more than 20 percent of test cases can detect failures due to its fault.
The statistics of the selected modified versions from [12] are shown in the rightmost column of Table 5.
We implement a tool to automatically generate test cases for each application.
Based on WSDL specifications, XPath queries, and workflow logics of the application (not using modified versions), we generate test cases to ensure that the generated test cases can cover all workflow branches, XRG branches, and WSDL elements (dubbed CM-3 elements) at least once.
In total, for each application,
 construction process is also adopted in [5][19].
(a) 25 Percentile

















 Level-1 Level-2






 Level-3 (b) 50 Percentile




























 (c) 75 Percentile










 (d) Mean




























































 Level-3 Level-1 Level-2 Level-3 Figure 10.
Overall comparisons using APFD measurement.
Level-2 Level-1 Level-1 Level-2






 Level-3
















 (a) atm (b) buybook























































 (e) loanapproval
























 (f) marketplace





























 (c) dslservice
























 (g) purchase (d) gymlocker
























 (h) triphandling




























































































 Figure 11.
Comparisons on each application using APFD measurement (CM-3 techniques always outperform random).
Ref.
Table 6.
Statistics of test suite sizes.
Avg.
Size Maximum Average Minimum We select test cases one by one randomly from a test pool and put them into a test suite (which is initially empty).
Such selection is iteratively done until all the CM-3 elements have been covered at least once (and each fault has been detected by at least one selected test case).
The process is similar to the test suite construction in [5][17].
We apply each test suite to applicable modified versions of corresponding application.
In total, we successfully generate 100 test suites for each application.
Table 6 shows the statistics of the test suites.
For each subject program and for each constructed test suite, our tool applies C1, C2, and M1 M10 to prioritize the test suite.
For every prioritized test suite, the tool executes each modified version of the corresponding subject program over the test cases according to their order in the prioritized test suite.
Since all the test case execution results on these applications are determined, we can figure out whether a fault has been revealed by a test case through comparing the test result on the modified version to that on the original program.
Our tool automates the comparisons.
For each application, we apply C1, C2, and M1 M10 on a test suite, run the modified applications over the test suite, and then calculate APFD values.
We repeat this procedure 100 times using the generated test suites.
In total, 69,440 test cases have been executed, and we collect 833,280 APFD values.
The results are represented using box-plots in Figures 10 and 11.
A box-plot shows the 25th percentile, median, and 75th percentile of a technique in a graph.
We summarize the overall results using the 25th percentile, median, 75th percentile, and mean APFD in Figure 10, respectively.
The results for individual applications are given in Figure 11.
In Figure 10, we find that M6 and M7 M10 (i.e., one technique at level 2 and all techniques at level 3) are generally better than all the other techniques except the optimal technique (C2).
When we focus on the techniques M1 M6, M2 and M4 are the best two techniques among the techniques in the same level.
Both M3 and M5 are better than M1.
M1 reports the worst performance among M1 M10 in this experiment.
The overall result may not represent the result of each benchmark application, and hence we further compare C2 and M1 M10 with the random technique (C1).
If the APFD achieved at the 25th percentile of a technique is larger than, equal to or smaller than the random technique (C1), then we add 1 at the category  > Random ,  = Random , and  < Random  of the technique, respectively.
Similarly, we compare C2 and M1 M10 with C1 using the median and 75th percentile APFD values.
Table 7 shows the comparison results.
Table 7.
Comparisons with random technique.
Technique



















 Category
 Median
 > Random < Random > Random < Random > Random < Random




































































 From Table 7, we note that C2, M4, M8, and M10 outperform the random technique (C1) in all categories.
It is not surprising that C2 is better than C1, since C2 is an optimal approximation technique.
C1 shows the worst performance generally.
Among our techniques (M1 M10), M1, M3, and M5 show the worst performance when comparing to C1.
This observation also holds when using the mean APFD values, as shown in Figure 10(d).
We further apply hypothesis analysis on the results to identify the differences among different techniques.
We follow [9] to explore where the differences lie by using a multiple-comparison procedure.
The Least Significant Difference (LSD) method was employed in multiple-comparison to compare test case prioritization techniques [9].
If the significance level is less than 0.05, the difference among the metrics is statistically significant.
We compare each pair of techniques for each application, and categorize the results into two groups (> 0.05 and < 0.05) using the significance of the mean difference.
We do not show the cases when x   y = 0.
The results are shown in Table 8.
We group M1 M10 into three groups according to the coverage model: M1 M2 (CM-1), M3 M6 (CM-2), and M7 M10 (CM-3).
M1 M2 and M3 M6, between M1 M2 and M7 M10, and between {M4, M6} and M7 M10.
The within-group comparisons measure the differences within M1 M2, M3 M6, and M7 M10.
Due to page limit, we only report the results on comparing M4 and M6 in the group M3 M6 (CM-2) to compare with M7 M10.
We choose M4 and M6 as the representative techniques for M3 M6 because these two techniques show a better performance in Figure 10.
We mark the rows which indicate CM-2 techniques and CM-3 techniques are significantly better than CM-1 techniques into gray in background.
Table 8.
Multiple comparisons (least significance differences).
Category
 vs.
Between Group
 vs.
vs.
Within Group Techniques (x, y)




































 Sig.
< 0.05 Sig.
> 0.05 x y>0 x y<0 x y>0 x y<0



















































































































































 In the between-group category, M4 and M6 M10 all show significantly better results than both M1 and M2 (using the workflow coverage data).
The difference between {M3, M5} and {M1, M2} are not significant.
In the within-group category, we note that both M4 and M6 are significantly better than M3 and M5, which confirms our observation in Figure 10.
The techniques within M7 M10 are similar in performance, and we only find the significant differences between M8 and {M7, M9, M10}.
This section analyzes the impact of different levels of coverage data on the effectiveness of the technique.
We use the overall mean APFD result of each technique in Figure 10(d).
C1 and C2 report the worst and best mean result using the mean APFD in the box-plot of Figure 10(d).
Let us focus on the mean effectiveness of M1 M10.
Using the mean APFD in Figure 10(d), the techniques using the additional coverage data are better those using the total coverage data.
The pairs of techniques (M1, M2), (M3, M4), (M5, M6), (M7, M8), and (M9, M10) all demonstrate this conclusion.
We also check the subsumption relations (Figure 9) and overall effectiveness (Figure 10) for two groups of techniques: (M1, M5, M9) and (M2, M6, M10).
The comparison result indicates a technique being subsumed may achieve a higher fault detection rate.
For example, M5 is better than M1, and M9 is better than M5.
We observe that the mean effectiveness increases when more types of artifact have been considered in test case prioritization technique (i.e., as we include  , to   and  , and finally  ,  , and  ).
For example, when we categorize the techniques at Level 2 and Level 3 into pairs (M3, M7), (M4, M8), (M5, M9), and (M6, M10), the differences between two techniques in each pair support our observation.
Similar observation can also be found the between-group comparisons in Table 8.
The construct validity of our experiment relates to the metrics used to evaluate the effectiveness of test case prioritization.
We use the metrics APFD in the experiment.
Although normally knowing the faults exposed by a test case in advance is impractical, and hence an APFD value cannot be estimated before testing has been done.
However, APFD can be used as a measure to show the feedback of prioritization techniques when testing has finished.
in The external validity is whether the experiment can be generalized.
We use WS-BPEL applications as subjects.
They are a representative kind of service-oriented business application.
Our experiments can be conducted using other service-oriented applications that use XPath queries and WSDL specifications.
We will find more such applications to evaluate our techniques.
First, we use XRGs to model XPath queries in the presence of WSDL specifications.
Other models to represent the XPath queries can also be used after defining coverage properly.
However, the effectiveness of different XPath models may be different.
In addition, our coverage model arranges the three artifacts in a particular order:  workflow, XPath, WSDL .
It would be interesting to study the effectiveness of other potential orders (such as  workflow, WSDL, XPath ), and compare them with our proposed techniques.
Other such orders may result in different test case prioritization techniques.
We plan to collaborate with the industry to apply our techniques in real-world projects and study the effectiveness of our presented techniques.
We also plan to apply other statistical analyses of the results to gain more insights in the future.
Second, test case prioritization techniques can be categorized generally into two types [19]: general test case prioritization and version-specific test case prioritization.
General test case prioritization reorders a test suite T for a program P to be useful in subsequent revised versions of P. Version-specific test case prioritization reorders test cases in a test suite T to be useful in a specific version P  of P. Our work is under the category of general test case prioritization.
It would be interesting to extend our techniques to version-specific test case prioritization.
This section reviews the related literature.
In the context of test suite construction, Martin et al. [10] generated test cases based on WSDL specifications and treated them as requests for web services.
Their technique perturbed the web requests, in the spirit of mutation testing, to test whether web services may robustly handle the perturbation.
Their work discussed briefly the potential usage of the technique in regression testing of web services.
Our previous work [2] applied metamorphic relations to construct test cases for stateless web services.
Our previous work [12] proposed XPath Rewriting Graphs (XRGs) to represent conceptual paths (see Section 2.2).
The XRGs help reveal the connection between WSDL and Workflow.
It also proposed several unit testing criteria to exploit such connections generate test suites but study the techniques to reorder existing test suites for regression testing.
Next, we review the research on test case selection for service-oriented applications.
Ruth and Tu [20] proposed to conduct impact analysis to identify revised fragments of code in a service by comparing the control flow graph (CFG) of the new version with that of the preceding version.
Their technique selected test cases associated with modified edges of the CFG.
We study the test case prioritization problem in regression testing, rather than the test case selection problem.
According to [15], these are distinct (but important) problems in regression testing research.
Hou et al. [6] proposed to add quota to constraint the number of requests to specific web services.
They further developed techniques to prioritize test cases to maximize test requirement coverage under such quota constraints.
Our work studies the internal organization of service-oriented business applications (representing the internal organization using a multilevel coverage model) and prioritizes test cases according to such properties.
Regression testing is the de facto means to assure the quality of a program against unintended effects of maintenance.
Test case prioritization has been an effective means to order test cases in regression test suites so that faults can be detected earlier.
When maintaining a service-oriented business application such as one written in WS-BPEL, process engineers may unintentionally introduce faults into various artifacts including the workflow programs, XPath queries, and WSDL specifications.
Traditional test case prioritization techniques, which do not take all the artifacts into consideration, may no longer be effective for such an application.
In this paper, we have examined the important impact of considering these heterogeneous artifacts on test case prioritization in the regression testing of service-oriented business applications, and demonstrated the shortcomings of traditional test case prioritization techniques in this aspect.
We have proposed a family of test case prioritization techniques that take into account the coverage data of test cases at three levels (workflow, XPath, and WSDL).
We have further presented a hierarchy of subsumption relations among the test case prioritization techniques.
To the best of our knowledge, this is the first hierarchy to relate test case prioritization techniques in the literature.
The experiment results show that our techniques significantly outperform conventional test case prioritization techniques in terms of the fault detection rate (the most widely used metric for evaluating test case prioritization techniques in the software engineering community).
Our experiment results also confirm that considering the artifacts level by level is an effective strategy the quality of service-oriented business applications.
testing for assuring in regression In the future, we will continue to study how to make use of non-imperative artifacts to develop effective techniques to address other challenges in the regression testing of service-oriented business applications.
It would also be interesting to adapt our techniques to other service-oriented applications.
