Domain transfer for high dimensional datasets, such as microarray data, text data, web log data, is a challenging problem.
When the number of features is in thousands, in-domain and out-domain data rarely share the exact set of feature.
In the same time, there may not be any labeled example from in-domain.
When one uses the union of the overlapping and non-overlapping features and leave the missing values as  zero , the distance of two marginal distributions p(x) can become asymptotically very large.
Otherwise, if one only considers those common features, such information may be limited in its predictability, and many algorithms may have di culties to  nd transferable structures across the two domains.
Therefore, one main challenge to transfer high dimensional overlapping distribution is on how to e ectively use those large number of features that are non-overlapping or present in one domain but not in the other.
Nonetheless, the main task for inductive learning is to identify  substructures  between the two domains where it is transferable or the conditional probabilities p(y|x) across these structures are similar.
This is particularly di cult under the given scenario.
First, there are no labeled data from in-domain, hence the relationship of p(y|x) across the two domains are not directly measurable.
Second, the problems are high dimensional with missing values.
Thus the second main challenge is on how to look for transferable substructures (or considering subset of features in conditional probability) in this space.
In order to resolve these two main challenges, we  rst bring the marginal distribution of the overlapping distributions asymptotically closer by  lling up  the missing values in some reasonable way to be discussed.
Then to resolve the high dimensional problem, we map the original feature space into a low-dimensional  latent space  where each feature is a linear combination of high dimensional features.
We show that in this low dimensional space, transferable concepts are easier to discover and their prediction error can be bounded.
By default, most inductive learner usually makes are likely to have the same class label.
This is more likely to hold true in low-dimensional space.
Speci cally, it has been shown formally that as the dimensionality of the space increases, the Euclidean distance between any points in the high dimensional space is getting asymptotically closer.
In other words, distance-based classi cation is unreliable in high dimensional space.
For example, in Figure 1(a), we plot two domains  data in 3-D space, where the pluses(+) and crosses( ) are labeled out-domain positive and negative instances, respectively.
The stars(?)
and squares((cid:3)) are the unlabeled in-domain positive and negative instances, respectively.
The two domains  data are related since the positives from both domains lie on the x-y plane while all the negatives lie on y-z plane.
However, two domains are di erent since they have quite di erent distribution.
When classifying the stars using cosine similarity, because they are closer to some of the crosses than to the pluses, they are classi ed incorrectly, The cluster assumption is obviously violated in this space.
Next, we will brie y summarize the proposed approach and come back to this example to see how the problem is being resolved.
Given these challenges, we propose a new latent space based method for domain transfer where there is no labeled in-domain data and both out-domain and in-domain are high-dimensional.
Brie y, the proposed approach has three main steps, as depicted in Figure 2.
We  rst employ a  multiple regression method  to  ll up those  missing values  across the two domains in order to draw the marginal distributions closer, here we assume that the discrepency between distributions of two domains over these overlapping features are reasonably small (these features are shared by two domains).
Then, both the high dimensional out-domain and in-domain data are mapped into a low-dimensional latent-space.
In order to classify an unlabeled in-domain example, we retrieve the closest neighbors in the latent space to the in-domain example and use weighted voting as the predicted class label.
To be exact, regression models are built using out-domain data, taking overlapping features as independent variables and non-overlapping features as dependent variables.
Missing values in in-domain are  lled up by these models.
Intuitively, these uniform transferable models describe feature dependency in both domains, thus data will lie in the same space and marginal distribution p(x) become closer (see Section 3.2).
Second, we propose to use SVD (Singular Value Decomposition) for dimension reduction and similar structure discovery.
SVD  rst maps the high dimensional out-domain and in-domain data to a latent space with lower dimension.
In this space, data giving the same concept will lie nearby[5] (see Section 3.3), i.e. the closer two instances are, the more likely they are having the same label, thus p(y|x) across domains will be similar within cluster where instances are coherently nearby.
To exploit this similar conditional distribution, given an in-domain instance x0, those out-domain instances which are most close to x0 are retrieved and x0 is classi ed by similarity-weighted voting.
Now, let s revisit the previous example.
We apply SVD on two domains  data (step 3 in Figure 2) and the lower dimensional representation in latent space are obtained, as plotted in Figure 1(b).
For each in-domain point x(star or square), we retrieve p nearest out-domain points (plus or cross) and classify x according to the labels of the retrieved points and the corresponding similarity (step 4 in Figure 2).
z z

 y out-domain positive out-domain negative in-domain positive in-domain negative







 x

 (a) High dimensional space


 x


 positive out-domain negative out-domain positive in-domain negative in-domain -0.05 -0.05





 z (b) Low dimensional space Figure 1: Illustrating Example Note that the cluster assumption holds in this space.
We can see clearly that the stars/squares are now approximately in the same direction of the pluses/crosses, all retrieved out-domain points will be pluses/crosses.
Our contributions are as follow: (1) We propose a transfer learning framework which make joint distributions of two high dimensional domains with overlapping features easier to measure, and thus identify transferable concepts is straightforward.
Multiple regression and SVD are used to resolve the di culties to measure and identify structures with similar p(x) and p(y|x) respectively.
The experiments results demonstrate the proposed framework outperforms traditional learning algorithms including SVM.
(2) Formal Figure 2: Flow Chart of the Proposed Framework Symbol
 (X u) (  X u) ` u p(x) p(y|x)
 Fc





 k De nition Labeled out-domain data Unlabeled in-domain data X u with predicted missing values Number of out-domain Number of in-domain points marginal distribution conditional distribution Features exist only in out-domain Features shared by two domains input high dimensional space low dimensional latent space Data matrix combining X ` and  X u Matrix of principle direction Matrix of singular values Matrix of principle component Dimensionality of latent space domain points on F+ can be seen as a series of column vector yl +, l = 1, .
.
.
, |F+| (not class label) while the values of in-domain points on F+ are missing.
|F+| regression models M = {M1, .
.
.
, M|F+|} can be built using H ` as observation of independent variables (namely, features in Fc) and yl +, l = 1, .
.
.
, |F+| as observation of dependent variables (namely, features in F+), thus Ml gives the estimated functional relationship between Fc and f l +.
Ml is then applied on H u to predict the values of all in-domain points on f l +, and thus missing values are  lled up.
Various regression models can  t into the proposed framework, since they are basically seeking a function such that the predictions are expected to have deviation from the actual values within a given small tolerance  for all the training data.
For implementation, -Support Vector Regression (-SVR)[13, 14] is employed to  ll up missing values, it can be formulated as: analysis demonstrates that missing values importation via regression asymptotically reduces the di erence between two marginal distributions p(x) than one without importation.
In the low dimensional latent space, we give the upper bound of nearest neighbor classi er used in the given transfer learning scenario.
This condition is guaranteed by the clusters recovery process[5].
(3) The proposed framework can be generalized to include various regression and cluster methods, not limited to the experimental choices.
We introduce latent space based transfer learning framework between two domains that lie on two di erent spaces that are at most overlapping.
We summarize symbols and their de nitions in Table 1.
Suppose we have ` labeled out-domain instances (X `, Y `) = {(x1, y1), .
.
.
, (x`, y`)} and u unlabeled in-domain instances X u = {x`+1, .
.
.
, x`+u}.
Let X ` and X u also denote matrices, each row represents an instance while each column represents a feature.
We assume that two domains  data fall in two categories (y   {0, 1}).
As an important step, we  rst discuss how to perform multiple regression to  ll up the missing values, then SVD-based dimension-reduction and clustering is brie y discussed.
Since two spaces are overlapping or they only share some small number of features, when a classi cation model is trained on out-domain data using all its features then subsequently use to prediction in-domain documents, one need to consider how to properly handle those  missing values  or features in F+ that exist only in out-domain but not in in-domain.
As we shall see in Section 3, if we leave those missing values as  zeros , the distance between any two points can be asymptotically very large and order of di erence between di erent points are di cult to distinguish.
Nonetheless, if we  ll up the missing values in some reasonable way, the order of the di erence between closer points and further points is more measurable.
We propose to use  multiple regression  to  ll up those missing values across the two domains.
Let Fc denote the set of features overlap across domains, and H ` and H u denote the out-domain and in-domain data X ` and X u projected on Fc
 respectively.
Let F+ = {f 1 + }.
The values of out-+, .
.
.
, f minimize s.t.
kwk2 +  

 n Xi=1 ( i +   i ) yli +   hw, hii   b    +  i +    +   hw, hii + b   yli  i,   i   0 i where hi is the i-th row in H ` and yli + is the i-th entry of +.
 i,   yl i are slack variables, w is the model parameter and   determines the trade-o  between the generalization ability (measure by kwk2) and the amount up to which deviations larger than  are tolerated.
The functional relationship +, i.e. Ml, is then applied on H u (can learned using H ` and yl be seen as new-coming examples in the same space with H `) and give prediction on yl +.
The aim of multiple regression is to minimize the discrepency between the marginal distributions of two domains.
Admittedly, H ` and H u come from di erent distributioins, however generalization error bound is given in [16] when training and test data are from di erent distribution, and this justi es our regression strategy.
The marginal distributions p(x) of two domains are made easier to measure and quantify in the input space W via missing values importation.
However, the dimension of the vector space is still so high that makes it hard to identify similar structures across domains.
Given a point x   Rn, as n grows, the distance between the  nearest  neighbor to x will approximate to the distance from the  farthest  neighbor to x.
Thus, the distance in high-dimensional space is meaningless.
Since for any reasonable problem, one makes the the clustering-manifold assumption that nearby points will have similar label, the Euclidean distance in the high dimensional space is no longer a good measure to even apply this assumption.
In addition, there is no labeled in-domain data, no information of p(y|x) is given, thus the relationship of p(y|x) across domains are not directly measurable.
To solve this problem, one ought to map the data to a low dimensional space.
In this space, substructures are discovered in places where in-domain and out-domain points will have similar labels and the cluster assumption holds across true with high probability.
We propose to use SVD for latent space mapping and substructures discovery.
In short, SVD  rst maps the data to a lower dimensional space, this mapping has been proved to be consistent with k-means clustering s objective function, namely, the low di-dicators from which clusters structure can be reconstructed [5].
By cluster assumption [1], points in the same cluster has similar conditional distributions regardless of whether the points are from the same domain or not, thus we have identify transferable substructure in latent space.
Specifically, let A   Rt d be the feature-instance matrix of two domains (see Table 1), then each row of A is an instance and each column of A is one dimension of the union of two space.
We further assume that the the  rst ` rows are out-domain instances and the next u rows are in-domain instances and the  rst |F+| columns are features exist only in out-domain and the next |Fc| columns represent those overlapping dimensions of two spaces, thus d = ` + u and t = |F+| + |Fc|.
SVD is applied on A
 (1) where   is a t   d matrix with diagonal entries ( i, i   {1, .
.
.
, min{t, d}) being the singular values  1    2            min{t,d} and o diagonal entries being zero.
The t   t matrix U and the d   d matrix V are the left and right eigenvector matrices, respectively.
Note that both U and V are orthogonal matrices: U TU = It, V TV = Id.
Then SVD can be seen as a solution of PCA by the following equation:
 where V = {v1, .
.
.
, vd} is the eigenvectors.
For dimension reduction, we obtain the top k   1 eigenvectors with largest eigenvalues ( 2 k 1) as new data representation.
That is let Vk 1   Rd k 1 contains (v1, .
.
.
, vk 1) as columns, then each row  i, i = 1, .
.
.
, d in Vk 1 is a new representation of xi, i = 1, .
.
.
, d in the k   1-dimensional space, i.e.
1, .
.
.
,  2 Vk 1 = ATUk 1 1 k 1 (2) k 1Uk 1 1 where Uk 1 is the  rst k   1 columns of U and  k 1 is a k   1   k   1 matrix keeping the  rst k   1 columns and rows of  .
Note that we use ATUk 1 1 k 1 to approximate the top k   1 columns of the principle component matrix instead of truncating the V to k   1 columns which equals
 k 1.
This is common in information retrieval and justi ed by the fact that Ak 1 is the best rank k   1 approximation of A. Uk 1 1 k 1 can be considered as a mapping T : W   S which maps data in the space W (space where each dimension represents one feature) to a new lower dimensional space S (called latent space).
For clustering, [5] has proved that the above dimension reduction also reveals the information of the clusters structure which k-means seeks to discover.
Speci cally, k-means uses k centroids to represent k clusters which are determined by minimizing the sum of squared error Jk = k Xl=1 Xi Cl (xi   ml)2 (3) where ml = Pi Cl xi/nl is the centroid of cluster Cl and nl is the number of points in Cl.
Let Qk 1 = (q1, .
.
.
, qk 1) be cluster membership indicator vectors (we ll show how to reconstruct cluster structure later) such that QT k 1Qk 1 = Ik 1   R(k 1) (k 1) and qT l e = 0, l = 1, .
.
.
, k   1.
Then the k-means objective can be written as Jk = Tr(ATA)   eTATAe/n   Tr(QT k 1ATAQk 1) (4) since the  rst two terms in Equation (4) have nothing to do with Qk 1, then the k-means objective becomes Tr(QT k 1ATAQk 1) max Qk 1 (5) The objective (5) has a closed form and global optimal solution which is the eigenvectors Vk 1 = (v1, .
.
.
, vk 1) of ATA.
The clusters structure can be recovered by constructing a connectivity matrix: Sim = Vk 1V T k 1 (6) The entry Simij can be interpreted as connectivity between xi and xj, we can associate a connectivity probability be-jj.
Finally, the tween xi and xj: cij = Simij/Sim clusters structure is determined such that xi and xj are in the same cluster if and only if cij >  .
ii Sim



 The e ect of SVD is threefold.
First, further bring the marginal distributions p(x) in S close given p(x) is su -cient close in W , this is achieved by the mapping given by Uk 1 T k 1 as we will see in Section 3.
Second, identify transferable substructure through which out-domain knowledge can be used for in-domain learning.
Given an unlabeled in-domain instance, we exploit the cluster structure by using top p nearest labeled instances for weighted voting.
Label0(xi) = Xxj  N(xi) Label(xj)Sim(xi, xj) (7) where Label0(x) is the prediction and Label(x) is the true label of x and N (xi) is the set of p nearest neighbors of xi according to Sim.
Since given xi, those xj with highest cij must be in the same cluster of Xi, given the size of the cluster is large enough, thus points in N (xi) must be those out-domain points which have the most similar conditional distribution with xi by the cluster assumption[1].
Finally, we see that the dimension is simultaneously reduced to k 1.
The above discussion provides us some insights to solve the problems raised in Section 1.
We propose to use both regression and latent space mapping to bring the distributions p(x, y) of in-domain and out-domain close and at the same time reduce the dimensions.
We call the proposed framework LatentMap in the sequel.
As shown in Algorithm 1, LatentMap consists of two key steps to deal with distributional di erence between domains.
First, X ` is used to give models which describe the relationship between features Fc and each feature f l +, l = 1, .
.
.
, |F+|, then we apply these models on X u to  ll up the missing values, the resulting data is denoted by  X u.
This step ensure we can bring the in-domain and out-domain marginal distributions in W close, as shown in Section 3.
Then these data is used to construct the data matrix A, which incorporates X ` as its  rst ` rows and  X u as its next u rows.
A is decomposed into three matrices using SVD and new representation of two domains  data are obtained in the latent space S. It has to be noted that this representation is indicators of clusters structure, instances having the same concept will be close in the latent space S. In other words, points in the same cluster express similar concepts and their labels are expected to be the same, thus we bring the conditional distributions p(y|x) of two domains close in S. Note also that this new representation of data have much lower dimensions than the data lows is instance retrieval step, given an in-domain instance xj , j = ` + 1, .
.
.
, ` + u, the labels of p out-domain instances which are most close to xj are use to vote for the label of xj .
This retrieval process may use instances from other clusters which xj does not belong to according to the cluster recovery process, nonetheless, the way that points are clustered depends on   (see the previous section): if   is small, size of clusters will become large and otherwise small, the clustering result may not be perfect.
By using similarity-weighted voting, we can ensure that the nearest neighbors (which are most likely to have the same conditional probability) have the most signi cant e ect on deciding xj s label while those points with less similarity to xj are not excluded entirely but have a weaker e ect on labelling xj .
Algorithm 1 LatentMap: Transfer Learning between High Dimensional Overlapping Distributions






 as next u rows.
ATUk 1 1 k 1


 Calculate the similarity between  i and all the out-domain instance  j , j = 1, .
.
.
, ` Retrieve top p out-domain instances based on the calculated similarity.
Label xi by voting using the retrieved instances  labels.
12: end for space do where   is the parameter of the model.
For i = 0, 1, let windows method justi es this: pn(x) =
  n n Xi=1 exp  (x xi )2 2 2 (8) where pn(x) is the estimated density at x, given instances xi, i = 1,       , n. Thus pn(x) is the sum of n Gaussians centered at xi.
pn(x) is a function of the distance between x and the instances xi.
In the sequel, we shall consider the marginal distribution p(x) as the empirical estimated marginal distribution pn(x) given by Equation (8).
ing and Test Distributions As stated before, the challenge of transfer learning is that the joint distributions p(x, y) of the two domains are di er-ent.
LatentMap aims at bridging this di erence by bringing  p1(x) closer to p0(x).
This strategy is justi ed by a theorem given in [16] that states that when the joint probabilities p(x, y) of the training and test data are di erent, the generalization error can be bounded asymptotically and the bound has two terms: one bounds the out-domain generalization error and the other bounds the di erence between the two distributions.
Assume that from the training data X `, Y ` we can obtain a Bayesian predictive model p(y|x, X `, Y `) = Z p(y|x,  )p( |X `, Y `)d  (9) Gi(`) = Ei x,yE0 X `,Y ` [log pi(y|x) p(y|x, X `, Y `) ] (10) Note that the expectation Ei x,y is over x and y with distribution pi(y|x)pi(x).
Thus G0(`) and G1(`) correspond to the generalization error with and without domain distributional di erence, respectively.
To give the generalization bound, two assumptions are made in [16]: (A) Gi(`) has an asymptotic expansion and Gi(`)   Bi as `    , where Bi is a constant.
(B) The largest di erence between the training and test distributions is  nite, i.e.
max x,y p0(y|x)p0(x) [ p1(y|x)p1(x) p0(y|x)p0(x) ] <   (11) Theorem 3.1 Under the assumptions (A) and (B), the generalization error G1(`) asymptotically has an upper bound,
 (12) where D1 = Z p1(y|x)p1(x) log p1(y|x) p0(y|x) dxdy D2 = 0 if p1(y|x) = p0(y|x) and 1 otherwise.
The detail of the proof can be found in [16].
G0(`) represents the out-domain generalization bound.
D2 is 1 since p0(y|x) 6= p1(y|x).
To minimize the asymptotically generalization error upper bound in transfer learning, we want to minimize the rest two terms: D1 and M0 relying on how close the two domain distributions are.
First, D1 is KL-divergence between the two conditionals p0(y|x) and p1(y|x).
Second, maxx,y p0(y|x)p0(x) p1(x)/p0(x) will be minimized when the di erence between the induced in-domain marginal  p1(x) and p0(x) ( xed) can be minimized.
In this section, we provide the theoretical basis for La-tentMap.
In transfer learning, the distributions of the two domains are di erent.
For convenience, let 0 and 1 be the subscripts denoting out-domain and in-domain, respectively.
We assume data from two domains are generated according to two unknown distributions p0(x, y) = p0(x)p0(y|x) and p1(x, y) = p1(x)p1(y|x), where pi(x), i = 0, 1 are the marginals, pi(y|x), i = 0, 1 are the conditionals, and p0(x) 6= p1(x) ,p0(y|x) 6= p1(y|x).
It is di cult for learning algorithms to learn e ectively since they usually assume p0(x, y) = p1(x, y).
The pivot is to  nd ways to mitigate the problem arising from this di erence.
The outline of the analysis is as follows.
First, we show that missing value prediction and participation of the in-domain data in SVD computation allow us to establish a bound for |p0(x)   p1(x)| in latent space, where  p1 represents the induced in-domain marginal in latent space.
Next, under the clustering assumption [1] and by the bounded di erence between the two marginals, we can further assume that the di erence between the two conditionals is also bounded or at least similar across the two domains.
Thus we can show that SVD not only brings two marginal distributions closer but also helps us discover clustering structures, where out-domain instances have their conditionals similar to a given in-domain instance.
Note that we measure marginal distribution discrepency using Euclidean distance, and the Parzen {x`+1, .
.
.
, x`+u} generated according to p1(x, y), if the two conditional distributions are similar, then for all i = ` + 1, .
.
.
, ` + u, p(y|xi) will deviate from p(y|xi) only by a small amount.
Thus the estimated D1 will be small.
In addition, given out-domain points X ` = {x1, .
.
.
, x`}, if in-domain points X u are close to X `, according to the Parzen windows method, the estimated marginals p1(xi) and p0(xi) at xi, i = 1, .
.
.
, ` as a function of the distance between xi and points in X ` and X u, respectively, will also be close.
We conclude that if the di erence between the two distributions can be minimized, then the upper bound of generalization error can be minimized.
In this section, we analyze the e ect of missing value prediction via regression.
Given out-domain samples xi   X ` and an in-domain instance xj   X u, xj s values along features F+ are missing (treated as zeros), while xi have values along features F+   Fc.
Assume that  xj is the same as xj except having its values along F+ predicted via regression models trained using X `.
x can be projected onto F+ and Fc, respectively and be written as x = ((xa)t, (xb)t)t, where xa is the projection of x on F+ and xb is the projection on Fc.
The squared distance between x and xj is given by D(x, xj) = kx   xjk2 = kxa   xa j k2 + kxb   xb jk2 (13) i   xb We assume that the di erence between the two marginals over Fc is bounded, i.e.  M1 > 0 such that  xi   X `, xj   jk2 < M1.
Thus the second term in Equation X u, kxb (13) is the same for both D(xi, xj ) and D(xi,  xj ).
Consider the  rst term in D(xi,  xj).
When ylj + are predicted with SVRs, we have kyli +   ylj + k2 = k < w, xb   kwk2kxb i >   < w, xb i   xb j > k2 jk2 < kwk2M1 where kwk2 is minimized when training the SVR model.
+ ,       , y|F+|i Since xa )t, j k2 is also bounded and minimized.
That is, thus kxa   > 0 such that i = (y1i i    xa + ,       , y|F+|j j = (y1j )t and  xa + + kxa i    xa j k2 =
 Xl=1 (yli +   ylj + )2 <   and   is minimized through the regression model.
On the other hand, kxa i   xa j k2 =
 Xl=1 (yli +   0)2 = kxa i k2 which is  xed depending on xi.
Compared with the minimized D(xi,  xj), D(xi, xj) is bounded but not minimized.
Thus we conclude here that the marginal distribution is minimized via regression.
Projecting the data onto a lower dimensional space (latent space S) is necessary since the dimensionality of W is high.
In this section, we  rst prove that the di erence between p0(x) and p1(x) can be bounded in S, given it is bounded in W .
Then by the clustering assumption, we bound the di erence between p0(y|x) and p1(y|x) in S. Once these two distributional di erences are bounded, then by applying Theorem 3.1 we claim that the generalization error in transfer learning is bounded through the proposed regression and SVD-based dimension reduction strategy.
Bound the marginal distributions in latent space In the previous section, we have discussed how the marginal distribution di erence can be bounded in the space W .
In this section, we further investigate properties of SVD and show how the di erence between the two marginals can be bounded in latent space S, given this di erence is bounded in W .
In step 7 in Algorithm 1, the data matrix A in W is mapped to S by Vk 1 = ATUk 1 1 k 1 where Uk 1 = (u1, .
.
.
, uk 1).
The mapping is T = Uk 1 1 k 1 : W   S, a point x   W is mapped to T x   S that is called x s image in S. S can be seen as a space spanned by basis (u1/ 1, .
.
.
, uk 1/ k 1).
The following theorem concludes that we can further bound the marginal distributions of two domains by the mapping T .
Theorem 3.2 Let x, x0 be two vectors in W and T x, T x0 be their images in the latent space S under the mapping T = Uk 1 1 k 1 : W   S. If kx   x0k2 <  ,   > 0, then kT x  
  2 k 1 j=1 T x0k2 <  rPk 1 norm kTj k2 = qPt Proof.
First, each column of T , Tj, j = 1, .
.
.
, k   1 has j = 1/ j .
Second, the Frobenius norm the the linear transform T can be expressed as follow: i=1 |(T )ij|2 = qkuj k2 2/ 2 kT k2
 k 1k2
 k 1 Xj=1 t Xi=1 |(T )ij|2 = k 1 Xj=1 t ( Xi=1 |(T )ij |2) = k 1 Xj=1 kTik2
 k 1 Xj=1
  2 i Now we are ready to bound the distance of two images T x, T x0 in S.
kT x   T x0k2

 2kx   x0k2 2 <  2 k 1 Xj=1
  2 j thus we have kT x   T x0k2 <  rPk 1 j=1
  2 j Basically, since we only choose the top k   1(k <= 10) eigenvectors, we can use only those eigenvectors whose corresponding eigenvalues are larger than 1.
Thus the distance of marginal distributions of two domains can be bounded in latent space.
Bound the conditional distributions in latent space In this section, we show that under the clustering assumption [12], the proposed retrieval strategy is optimal in making the conditional distributions p0(y|x) and p1(y|x) similar.
Then in the next section, we derive the Bayes risk of this retrieval process.
Following [12], the clustering assumption states that nearby points tend to have the same label.
More precisely, let  (x) be a regression function of y on x,  (x) = p(y = 1|x), and I( ) be the indicator function.
Then cluster assumption can be written as (C) Let Ci, i = 1, .
.
.
, k be clusters, then the function x   X   I( (x)   1/2) takes the above assumption is equivalent to p(y = y0|x, x0   Ci)   p(y 6= y0|x, x0   Ci).
So points in each cluster have the same p(y|x).
This is similar to the manifold assumption made in [1].
The assumption requires that  (x) vary smoothly on the support of p(x) which is a compact manifold, i.e.  (x) should not vary signi cantly in a small enough area on the manifold.
LatentMap follows these assumptions.
In reality, however, the assumptions may not hold exactly, so we employ an instance retrieval strategy to approximate the cluster structures.
For each in-domain instance x, p nearest neighbors from the out-domain are retrieved.
These neighbors are most likely to be in the same cluster as x.
Weighted voting is used to predict the label of x, where a closer neighbor has more impact on deciding the label of x.
This is consistent with the manifold assumption that  (x) of two points should be close when they are nearby.
We conclude that the conditional distributions p(y|x) of two domains are brought close within each cluster.
si er across Domains In previous section, we show that the generalization error of transfer learning can be bounded.
Since k-nn is employed as classi er in latent space S, we further analyze the Bayesian risk of k-nn in LatentMap.
We conclude that the risk can be bounded and the upper bound can be minimized when two conditional distributions of the two domains are positive correlated.
Assume that the marginal distributions pi(x), i = 0, 1 are continuous and are measurable with respect to a  - nite measure  .
Next we show that for any in-domain instance x, the nearest neighbor of x in the out-domain converges to x with probability one.
We need some assumptions: (D) Both in-domain and out-domain data lie in the same space.
This is true since both in-domain and out-domain data are in the latent space S. (E) Let Bx(r), r > 0 be the ball { x   X : d(x,  x)   r} centered at x with radius r. Bx(r) has non-negative probability measure,  r > 0, with respect to the in-domain marginal probability.
Lemma 3.1 Let in-domain instance x be drawn according to p1(x) and out-domain instances x1, x2, .
.
.
be drawn according to p0(x).
These instances are independent.
Let x0 ` be the nearest neighbor to x from the set {x1, .
.
.
, x`}.
Then x0 `   x with probability one.
Proof.
From the second assumption, for a  xed in-domain if the distance between point x   X, for any   > 0 , x and the nearest neighbor x0 ` from the out-domain samples {x1,       , x`} is larger than  , then all the points in {x1,       , x`} are outside the sphere Bx( ), i.e., p1{d(x, x0 `)    } = (1   p1(Bx( )))`   0 Consider a series of point sets: P` = {x1,       , x`}, with increasing `, d(x, x0 `) is monotonically decreasing.
So the nearest neighbor of x converges to x with probability one.
To  nd out the bound, we investigate the Bayes decision risk in the transfer learning setting.
We de ne the loss function as 0-1 loss: L(i, j) = 1, if i = j and 0 otherwise.
Under such settings, the Bayes decision rule is r  = min j
 Xi=0 p(y|x)L(i, j) = min{p(y = 0|x), p(y = 1|x)} = min{p(y = 0|x), 1   p(y = 0|x)} The Bayes decision rule minimizes the Bayes risk R , de ned as R  = E[r ] = Z r f (x)dx i=0 p(y = i)p(x|y = i).
We have the following theorem that is the counterpart of the analysis of k-nn in [3] in the transfer learning setting.
where f (x) = P1 Theorem 3.3 Let p( |y = i), i = 0, 1 be such that with probability one, x is either 1) a continuity point of p( |y = i), or
 (probability of error) is bounded as max{R 









 0   2cov(r  1 , r 
 where R  risk, respectively.
i , i = 0, 1 is the out-domain and in-domain Bayes Proof.
For a  xed in-domain instance (x, y), let (x0 `, y0 `) be the nearest neighbor of x in the out-domain, where y and y0 ` are independent.
Then the risk of misclassifying x is given by ` are the labels of x and x0 `, respectively.
y and y0 r(x, x0 `) = E[L(y, y0 `)|x, x0 = p1(y = 0|x)p0(y0 +p1(y = 1|x)p0(y0 `|x, x0 `) `] = p(y 6= y0 ` = 1|x0 `) ` = 0|x0 `) Similar to [3], here we wish to show that r(x, x0 to the random variable r  `) converges 0 with probabilities 1.
By Lemma 3.1 and the continuity of p( |y = i), with `)   p( |x) for both in-domain and out-probability 1, p( |x0 domain posterior probabilities.
Thus 0   2r  1 + r  1 r  r(x, x0 `)   r (x) = p1(y = 0|x)p0(y = 1|x) +p1(y = 1|x)p0(y = 0|x) = p1(y = 0|x)(1   p0(y = 0|x)) +(1   p1(y = 0|x))p0(y = 0|x) 1 = min{p1(y = 0|x), 1   p1(y = 0|x)} and r  Since r 
 min{p0(y = 0|x), 1 p0(y = 0|x)}, r (x) can be expressed as 1(1 r  r(x) = r  Because r(x, x0 nated convergence theorem, 0 )+(1 r  `) is bounded below 1, applying the domi-
1 )r  `)].
`)] = E[r(x)] r(x, x0 R = E[ lim ` 





 = E[r 






 1 (x)r  0   2cov(r  0(x)] 1 , r 
 1 (x)r  0(x)] (14) (15) (16) (17) where R  i Rewriting Equation (14), we have is the Bayes risk that is the expectation of r  i .
R = E[r  1 (x) + r 

 0(x)   2r  0 (x)(1   2r  1 (x)r  0(x)]

 similarly, we have R   R  max{R 



 0   2cov(r  1 , r 
 0 thus we have





 Since r  = min{p(y1|x), 1   p(y1|x)}, r  itive correlated, giving a positive cov(r  1 and r  1), r 
 0 can be pos- Instances ` u





















 Features











 |Fc|










 |F+|/|Fc|










 Data Sets Re vs Si Au vs Av C vs R C vs S C vs T R vs S R vs T S vs T O vs Pe O vs Pl Pe vs Pl

 Remark (1) If the two conditional distributions p0(y|x) and p1(y|x) are identical, the lower and upper bounds are the same as k-nn s (see [3]).
Note that the upper bound can not be better than k-nn s upper bound.
(2) In transfer learning, p0(y|x) 6= p1(y|x), the lower bound is the larger one of
 1}, which indicates k-nn can not perform better when training and test data are from di erent domains than from a single domain.
The upper bound says that if p0(y|x) and p1(y|x) are positively correlated, then the upper bound will be lower.
However, when p0(y|x) and y1(y|x) negatively correlates, i.e. two domains  concepts contradict, then the upper bound grows.
This is consistent with our intuition: transfer learning will bene t from two domains  similarity.
One of the crux in LatentMap is to compute the SVD of a large matrix A.
We don t have compute the exact SVD which requires O(m2n + mn2) computational complexity where m and n are number of rows and columns respectively.
Iterative algorithms for computing the  rst k   1 eigenvectors (and the corresponding eigenvalues) exist such as Lanczos method.
Recently, randomized SVD are proposed, such as the method in [6], it sample columns of a large matrix according to a suitable probability distribution then a smaller matrix is constructed on which SVD is applied.
This method provides a good approximation of the exact SVD and its running time is O(mn + n).
Another issue is  nd the top p nearest neighbor in ` out-domain instances for u in-domain data in the k dimension space, the computational complexity is O(k   |`|   |u|).
To demonstrate the e ectiveness of the proposed framework, we carried out experiments on several data sets frequently used in transfer learning.
Results show that La-tentMap can map the data to a signi cant low dimension space where the distributions of two domains are similar.
Missing values are dealt with properly, which improve the performance when the missing values are relatively copious.
We conducted experiments on three text data sets, all of which have di erent in and out-domain distributions.
We used SRAA (Simulated Real Auto Aviation), 20 newsgroups and Reuters-21758 as three main document classi cation tasks in this experiment.
The SRAA corpus contains
 ulated auto racing, simulated aviation, real autos, and real aviation.
The 20 newsgroups corpus contains approximately

 in a hierarchical manner.
Our task is to classify documents into one of the top-level categories in the hierarchy.
For example, in one of the 20 newsgroups task, we want to tell whether a document comes from category comp or rec.
Since the distributions of in-domain and out-domain data are required to be di erent, we split documents from each top category into two subcategories, one as the in-domain category and the other as the out-domain category.
For example, the out-domain data consists of documents from comp.windows.x and rec.autos, while the in-domain data contains documents from comp.graphics and rec.motorcycles.
All three main data sets are such organized that there are totally 11 transfer learning tasks in the experiment.
Raw text  les are transformed into word vectors.
All letters are turned into lower case and IDF-TF is used to produce term values.
We discard terms whose occurrences are less than 2.
Each word vector is normalized such that the length of the vector equals one.
The Lovins stemming scheme is used to stem words appearing in the text.
Simple tokening and stop word processing are used according to weka s default setting.
In cross-domain text classi cation problems, some features (terms) appear in both domains, while others appear only in the out-domain and missing in the in-domain, and vice verse.
Table 2 shows the statistics of the features of all 11 tasks.
As we can see, all the cross-domain tasks have missing values, i.e., F+ is not empty.
The last column in Table 2 shows the ratio |F+|/Fc.
In some tasks, this ratio is quite signi cant.
For example, in the task of Real vs Simulated, |F+| is over one forth of |Fc|.
In other tasks such as Orgs vs Places, the ratio is less than 1/20.
Baseline methods We compare LatentMap with various learning algorithms, including naive Bayes (NB), Logistic regression (LR), decision trees (C4.5) and SVM.
For the implementation of naive Bayes, Logistic regression and decision trees, we use the Weka package.
SVMlight is used as SVM classi er.
Procedural parameters are kept as default for all the classi- ers.
To predict missing values, we use SVR as our predictor and the implementation is provided by libSVM.
The traditional learning algorithms assume that the training and test data are governed by an identical distribution p(x, y) = p(x)p(y|x).
For this reason, we provide these learning methods with the original high-dimensional data.
In particular, the union of F+ and Fc are used as a whole and missing values are set to zero.
We note that LatentMap has two key steps.
First, the missing values in the in-domain data are predicted from the out-domain data.
And as such, the induced in-domain marginal is closer to the out-domain marginal in input space W .
Second, projecting the data onto a lower dimensional latent space S built using both the in and out-domain data not only reveals cluster structures but also provides tight bounds for the two conditionals in the latent space.
To see the e ectiveness of these two steps, we include the following two methods in our comparison:
 mension reduction (called k-nnReg, short for k-nn after Regression) 2) Mapping the data with missing values set to zero to latent space (called pLatentMap, short for partial LatentMap).
In this section, the experiment results of LatentMap against the baseline methods are provided.
The results show clearly that LatentMap is able to bring two domains  joint distributions p(x, y) closer via regression and latent space projection, giving rise to a e ective transfer learning framework.
Overall Performance Study The results of LatentMap and other traditional methods on three data sets are summarized in Table 3 with the best results in bold font.
It can be seen that in most of the tasks (10 out of 11) the performance is improved signi -cantly.
One exception is in task Comp vs Sci where the accuracy is slightly lower (within 3%) than the best of the baseline methods.
Since all the baseline learning algorithms assume that the underlying distributions p(x, y) of training data and test data are identical, they perform poorly on most of the transfer tasks.
For example, in the task Rec vs Talk from the 20 newsgroups data set, the lowest accuracy (around 60%) is achieved by Logistic regression and the decision tree, while the best learner (naive Bayes and SVMs) make correct predictions around 72%.
In this situation, LatentMap outperforms the best baseline method with an improvement of near 20%.
Over all, the smallest margin of improvement is around 2% on the task Orgs vs People.
Comparison of LatentMap and two simpler implementations is depicted in Figure 3.
In Figure 3(a), we compare LatentMap and k-nnReg.
It is clearly shown that by  lling up the missing values, either LatentMap or k-nnReg outperforms the best of the baseline methods in 9 out of 11 tasks.
Among these 9 tasks, LatentMap greatly outperforms k-nnReg in 6 tasks (task 2,3,5,6,7 and 8) and very close to k-nnReg in 2 tasks (task 10 and 11).
This con rms that the second step that further discovers cluster structures in the latent space and moves the induced p1(y|x) closer to p0(y|x) can greatly improve the accuracy.
Notice that while LatentMap has a lower accuracy than k-nnReg in tasks 10 and 11, it is still higher than that of the baseline methods.
In Figure 3(b), we show the e ect of multiple regression.
By making the conditional distributions of two domains similar via latent space mapping, LatentMap and pLatentMap together outperform the baseline methods 9 out of 11 tasks (tasks 1,2,3,5,6,7,8,10 and 11).
Furthermore, by predicting missing values through regression analysis, La-tentMap outperforms pLatentMap in 5 out of these 9 tasks (task 2,3,6,7,11) with other three close to pLatentMap (tasks
 tentMap performs approximately the same as pLatentMap.
By examining the last column of Table 2, we can see that LatentMap can achieve greater improvement on tasks where two domains overlap less or higher |F+|/|Fc| ratio (tasks Comp vs Talk and Rec vs Sci have the minimal feature set overlapping among six 20 Newsgroups tasks).
Our results show that LatentMap can e ectively lessen the discrepancy of two domains distributions.
In particular, latent space mapping that discovers cluster structures can greatly improve the performance while regression analysis guarantees the performance when a large number of values is missing.
Parameter Sensitivity There are two important parameters in the LatentMap algorithm: dimensionality of the latent space k and the number of documents in out-domain to retrieve for voting p. We

 y c a r u c c










 y c a r u c c







 y c a r u c c












 y c a r u c c



















 y c a r u c c


 y c a r u c c











 y c a r u c c










 Baseline pLatentMap LatentMap Baseline knnReg LatentMap









 y c a r u c c









 Domain Transfer Tasks (a) k-nnReg









 Domain Transfer Tasks

 (b) pLatentMap Figure 3: E ect of Two Key Steps
 Reuters (Orgs_vs_Places) SRAA (Auto_vs_Aviation) Accuracy wrt p Best Baseline Accuracy Accuracy wrt p Best Baseline Accuracy Accuracy wrt p Best Baseline Accuracy











 #Retrieved Out Domain Instances (a) Accuracy wrt retrieved out-domain instances #Retrieved Out Domain Instances #Retrieved Out Domain Instances SRAA (Auto_vs_Aviation)


 Accuracy wrt k Best Baseline Accuracy Accuracy wrt k Best Baseline Accuracy Accuracy wrt k Best Baseline Accuracy






















 #Dimension of Latent Space (b) Accuracy wrt dimensionality of latent space #Dimension of Latent Space #Dimension of Latent Space Figure 4: Sensitivity Study choose one task from each of the three main tasks to examine the sensitivity issue.
Since the distributions of in-domain and out-domain data are di erent, parameters chosen using cross-validation on the out-domain data will not work for the in-domain data.
In this experiment, parameter p varies from 5 to 200 with increment 10 and k varies from 2 to 6 with increment 1.
When p is changing, k is  xed at 5.
The resulting accuracy curves are depicted in Figure 4(a).
These accuracies are compared with that obtained from the traditional learning algorithm whose performance is the best.
From the  gure, it is obvious that the accuracy is improved as the number of retrieved out-domain instances increases, yet it remains stable after a certain threshold such as p = 150.
This con rms the cluster assumption.
That is, when more nearest neighbors are selected for voting, the e ect of minor mis-clustered out-domain instances will be diminished or canceled out, leading to higher accuracy.
The accuracy with respect to dimensionality of the latent space is higher than the best baseline classi er.
Thus it is not critical which value k takes.
When the underlying latent space s dimensions are changing, we always retrieve 50 instances from the out-domain, the resulting curves are shown in Figure 4(b)

 One main challenge of transfer learning is how to resolve and, in the same time, take advantage of the di erence between two domains.
Some are based on instance weighting strategy ([2, 4, 7, 11]).
For example, [4] adopts the boosting weight formula as the re-weighting scheme.
Some other methods base on dimension reduction, which usually map Methods



 LatentMap
 Re vs Si Au vs Av









 C vs R




 C vs S





 C vs T R vs S









 Reuters R vs T




 S vs T




 O vs Pe O vs Pl Pe vs Pl














 data to a new representation facilitating domain transfer ([9]).
Recently, [8] proposes a locally weighted ensemble framework to combine multiple models for transfer learning by dynamically assigning weights of a model according to a model s predictive power on each test example.
[10] proposes a information theory framework to address cross-language classi cation problem.
[15] addressed the problem of cross-domain text classi cation using PLSA (Probabilistic Latent Semantic Analysis) to bridge domain transfer.
We address transfer learning challenges in text classi cation and other related problems, where the spaces of two domains are at most overlapping, the marginal and conditional distributions are di erent, and the dimensionality can be extremely high.
We propose a framework (LatentMap) which draws joint distributions of two domains closer.
The missing values are  lled up to minimize the gap between marginal distributions, then the data is mapped to a latent space where both the marginal distribution and the relationship of two conditional distributions become easier to measure.
Then, transferable substructures can be easily identi ed in the mapped low dimensional latent space.
The dimensionality of the latent space is usually below 10, remarkably smaller as compared to the usual several thousands of the original space.
Experiment over 11 text domain transfer tasks shows that LatentMap works as expected and achieves great improvement (up to around 20%) compared to traditional learning algorithms including SVM.
Comparison with two simpler strategies (k-nnReg and pLatentMap) in the same transfer learning scenario shows that LatentMap can combine the advantages of both  lling up missing value and latent space mapping.
Parameters sensitivity analysis shows that LatentMap works well in very low dimensional spaces and is immune to the variation of the number of retrieved out-domain instances.
