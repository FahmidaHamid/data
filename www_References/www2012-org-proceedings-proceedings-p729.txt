[1] Wikipedia manual of style.

http://en.wikipedia.org/wiki/Wikipedia:
Manual_of_Style.

[2] A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. On

smoothing and inference for topic models. In UAI, 2009.

[3] I. Bhattacharya and L. Getoor. A latent dirichlet model for

unsupervised entity resolution. In SDM, 2006.

[4] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation.

JMLR, 2003.

[5] J. Boyd-Graber, D. Blei, and X. Zhu. A topic model for word

sense disambiguation. In EMNLP, 2007.

[6] R. Bunescu and M. Pasca. Using encyclopedic knowledge
for named entity disambiguation. In European Association
for Computational Linguistics, 2006.

[7] W. L. Buntine. Operations for learning with graphical

models. JAIR, 1994.

[8] C.-C. Chang and C.-J. Lin. LIBSVM – A Library for Support

Vector Machines. http:
//www.csie.ntu.edu.tw/~cjlin/libsvm/.

[9] S. Cucerzan. Large-scale named entity disambiguation based

on Wikipedia data. In EMNLP, 2007.

[10] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, A. Jhingran,
T. Kanungo, S. Rajagopalan, A. Tomkins, J. A. Tomlin, J. Y.
Zien, J. Y. Zien. Semtag and seeker: Bootstrapping the
semantic web via automated semantic annotation. WWW’03.

WWW 2012 – Session: Entity and Taxonomy ExtractionApril 16–20, 2012, Lyon, France737[11] I. Fellegi and A. Sunter. A theory for record linkage. Journal

of the American Statistical Association, 1969.

[12] S. Geman and D. Geman. Stochastic relaxation, gibbs

distributions, and the bayesian restoration of images. PAMI,
1984.

[13] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. In
Proceedings of the National Academy of Sciences, 2004.

[14] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Hidden topic

markov models. In AISTATS, 2007.

[15] G. Heinrich. Parameter estimation for text analysis.

Technical report, 2005.

[16] S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti.

Collective annotation of wikipedia entities in web text. In
KDD, 2009.

[17] R. Mihalcea and A. Csomai. Wikify! linking documents to

encyclopedic knowledge. In CIKM, 2007.

[18] D. Milne and I. Witten. Learning to link with Wikipedia. In

CIKM, 2008.

[19] T. P. Minka. Estimating a dirichlet distribution. Technical

report, Microsoft Research, 2003.

[20] R. M. Neal. Bayesian mixture modeling. In Workshop on

Max. Entropy and Bayesian Methods of Stat. Analysis, 1992.

[21] D. Newman, A. Asuncion, P. Smyth, and M. Welling.
Distributed algorithms for topic model. JMLR, 2009.

[22] A. M. Paul Komarek. Making logistic regression a core data

mining tool with tr-irls. In ICDM, 2005.

[23] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth,
and M. Welling. Fast collapsed gibbs sampling for latent
dirichlet allocation. In KDD, 2008.

[24] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, and P. Smyth. The

author-topic model for authors and documents. In UAI, 2004.

[25] L. Shu, B. Long, and W. Meng. A latent topic model for

complete entity resolution. In ICDE, 2009.

[26] S. Singh, K. Kumar, R. Rastogi, P. Sen, S. Sengamedu. Entity

disambiguation with hierarchical topic models. KDD’11.

[27] A. Smola and S. Narayanamurthy. An architecture for

parallel topic models. In VLDB, 2010.

[28] Y. Teh, M. Jordan, M. Beal, D. Blei. Hierarchical dirichlet
processes. J. of the American Statistical Association, 2005.

[29] H. Wallach. Topic modeling: Beyond bag of words. In

ICML, 2006.

[30] X. Wang, A. Mccallum, and X. Wei. Topical n-grams: Phrase

and topic discovery, with an application to information
retrieval. In ICDM, 2007.

APPENDIX

A.

INFERENCE FOR GROUP DISCOVERY
To set up Gibbs sampling for the model shown in Figure 6, we
need show how to sample a group for a reference given annota-
tions to all other references in the corpus. More precisely, we need
to compute for a given reference r, the conditional distribution
− repre-
Pr(re = g|r
sent annotations to the rest of the references in the corpus. For this,
we will need the joint probability distribution which can be simply
read off the plate model (Figure 6):

−; α, β, γ) , ∀g ∈ G, where r

− and g

−, g

Pr(r, g, Θ, π, Λ, ; α, β, γ)

(1)

= Pr(π|γ)" G
Yg=1
∝" G
Yg=1

πmg +γg −1
g

Pr(λg|β)#" D
Yd=1
#2
664

Yg=1

G,E

e=1

Pr(θd|α)Yr∈d
2
3
Yd=1
664
775

D

g∈Gd

Pr(g|θd, π)Pr(r|λg)#
3
775

ndg +α/|Gd|−1
θ
dg

λnge+βe−1
ge

where Gd is the set of groups used by document d to assign refer-
ences within it to, mg is the number of documents using group g
across the corpus, nge is the number of references which refer to

entity e assigned to g across the corpus and ndg is the number of
references in d assigned to g. Note that, the last term has a α
|Gd|
in the exponent because we assume the prior on θd is a symmetric
Dirichlet prior whose parameters add upto α.

We will now get rid of the useless parameters by integrating out

π, Λ and Θ. To do this, we will need the following identities:

G

E

Z
Z

Yg=1
Ye=1
Gd→∞Z Yg∈Gd

lim

πmg +γg −1
g

λnge+βe−1
ge

dπ = QG
dλg = QE

g=1 Γ(mg + γg)

e=1 Γ(nge + βe)

Γ(Pg mg +Pg γg)
Γ(ng +Pe βe)

(2)

(3)

ndg +α/|Gd|−1
θ
dg

dθd = Qg∈Gd
= Qg∈Gd
= Qg∈Gd

Γ(ndg + α

|Gd| )

Γ(ndg + α

Γ(Pg ndg + α)
Γ(Pg ndg + α)

Γ(ndg)

|Gd| )

Γ(nd + α)

(4)

and ﬁnally, setting

lim

Gd→∞

where, to set the limit, we have simply set α/|Gd| to 0. Note that,
in Equation 4, if we add up the terms ndg in the numerator then this
differs from the denominator by exactly α. We will later pretend
that α is the probability mass of picking a new group from θd [20].

We now substitute Equations 2, 3 and 4 into Equation 1:

g=1 Γ(mg + γg)

Pr(r, g; α, β, γ) ∝ QG
Γ(Pg mg +Pg γg)
Yg=1 QE
Γ(ng +Pe βe)

e=1 Γ(nge + βe)

G

(5)

Γ(ndg)

D

Yd=1 Qg∈Gd

Γ(nd + α)

Having described the joint distribution in terms of counts, to com-
pute Pr(re = g|r−, g−; α, β, γ) we now need to imagine a corpus
without r. This corpus’ joint distribution will be of the same form
as Equation 5 with a few terms missing. If r were annotated with a
group g already in use in the document r belongs to then there are
differences only in the second and third terms of Equation 5. On
the other hand, if g is not in use in the document r belongs to, then
there are differences in the ﬁrst and second terms only. Dividing
the joint of the full corpus by the joint of the corpus with r missing
gives us the conditional probability distribution we need [13]:

Pr(d ∋ re ← g|r

−, g

−; α, β, γ)

n−

+βdest(r)

g,dest(r)
n−

g +Pe βe

n−

g,dest(r)

+βdest(r)

n−
d,g

n−

g +Pe βe

α

m−
Pg m−

g +γg
g +Pg γg

if g ∈ Gd

if g /∈ Gd

∝ 8><
>:

To recover Λ, which will be used while testing with the topic
model described in Figure 7, we simply collect all g which have
been assigned at least one reference in the corpus and compute the
corresponding multinomials describing the groups:

∀g ∈ G s.t. ng > 0 : λg(e) ∝ ng,e + βe, Xe

λg(e) = 1

To learn the hyperparameters α, β and γ, we simply pretend that
at the time of updating these the number of groups in use were
the number of groups we had originally begun sampling with [20].
Thus, the group discovery process does not interfere with learning
the hyperparameters and we can simply use Minka’s updates [19].

WWW 2012 – Session: Entity and Taxonomy ExtractionApril 16–20, 2012, Lyon, France738
