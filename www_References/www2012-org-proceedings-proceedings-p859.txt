[1] M. Balabanovi´c and Y. Shoham. Fab: content-based,

collaborative recommendation. Communications of the
ACM, 40:66–72, March 1997.

[2] R. M. Bell and Y. Koren. Scalable collaborative

ﬁltering with jointly derived neighborhood
interpolation weights. In ICDM-07, 2007.
[3] J. Brown and P. Reinegen. Social ties and

word-of-mouth referral behavior. Journal of Consumer
Research, 1(3):350–362, 1987.

[4] C.-C. Chang and C.-J. Lin. LIBSVM: a Library for

Support Vector Machines, 2001.

[5] C. Cortes and V. Vapnik. Support-vector networks. In

Machine Learning, pages 273–297, 1995.

[6] P. Cui, F. Wang, S. Liu, M. Ou, and S. Yang. Who

should share what? item-level social inﬂuence
prediction for users and posts ranking. In
International ACM SIGIR Conference (SIGIR), 2011.
[7] K. Lang. NewsWeeder: Learning to ﬁlter netnews. In

12th International Conference on Machine Learning
ICML-95, pages 331–339, 1995.

[8] W.-J. Li and D.-Y. Yeung. Relation regularized matrix

factorization. In IJCAI-09, 2009.

[9] D. C. Liu and J. Nocedal. On the limited memory

BFGS method for large scale optimization.
Mathematical Programming, 45(1):503–528, Aug 1989.

[10] H. Ma, I. King, and M. R. Lyu. Learning to

recommend with social trust ensemble. In SIGIR-09,
2009.

[11] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: Social

recommendation using probabilistic matrix
factorization. In CIKM-08, 2008.

[12] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.

Recommender systems with social regularization. In
WSDM-11, 2011.

[13] A. Ng, M. Jordan, and Y. Weiss. On spectral

clustering: Analysis and an algorithm. In NIPS 14,
2001.

[14] K. B. Petersen and M. S. Pedersen. The matrix

cookbook, 2008.

[15] P. Resnick and H. R. Varian. Recommender systems.
Communications of the ACM, 40:56–58, March 1997.

WWW 2012 – Session: Collaboration in Social NetworksApril 16–20, 2012, Lyon, France867[16] R. Salakhutdinov and A. Mnih. Probabilistic matrix

factorization. In NIPS 20, 2008.

[17] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox:

large scale online bayesian recommendations. In
WWW-09, pages 111–120, 2009.

[18] E. B. Wilson. Probable inference, the law of

succession, and statistical inference. Journal of the
American Statistical Assoc. (JASA), 22:209–212, 1927.

[19] S. H. Yang, B. Long, A. Smola, N. Sadagopan,

Z. Zheng, and H. Zha. Like like alike: Joint friendship
and interest propagation in social networks. In
WWW-11, 2011.

APPENDIX
A. GRADIENT-BASED OPTIMIZATION

We seek to optimize sums of the objectives in Section 3

and will use gradient descent for this purpose.

For the overall objective, the partial derivative w.r.t. pa-

rameters a are as follows:

X

∂
∂a

X

∂
∂a

∂
∂a

Obj =

(16)
Anywhere a sigmoidal transform occurs σ(o[·]), we can

Obj i

i

i

λiObj i =

λi

∂
∂a

easily calculate the partial derivatives as follows
o[·].

σ(o[·]) = σ(o[·])(1 − σ(o[·]))

(17)
Hence anytime a [σ(o[·])] is optionally introduced in place of
o[·], we simply insert [σ(o[·])(1−σ(o[·]))] in the corresponding
derivatives below.

∂
∂a

Because most objectives below are not convex in U , V , or
w, we apply an alternating gradient descent approach [16].
In short, we take derivatives of U , V , and w in turn while
holding the others constant. Then we apply gradient descent
in a round-robin fashion until we’ve reached local minima
for all parameters; for gradient descent on one of U , V ,
or w with the others held constant, we apply the L-BFGS
optimizer [9] with derivatives deﬁned below.

Before we proceed to our objective gradients, we deﬁne

abbreviations for three useful vectors:

s = U x
t = V y
r = U z

sk = (U x)k; k = 1 . . . K
tk = (V y)k; k = 1 . . . K
rk = (U z)k; k = 1 . . . K

∂
∂U Obj pmcf =

∂
∂V Obj pmcf =

δx,y

1
2

ox,y

}|

1
CA

0
z
{
B@(Rx,y − [σ]
|
{z
}
xT U T V y)
δx,y[σ(ox,y)(1 − σ(ox,y))]tx
1
CA

0
{
z
B@(Rx,y − [σ]
|
{z
}
xT U T V y)
δx,y[σ(ox,y)(1 − σ(ox,y))]sy

}|

ox,y

1
2

δx,y

T

T

2

2

∂
∂U
= −

X
(x,y)∈D
X
(x,y)∈D

∂
∂V
= −

X
(x,y)∈D
X
(x,y)∈D

All matrix derivatives used for the objectives below can be
veriﬁed in [14].

∂
∂U Obj cp =

∂
∂U Obj ru =
∂
∂w

Obj rw =

∂
∂U
∂
∂w

1
2
1
2

tr(U T U ) =U

T

w

w = w

X

X

∂
∂U Obj rs =

∂
∂U
= −

x

X

z∈friends(x)
X

T
δx,yU (xz

δx,y

T
+ zx

)

∂
∂V Obj rv = V

1
0
CA
B@Sx,z − x
|
}
{z
T U T U z

2

1
2

∂
∂U Obj rss =

=

∂
∂U

X

x

∂
∂w

Obj phy =

∂
∂w

= −

∂
∂U Obj phy =

∂
∂U

∂
∂V Obj phy =

∂
∂V

x

z∈friends(x)

X

X

x

X
z∈friends(x)

x,z(x − z)
S+

T U T U (x − z)

1
2

T

x,zU (x − z)(x − z)
S+
z∈friends(x)
0
BB@Rx,y − [σ]
|
x,y)(1 − σ(o1

X
(x,y)∈D
X
(x,y)∈D

δx,y[σ(o1

1
2

δx,y

T

o1
x,y

z }| {
fx,y −[σ]x
{z

w

x,y))]fx,y

2

1
CCA

}
T U T V y

= −

δx,y[σ(o2

0
BB@Rx,y − [σ]w
|
x,y)(1 − σ(o2

T

fx,y − [σ]
{z

δx,y

T
x,y))]tx

0
BB@Rx,y − [σ]w
|
x,y)(1 − σ(o2

T

fx,y − [σ]
{z

δx,y

x,y))]sy

T

z

o2
x,y

1
{
}|
CCA
}
T U T V y
x

2

z

o2
x,y

1
{
}|
CCA
}
T U T V y
x

2

1
2

1
2

X
(x,y)∈D
X
(x,y)∈D

X
(x,y)∈D
X
(x,y)∈D

= −

δx,y[σ(o2

0
B@Px,z,y − x
|

{z
T U T

1
CA
}
diag(V y)U z

2

δx,z,y

1
2

T
δx,z,y diag(V y)U (xz

T
+ zx

)

X

(x,z,y)∈C
X

∂
∂U
= −

(x,z,y)∈C
X

In the following, ◦ is the Hadamard elementwise product:
∂
∂V Obj cp =

∂
∂V

T U T

(x,z,y)∈C

X

(x,z,y)∈C
X

1
2

(Px,z,y − x
0
B@Px,z,y − (
|
δx,z,y(s ◦ r)y

1
2

T

(x,z,y)∈C

=

∂
∂V
= −

2
diag(V y)U z)
1
CA

rz}|{
U z )

}
T V y

2

sz}|{
U x ◦
{z

δx,z,y

WWW 2012 – Session: Collaboration in Social NetworksApril 16–20, 2012, Lyon, France868
