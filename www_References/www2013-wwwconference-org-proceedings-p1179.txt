A Predictive Model for Advertiser Value-Per-Click in

Sponsored Search

Eric Sodomka∗
Brown University

Providence, Rhode Island

Sébastien Lahaie∗
Microsoft Research
New York, New York

Dustin Hillard∗
Microsoft Corp.

Redmond, Washington

ABSTRACT
Sponsored search is a form of online advertising where ad-
vertisers bid for placement next to search engine results for
speciﬁc keywords. As search engines compete for the grow-
ing share of online ad spend, it becomes important for them
to understand what keywords advertisers value most, and
what characteristics of keywords drive value.
In this pa-
per we propose an approach to keyword value prediction
that draws on advertiser bidding behavior across the terms
and campaigns in an account. We provide original insights
into the structure of sponsored search accounts that moti-
vate the use of a hierarchical modeling strategy. We pro-
pose an economically meaningful loss function which allows
us to implicitly ﬁt a linear model for values given observ-
ables such as bids and click-through rates. The model draws
on demographic and textual features of keywords and takes
advantage of the hierarchical structure of sponsored search
accounts. Its predictive quality is evaluated on several high-
revenue and high-exposure advertising accounts on a major
search engine. Besides the general evaluation of advertiser
welfare, our approach has potential applications to keyword
and bid suggestion.

Categories and Subject Descriptors
J.4 [Social and Behavioral Sciences]: Economics; K.1
[The Computer Industry]: Markets

Keywords
Sponsored search; Hierarchical model; Regret loss function

1.

INTRODUCTION

Online advertising spend exceeded $100 billion for the
ﬁrst time in 2012, with a signiﬁcant fraction going to ad-
vertising on search engines, a segment known as sponsored
search.1 Sponsored search refers to the practice of display-
ing ads alongside search results whenever a user issues a
query. Advertisers develop campaigns by selecting the key-
words they wish to advertise on and setting bids for those
keywords, with the placement and cost of ads determined
via an auction process [15]. Developing a high-performance
∗This work was done while the authors were at Yahoo! Inc.
1www.emarketer.com/Article.aspx?R=1009592

Copyright is held by the International World Wide Web Conference
Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink
to the author’s site if the Material is used in electronic media.
WWW 2013, May 13–17, 2013, Rio de Janeiro, Brazil.
ACM 978-1-4503-2035-1/13/05.

online ad campaign is a complicated task. Advertisers must
select keywords and bids to optimize returns, and it may re-
quire costly experimentation to uncover the keywords that
yield the highest proﬁts per click.

This paper proposes a hierarchical linear model to infer
an advertiser’s value per click on search terms. Our choice
of model is based on the hypothesis that keyword values are
linked to characteristics such as the demographic proﬁle of
users they attract. The growing trend towards geographic,
demographic, and even behavioral targeting is evidence that
these characteristics can be closely linked to returns [21].

Our model is composed of two key features. First, the
regression model uses an economically meaningful loss func-
tion to determine the regression coeﬃcients. Keyword auc-
tions are not truthful, so bids cannot be taken as proxies
for values—independent work has shown that values can be
as high as 125% of bids [7]. However, while an advertiser’s
actual value per click is unobservable, it is possible to ﬁt
a model of values such that the advertiser’s observed bid-
ding behavior, under the values imputed by the model, is as
close to rational as possible in the sense that it minimizes
foregone proﬁt, or regret. We explain that our regret-based
loss function falls in the well-known class of Bregman diver-
gences used in machine learning [2], and therefore allows for
eﬃcient convex optimization algorithms to ﬁt the model. To
deﬁne the loss function on each keyword we need a model
of advertiser beliefs about the clicks and costs that are ob-
tained at diﬀerent bids. For this we draw on the recent work
of Pin and Key [16] to develop click and cost function esti-
mates, and in doing so provide an independent evaluation of
their approach.

Second, our model exploits the hierarchical structure of
advertiser accounts to better estimate keyword values. Ad-
vertiser accounts consist of various levels of organization,
with diﬀerent decisions (such as demographic targeting or
bidding) being made at each level. The advertiser account
structure introduces commonality in keyword characteris-
tics, and therefore likely indicates commonality in keyword
values. We show that advertiser account structures demon-
strate the kind of skew in campaign sizes and bids that mo-
tivate hierarchical modeling, and that our hierarchical re-
gression model can improve predictions over a baseline that
only takes into account local account structure.

Information about bidder values can inform many aspects
of the keyword auction design,
including changes to the
ranking rules to improve revenue and reserve pricing poli-
cies [14]. We see two immediate applications of the value es-
timates provided by our approach: keyword and bid sugges-

1179tion. Search engines typically provide keyword suggestion
tools to help advertisers augment their campaigns. The cur-
rent state of the art provides keyword suggestions based on
statistical and semantic similarities using a campaign’s ini-
tial set of keywords [6], but we have not found any research
on how to ﬁlter and rank keyword suggestions according to
value to the advertiser. The value estimates from our model
provide a principled ranking criterion.

The only work on bid generation we are aware of is the
recent paper by Broder et al. [5], who use machine learning
to directly predict bids based purely on textual features of
keywords and ads. While they report good prediction perfor-
mance, their approach to recommending bids cannot react to
changes in opponent bids, and does not provide a criterion
for ranking suggestions—high bids may indicate the most
competitive keywords, rather than the most valuable. By
uncovering the primitives behind advertiser behavior (i.e.,
values) it becomes possible to automate the complete pro-
cess of keyword ranking and bidding.

To summarize, our work makes the following four contri-
butions, with the evaluation of our approach being the main
contribution.

• Original insights on the structure of sponsored search
accounts that motivate hierarchical regression model-
ing (Section 3).

• A regret-based modeling strategy that allows one to ﬁt
a model of unobserved values based on observed bids
and click-through rates (Section 4).

• An independent evaluation of the Pin and Key [16]
approach to modeling advertiser beliefs on click and
cost functions (Section 5).

• An experimental evaluation of our modeling approach
using real sponsored search account data (Section 6).

The remainder of this section reviews related work. Sec-
tion 2 provides the background on sponsored search needed
to follow the paper, while Section 7 concludes with directions
for improvement and future work.

Related work. Regression models of account perfor-
mance (e.g., click-through rates) have appeared in the mar-
keting literature for single accounts [11, 13]. Building such
models on the search engine side can be an insightful exer-
cise because, while the search engine may not have conver-
sion data, it may have much ﬁner-grained information about
the user traﬃc that visits the ads. The paper of Rutz and
Bucklin [17] is most closely related to ours in that it ex-
pressly addresses the problem of estimating values, in their
case as implied by conversions. Using data from the paid
search campaign of a hotel chain, they apply several logit
models to predict conversions based on features such as the
presence of brand or geographic information.

Our work also connects with the small but inﬂuential eco-
nomic literature on equilibrium models of sponsored search.
Edelman et al. [9] introduced the solution concept of envy-
free equilibrium, while Varian [20] showed how it could be
applied to derive bounds on values per click using bid data
from Google. Athey and Nekipelov [1] develop a model that
incorporates uncertainty in competitors and quality scores
in order to provide more reﬁned bounds and even points es-
timates. Pin and Key [16] develop a similar method that is
much more scalable, and we draw on their work to estimate
clicks and cost as functions of bid.

Figure 1: Hierarchical structure of a sponsored
search account.

2. SPONSORED SEARCH

We now describe the process of sponsored search and
the associated terminology used in this paper. At a high
level, the process of bidding in sponsored search proceeds
as follows: each advertiser speciﬁes a list of keywords (or
terms), a standing bid for each of those keywords, and a
speciﬁc advertisement (or creative) they wish to display
for each keyword. When a user issues a query to the search
engine, an auction is run among the relevant advertisers to
determine which ads appear, in what order on the page they
appear, and how much money each advertiser must pay.

The ranking of an ad is determined by a combination of
its bid and quality score, which is meant to capture the
ad’s relevance to the keyword; an important ingredient in
this score is the search engine’s estimate of the ad’s prob-
ability of being clicked, known as the click-through rate
(CTR). By convention, the payment scheme is per click,
meaning the advertiser only pays when its ads are clicked,
not simply when they are shown; the price of a click is often
called the cost per click (CPC). Advertisers receive some
expected value per click, which is private information, and
together with the advertiser’s estimates of CTR and CPC,
drives their decision making.

Advertisers can also make additional decisions beyond
what to bid to obtain ﬁner control over who sees their ads.
One popular reﬁnement is the choice of standard or ad-
vanced match. For example, if advanced match is enabled
for the keyword “sports shoes”, the search engine can show
the same ad for the related queries “running shoes” or “track
shoes” rather than just that exact query. Advertisers can
also impose targeting settings to specify that their ads
should only be shown to users from speciﬁc locales or de-
mographics. For example, an advertiser might target their
ad to only appear to male searchers in their thirties from
California. Finally, advertisers can set budgets to limit the
amount they spend in a given day; when the advertiser hits
its budget, it drops out of the day’s auctions. The stochas-
ticity these additional settings create across auctions can be
leveraged to obtain models of advertiser beliefs over clicks
and costs [1, 16].

Search engines provide advertisers with some organiza-
tional structure for their accounts to enable them to ap-
ply targeting and budgeting decisions to several keywords
at once. Figure 1 illustrates the hierarchical structure of a
sponsored search account, which is shared among the lead-
ing search engines. Keywords and creatives can are grouped

AccountCampaign 1Ad Group 1Keyword 1Keyword 2Ad 1Ad Group 2Keyword 1Keyword 2Ad 1Ad 21180together into ad groups; all keywords in the ad group dis-
play the same rotation of ads. Advertisers are also able to
place a single bid at the ad group level, and all keywords
with unspeciﬁed bids will default to that ad group bid. Ad
groups are grouped into campaigns; budgets and targeting
options are typically set at the campaign level. The hierar-
chical structure of these accounts should provide information
about an advertiser’s keyword values, since keywords in the
same ad group or campaign share features as deﬁned by the
account hierarchy. This account structure provides partial
motivation for hierarchical regression modeling.

3. PRELIMINARY ANALYSIS

Our data set consists of Yahoo’s sponsored search logs
over one month in the ﬁrst half of 2010. For each query in
the data set, we have information about the auction, dis-
played advertisers, and user (who issued the query). Auc-
tion information includes the query and the number of ads
displayed at the top and to the right of the page. Adver-
tiser information includes each advertiser’s bid, whether the
ad was displayed via exact or advanced match, the original
keyword each advertiser bid on, the displayed creatives, and
the predicted CTRs decomposed into position and adver-
tiser eﬀects. User information includes demographics such
as predicted age, gender and zip code.

We focus our investigation on 100 advertiser accounts sam-
pled from the set of all accounts. We obtained the total
impressions, clicks, and revenue (for Yahoo, or equivalently,
advertiser costs) for each account in Yahoo’s database and
found that, across accounts, impressions and revenue follow
lognormal distributions while clicks follow a Pareto distri-
bution (i.e., power law). In particular, the top 10% of ac-
counts by click volume are responsible for over 80% of the
monthly clicks, with a similar skew in the distributions for
impressions and revenue. Therefore, uniform sampling is in-
appropriate because the sample would be overwhelmed by
accounts with low click volume and revenue.

We found that click volume has a strong (linear) correla-
tion of at least 0.5 with impression volume and revenue.
Therefore, we choose to sample accounts proportional to
click volume so as to strike a balance between high-revenue
and high-exposure accounts. Note that these accounts are
not intended to be a representative sample of the entire ac-
count space. Rather, they are meant to be a representative
sample of the accounts for which it is most worthwhile to
provide value estimation services: their high revenue makes
them valuable to the search engine, and their high click
volume indicates their ads are relevant to users. Our 100
sampled accounts are responsible for hundreds of thousands
of clicks throughout the month, and taken together they
contain nearly 150K terms. The remainder of this section
provides a detailed examination of the structure of the ac-
counts in our sample, which serves to motivate and inform
our hierarchical modeling strategy.
3.1 Account Structure

A summary of the basic structure of the sampled accounts
is given in Table 1. First, note that even though we sampled
proportional to clicks, some accounts are very small:
just
three terms or one ad group. Further ﬁltering of accounts
is needed to restrict our attention to accounts with enough
terms to model. An important observation is that the me-
dian and mean number of terms per ad group is very small;

this makes sense considering the terms in an ad group share
the same ads. This suggests that ad groups are very homo-
geneous, and we would expect clicks from diﬀerent terms in
an ad group to have similar values to the advertiser. Once
one moves to the campaign level the number of terms starts
to be large enough to support model ﬁtting.

metric

min median mean max

terms per ad group

terms per campaign
ad groups per campaign

terms per account
ad groups per account
campaigns per account

1

1
1

3
1
1

2

30
10

400
300
20

6.3

145.4
30.9

1456.0
431.0
25.5

100

4000
400

10000
8000
100

Table 1: Summary of account structure across the
100 sampled accounts. The min, median, and max
have been rounded to one signiﬁcant ﬁgure.

A hierarchical structure to the data alone does not com-
pletely motivate hierarchical modeling. The latter is useful
when groups have uneven sample sizes and some groups are
small. In that case the group intercepts are pulled towards
those of larger groups, so that information from large groups
is taken into account for predictions within small groups,
which otherwise do not have enough data to support infer-
ence.

Figure 2 provides plots that describe the distributions of
ad group and campaign sizes across the accounts. Recall
that skewness is a measure of the asymmetry of a distri-
bution. We see that ad group sizes have a positive skew-
ness for almost all accounts, which indicates a long right
tail—there are a few large ad groups and many small ones,
rather than the reverse. The kurtosis is a measure of the
sharpness of a distribution’s peak. Most accounts have ad
group sizes whose kurtosis exceeds that of the normal dis-
tribution, which means accounts consist mainly of relatively
small ad groups. The same pattern holds for campaign sizes,
which show a positive skewness and high kurtosis in general,
though not to the extent of ad group sizes. This justiﬁes a
hierarchical model that includes every level of an account.

Figure 2: Skewness and kurtosis of group (ad group
and campaign) sizes for the 100 accounts. The refer-
ence lines show the standard moments of the normal
distribution: 0 for skewness and 3 for kurtosis.

skewnesskurtosis05010002468adgroups per campaign02468terms per adgroup11813.2 Tail Contribution

Our observations on account structure might lead one to
believe that only a few large ad groups matter in an account,
but this is incorrect. In aggregate, small ad groups typically
make up a substantial fraction of an account’s click volume
and revenue, so that it remains important to model adver-
tiser value on terms in small ad groups. To conﬁrm this, we
examine the tail of the distribution of impressions, clicks,
and revenue across the ad groups in an account, where the
tail is deﬁned as the bottom 80% of ad groups for the asso-
ciated metric. We deﬁne three regimes: the ‘Pareto’ regime
corresponds to a tail contribution of less than 20% to the
total; the ‘Long-Tail’ regime corresponds to a contribution
of at least 50%; and ‘Intermediate’ lies between the two.

We ﬁnd that around 30 accounts fall under the Pareto
regime for clicks or for revenue; thus, we cannot focus on
just the head (top 20%) of ad groups in general. There are
13 accounts that fall under the Long-Tail regime for both
clicks and revenue, and 19 accounts that are Long-Tail for at
least one of them. Furthermore, the accounts in the Pareto
regime are not responsible for much click volume or revenue
in our account sample (less than 15%). We conclude that
tail ad groups and terms, in aggregate, often generate much
of the click volume and revenue in an account, and it is
therefore important to develop models of value on all the
terms and ad groups.
3.3 Bid Variation

We now examine the bid variation in accounts. We ﬁrst
consider bid changes across time. For each term we counted
the number of distinct bids placed on that term throughout
the month, and averaged over terms in an account. The
numbers are low: the maximum number of distinct bids
is 60, or just 2 new bids per day.2 The relative standard
deviation (standard deviation over the mean, expressed in
percents) of the bids on a term, averaged over the terms
in an account, is also very small: the mean is 3% over the
accounts. For these reasons we choose to consider just the
average bid across time on a term—a similar simpliﬁcation
was made by Broder et al. [5], who developed regression
models of bids in sponsored search.

We next consider bid variation across the terms in an ac-
count; we would like to understand at what level (ad group,
campaign, account) the variation arises. To decompose the
variance among levels we make use of a simple hierarchical
model that also serves as a precursor to our later model for
values. Let bi be the bid on term i. We model the bids in ad
group j as drawn from a normal distribution with mean bj.
The means bj of the ad groups in a campaign k are them-
selves drawn from a normal distribution with mean bk, and
the campaign means are normal with mean bh:

bi ∼ N (bj, σ2
1),

bj ∼ N (bk, σ2
2),

bk ∼ N (bh, σ2
3).

Here it is implied that term i is in ad group j, which is in
campaign k. The proportion of variance at the ﬁrst (i.e.,
ad group) level is σ2
3), and similarly for the
other levels. For each account, the hierarchical model was
ﬁt using the lmer function in R. The assumption of normal

1 + σ2

1/(σ2

2 + σ2

2This does not necessarily mean that no interesting bidding
behavior occurs across time—advertisers could be using in-
tricate strategies that cycle through diﬀerent bids. This has
been observed in some early studies [8].

Figure 3: Proportion of bid variance that arises
among terms within ad groups, ad groups within
campaigns, and campaigns within accounts.

distributions amounts to ﬁtting a model with squared loss.
Figure 3 gives the variance proportions for the sampled ac-
counts. We ﬁnd a wide spectrum of proportions: for some
accounts almost all the variance occurs at the ad group level,
while for others it is all at the campaign or account level.

Figure 4: Relative standard deviation of bids. For
each account, we take the relative standard devia-
tion of bids within its ad groups, then average these
over the ad groups in the account. We do the same
for campaigns. The ﬁnal line lists the relative stan-
dard deviation of bids over terms in each account.

The amount of bid variation at each level is also informa-
tive. For each account we looked at the relative standard
deviation of bids within each ad group, and averaged over
ad groups. We did the same looking at bids within cam-
paigns, and within the account. The results are given in
Figure 4. We ﬁnd that relative standard deviation within
ad groups is small: the median across accounts is 5%. The
median within-campaign and within-account relative stan-
dard deviations are 26% and 41% respectively. This seems
to suggest that many advertisers make their bidding deci-
sions at the ad group level, and the bids on terms within an
ad group do not stray far from their ad group baseline. The
most interesting and challenging inference problem in such
cases is to predict ad group-level average values rather than
term-speciﬁc values.

AccountsVariance Proportion020406080100AdgroupCampaignAccountBid Relative Standard DeviationAccountCampaignAdgroup05010015011824. HIERARCHICAL MODEL

The basis for our value estimation approach is a simple
quasi-linear model of advertiser utility on each term. We fo-
cus on an individual advertiser with n terms in its account,
with i used to index terms. Let vi be the advertiser’s value
per click on term i. Let ci : R → R∪{+∞} be an extended
real-value function that gives the expected cost per impres-
sion ci(xi) of obtaining a click-through rate (CTR) xi on
term i. The purpose of introducing +∞ into the range is to
implicitly encode the domain of ci as {xi ∈ R : ci(xi) <
+∞}, following standard conventions in convex analysis.
For instance, negative CTRs are infeasible and would have a
cost of +∞. We assume that ci is convex and diﬀerentiable.3
The cost functions estimated later in Section 5 are in fact
piece-wise linear, but by selecting a subgradient at the (ﬁ-
nite) number of points of non-diﬀerentiability, the following
applies with minor technical changes.

We assume that the advertiser’s utility for clicks on a term

is quasi-linear in cost, so that it takes the form

ui(xi, ci) = vixi − ci(xi).

(1)

Letting wi be the search volume for term i, the aggregate
utility to the advertiser of obtaining the vector of CTRs x =
(x1, . . . , xn) across the terms in its account, given the vector

of cost functions c = (c1, . . . , cn), is u(x, c) =(cid:80)n

i=1 wiui(xi, ci).

Our goal is to obtain a regression model of the vi values, but
these are not observable. Instead, we observe the CTRs xi
chosen by the advertiser via its bids on each term, and we
can estimate the cost functions ci.
4.1 Loss Function

A naive approach to developing a regression model of val-
ues would be to ﬁrst estimate the values vi on each term, and
then run a standard regression on top of these (e.g., using
least squares). Indeed, from the utility form (1) and our con-
vexity assumptions on ci, it follows that at the advertiser’s
observed choice of CTR xi we must have vi = ∇ci(xi) un-
der utility-maximizing behavior (where ∇ refers to the ﬁrst
derivative).

The issue with this approach is that typical loss functions
like squared error ﬁnd little justiﬁcation in economic set-
tings like sponsored search: variance in the estimated values
within an ad group may be a result of optimization error
on the part of the advertiser, rather than statistical error,
and there is no sound basis for optimization errors to be
Gaussian [19]. Here we develop an economically meaningful
loss function that is also computationally appealing because
the problem of model ﬁtting remains convex, as with con-
ventional statistical loss functions. Under our loss function,
value estimation occurs in tandem with model ﬁtting, rather
than as a preliminary step.

The idea is to draw on the notion of Bregman divergence,
which has seen increasing attention in machine learning [2].
First, we need the concept of a convex conjugate. In what
follows, we will suppress the term subscript i on values,
costs, and CTRs for clarity. The dual space R∗ is the vector
space of all linear functions on R. Observe that values can

3Since agent values are linear in CTR, replacing ci with its
convex envelope (the largest convex function it dominates)
does not change the agent’s utility maximization problem.
Therefore, the assumption that ci is convex is without loss
of generality when it comes to analyzing an agent’s choice
of bid and CTR on a keyword.

be identiﬁed with elements of the dual space. The convex
conjugate c∗ : R∗ → R of cost function c is deﬁned as

∗

c

(v) = sup
x∈R

{vx − c(x)} .

(2)

Observe that c∗ is convex, even if c is not, because is it
the supremum of aﬃne functions. A Bregman divergence
is deﬁned with respect to a strictly convex, diﬀerentiable
function, in our case the conjugate cost function c∗. Given
c∗, the divergence between two values v and v(cid:48) in its domain
is deﬁned as

Dc∗ (v

(cid:48)||v) = c

∗

(cid:48)

) − c

∗

(v) − ∇c

∗

(v) · (v

(cid:48) − v).

(v

(3)

This is the loss function between values that we propose.
Here v(cid:48) should be viewed as the estimated value, and v the
true value (not directly observed).

The following proposition collects some standard facts
about Bregman divergence (e.g., see [2]) that serve to mo-
tivate this choice of loss and explain how it is evaluated in
practice.

Proposition 1. Let c∗ be a convex and diﬀerentiable func-
tion. The associated Bregman divergence Dc∗ satisﬁes the
following properties.

1. Dc∗ (v(cid:48)||v) ≥ 0 for all v, v(cid:48) in the domain of c∗, with

equality if v = v(cid:48).

2. Dc∗ (v(cid:48)||v) is convex in its ﬁrst argument.
3. Dc∗ (v(cid:48)||v) = Dc(x||x(cid:48)) where x, x(cid:48) are such that

v = ∇c(x) and v(cid:48) = ∇c(x(cid:48)).

Property 1 conﬁrms that Dc∗ is a sensible loss function.
Bregman divergence in fact generalizes losses such as squared
loss, KL-divergence, and Itakura-Saito distance, which can
all be recovered with a suitable choice of convex function
c [2]; for example, squared loss corresponds to a quadratic
cost function. Property 2 ensures that is it computationally
tractable to ﬁt the estimate v(cid:48). Finally, Property 3 shows
that our loss function can be equivalently viewed as a loss
on CTRs, which are observable.

Economic Interpretation
To see the economic motivation for this loss function, let
v(cid:48) = ∇c(x(cid:48)) be the value for which the choice of CTR x(cid:48) is
optimal. Then by Property 3 loss function (3) evaluates to

Dc(x||x
(cid:48)

) − ∇c(x
) = c(x) − c(x
(cid:48)
(cid:48) − c(x
)] − [v
(cid:48)
(cid:48)
x

= [v

(cid:48)

) · (x − x
(cid:48)
x − c(x)].

(cid:48)

)

(4)

(5)

This is the regret from bidding so that CTR x is received,
rather than the optimal choice x(cid:48) when the advertiser’s value
is v(cid:48). Stated another way, in order to minimize the loss we
seek an estimated value v(cid:48) such that the advertiser’s regret
from obtaining the observed CTR x under this value is min-
imized.

Varian [19] has proposed several ‘economically meaning-
ful’ loss functions of this sort for both parametric and non-
parametric models. For parametric models of production
analysis, he proposes a loss function which captures the de-
gree to which the observed choice behavior fails to maximize
the estimated production function, which is precisely the re-

1183gret in (5).4 In microeconomics the conjugate (2) is known
as the indirect utility function; intuitively, it gives the maxi-
mum utility that an advertiser with value v can achieve. Our
loss function is the Bregman divergence associated with in-
direct utility.
Evaluating Loss
The loss function (3) is deﬁned in terms of unobservable
values, whereas its alternate form (4) is deﬁned in terms of
the observable CTR and cost function, but does not allow
one to incorporate a linear model for value. Instead we will
work with the following form for loss. Let x be the optimal
choice of clicks when the advertiser’s value is v; from (2) and
Danskin’s theorem [3, p. 717] we have x = ∇c∗(v). Thus,

Dc∗ (v

(cid:48)||v) = c
= c

= c

∗
∗
∗

(cid:48)
(cid:48)
(cid:48)

(v

(v

(v

∗

∗

(v) − ∇c

) − c
(v) · (v
) − [vx − c(x)] − x · (v
) − xv

+ c(x).

(cid:48)

(cid:48) − v)
(cid:48) − v)

Now, if we have a linear model for the ﬁtted value, so that
v(cid:48) = a · β where a is the feature vector and β are the coeﬃ-
cients, then the loss function can be written as

Dc∗ (v

(cid:48)||v) = c

∗

(a · β) − x(a · β) + c(x).

(6)

This loss is convex in the parameters β and is formulated
in terms of the observed chosen CTR x, the cost function c,
and its conjugate c∗. The conjugate c∗ of a one-dimensional
convex cost function can be computed in linear time using
elementary convex hull algorithms. Section 5 addresses the
problem of estimating the cost function c.
4.2 Levels

We now describe the aggregate loss function across all
terms in an account. There are three levels in an account:
(1) terms within ad groups, (2) ad groups within campaigns,
and (3) campaigns within the account. Our regret-based loss
function applies to the ﬁrst level, while standard squared
loss is used at higher levels for regularization. We index the
terms, ad groups, and campaigns in the advertiser’s account
by i, j, and k respectively, and with an abuse of notation
write i ∈ j to denote that term i belongs to ad group j, and
j ∈ k to denote that ad group j belongs to campaign k.

We implicitly observe vi for each term i via the chosen
CTRs xi. Let ai be the vector of predictors for term i and
let vj be the intercept for ad group j. Let β1 be the model
coeﬃcients at the ad group level. Writing Di for the diver-
gence (3) associated with c∗
i , the aggregate loss at the ﬁrst
level is

Di(vj + ai · β1||vi).

(7)

(cid:88)

(cid:88)

(cid:88)

j∈k

i∈j

k

As explained above each term in the summation can be eval-
uated in practice via (6), and because Di is convex in its ﬁrst
argument it is also convex in the parameters β1 and vj.

Analogous aggregate loss functions could be derived for
the second and third levels, but it is unclear whether ad-
vertisers reason in terms of regret at those levels. Another

4Our model of advertiser utility is closer to producer theory
than consumer theory. We have surveyed the economet-
ric literature and, despite Varian’s convincing arguments, it
seems the idea of ‘economic’ loss functions has not caught
on, and standards like squared loss are still in favor. This
may be due to computational reasons: not all of Varian’s
loss functions are convex in the estimated parameters.

problematic aspect of using regret-based loss at higher lev-
els is that parameters appear in the second argument, where
convexity is not guaranteed. Because the purpose of higher-
level loss is regularization, we therefore use standard squared
loss. Again, let aj be the vector of predictors for ad group
j and let vk be the intercept for campaign k. Let β2 be the
model coeﬃcients at the campaign level. The loss at this
level is

(vk + aj · β2 − vj)2.

(8)

(cid:88)

(cid:88)

j∈k

k

(cid:88)

In the ad group-level loss (8), the “observation” vj is in fact
the intercept ﬁt at the lower level in (7).

We use the index h to refer to the intercept at the third

level (i.e., account level), where we also use squared loss:

(vh + ak · β3 − vk)2.

(9)

k

The “observation” vk here in fact corresponds to the inter-
cept from (8). Finally, the aggregate losses from the three
levels are summed up to yield the complete loss used to
ultimately ﬁt the model. Again we stress that only the ag-
gregate regret (7) captures the performance of the model;
losses at higher levels of the hierarchy are for regularization,
as motivated by our observations on account structure in
Section 3. Introducing (8) pulls all the vj ad group inter-
cepts towards a common intercept vk for the campaign, so
that information is shared across ad groups in the campaign,
and similarly for (9). This is completely analogous to the
motivation for multi-level models with standard losses like
least-squares [10].

The aggregate loss combined over all levels is convex in all
model parameters, so ﬁtting the model is straightforward via
algorithms like stochastic gradient descent. We implemented
an incremental subgradient algorithm [3, p. 614] similar to
the stochastic gradient descent used in [5] to develop a model
of bids. While the implementation details are beyond the
scope of this paper, we note that running times to ﬁt the
model ranged from under a second to around 25 minutes
on the largest account (we did not attempt to optimize the
code). The bottleneck is in reading in the account data
and setting up the data structures rather than running the
algorithm.

5. COST FUNCTION ESTIMATION

The preceding sections developed a linear model for esti-
mating a term i’s value-per-click from its features. The pro-
posed method required knowledge of advertiser costs ci(xi)
for a given observed (implicit) choice of click-through rate
xi; we now describe an approach to estimating this cost
function ci(xi).

As discussed in Section 2, there are numerous factors that
cause variability in the competitive landscape across auc-
tions. These factors make it impossible to compute exact
costs for the next upcoming search, making cost functions
based on individual auctions (as studied by Edelman et al.
[9] and Varian [20]) less realistic for our domain. The obser-
vation from Section 3 that advertisers infrequently modify
their bids further suggests that advertisers make decisions
at a coarser level of granularity than the auction level. Fol-
lowing recent approaches [1, 16], we assume the advertiser
creates a stochastic model of costs based on a distribution
over the observed competitive landscape.

1184Speciﬁcally, we use a version of the cost function described
by Pin and Key [16].5 The model is from the perspective of a
single advertiser, with each term in the advertiser’s account
treated independently. Each opponent participating in an
auction is assumed to stochastically submit a bid i.i.d. from
a known probability density function; let F (b) be the corre-
sponding c.d.f. that gives the probability an opponent sub-
mits a bid less than b. These bids are assumed to be weighted
to adjust for diﬀerences in opponent quality score6. Varia-
tion in F (b) comes from both the variation in these quality
scores across searches and from the search-speciﬁc set of
competing advertisers (determined by standard match, ad-
vanced match and targeting settings described in Section 2).
Let k be a possible slot, where k = 0 is the top slot.
Given the cumulative distribution over opponent bids F (b),
an advertiser can compute the probability φ of appearing in
the kth slot in a single auction when there are n opponents
and the advertiser places a bid b:

φ(k, n, b) =

(1 − F (b))kF (b)n−k

(10)

(cid:32)

(cid:33)

n
k

In words, this computation is the number of ways k oppo-
nents could be chosen to appear in slots above the advertiser,
times the probability that those k opponents appear in slots
above the advertiser, times the probability the remaining
n − k opponents appear in slots below the advertiser.

For a given term, let pn be the probability that n oppo-
nents participate in each auction, where n is at most N .
Each slot k has a known click-through rate sk, where sk = 0
if k exceeds the number of available slots. Assuming the
advertiser bids above the known reserve price r, the adver-
tiser’s expected click-through rate ˆxi(b) as a function of its
bid b is computed as follows:

N(cid:88)

n(cid:88)

n=0

k=0

ˆxi(b) =

pnφ(k, n, b)sk

(11)

Similarly, cost per impression ˆci(b) is computed as:

N(cid:88)

n(cid:88)

n=0

k=0

ˆci(b) =

pnφ(k, n, b)sk

(cid:90) b

r

b −

(cid:18)
(cid:124)

(cid:19)
(cid:125)

(12)

F (t)n−k
F (b)n−k dt

(cid:123)(cid:122)

The additional bracketed term is the expected cost per click,
given the advertiser appears in the kth slot against n bidders
with a bid of b and reserve price r.

A given bid b is thus associated with an expected cost per
impression ˆci(b) and an expected click-through rate ˆxi(b).
The expected cost per impression ci(xi) as a function of
click-through rate xi is then reconstructed by pairing the
calculated ˆxi(b) and ˆci(b) values for each possible bid b:

ci(xi) = ˆci(ˆx

−1
i

(xi))

(13)

5Pin and Key present several models of increasing complex-
ity; we use their most complex model that does not consider
distributions over promoted reserve price (i.e., reserve price
for ads shown at the top of the page), since this data was
unavailable.
6Recall from Section 2 that advertisers are ranked by their
bid b times their quality score w. The opponent bid distribu-
tion F (b) created by advertiser a gives the probability that
advertiser a’s weighted bid wb is greater than the opponent’s
weighted bid w(cid:48)b(cid:48).

This version of the Pin and Key model matches our en-
vironment closely but not exactly. First, reserve prices are
not known exactly and are not necessarily of a ﬁxed value in
our domain. Second, the slot click-through rates depend not
only on the slot k, but also the number of promoted slots
(i.e., shown at the top) and the number of opponents. To
extend the model, we take additional summations in Equa-
tions (11) and (12) over possible reserve prices r and possible
available promoted slots d, and slot click-through rate sk is
instead computed as skdn, which gives a diﬀerent slot click-
through rate depending on the competitive landscape.
5.1 Distribution Estimation

Thus far we have expressed the cost function ci(xi) in
terms of other stochastic variables (such as distributions over
participants and opponent bids), but we have not speciﬁed
how these other distributions are obtained. We follow the
approaches taken by Athey and Nekipelov [1] and Pin and
Key [16], in that distributions are created from actual spon-
sored search records for the given time period. For each
sampled account, we get all actual user searches for which
a term in the account appeared, and then get all competing
advertiser bids for those searches. For each term, histograms
are created of opponent weighted bids, advertiser reserve
prices, number of opponents and number of available pro-
moted slots, and slot click-through rates. Bids and reserve
prices are discretized by ten cents, which is the minimum
bid increment on Yahoo.
5.2 Cost Function Evaluation

We now evaluate the accuracy of our cost function, fol-
lowing the same evaluation technique used by Pin and Key
[16] for comparison purposes. For each term i in the 100
sampled accounts, we create predictions of expected CTR
ˆxi(b) using the method described above, and evaluate these
predictions against two metrics, both based on the actual
sponsored search data from individual auctions.

First, we compare against the expected slot CTR, given the
advertiser’s historically realized slots for that term. This
quantiﬁes the error associated with making the indepen-
dence assumptions described above and with assuming a
ﬁxed bid (taken to be the average) over the entire duration
of the month. Put another way, this gives an idea of how ac-
curately an advertiser might be able to predict CTR, given
the limitations of not knowing a priori the realized values
of stochastic processes. Figure 5(a) shows the relative error
in estimating CTR. Each term is a single data point, and
terms are sorted on the x-axis by the number of actual clicks
received for that term. We observe that the mean and me-
dian ratios between predicted and actual expected CTR are
0.99 and 0.97, respectively. As observed by Pin and Key,
we also ﬁnd that the variance in relative error across terms
decreases as the number of clicks increases.

Next, we compare our estimates to the realized slot CTR.
This takes into account the fact that our predictions of an
advertiser’s CTR may not be perfectly accurate, even if we
know exactly the slot that the advertiser appears in. Fig-
ure 5(b) shows, for each term, the estimated clicks for plac-
ing the bid versus the actual number of clicks. We see a bias
in these predictions, as our model is under-predicting the
amount of clicks that will occur. This can be explained by
our account selection process: since accounts with the most
clicks were favored in the sampling, these sampled accounts

1185(a) Predicted versus expected CTR.

(b) Predicted versus realized CTR.

Figure 5: Accuracy of the cost function’s CTR estimates when compared to the expected and realized CTRs
from actual sponsored search logs. Plots show a random sub-sampling of 10% of all terms for clarity. Model
accuracy is shown with respect to the number of clicks received by each term. The x-axis is on a logarithmic
scale and labels ommitted for conﬁdentiality reasons.

are more likely to have received more clicks than expected.
Advertisers whose terms experience this bias would likely be
unable to predict such a deviation from a learned model of
their click-through rate, particularly if the increased amount
of clicks is due to stochastic user behavior and not any
changes made to the advertiser’s ads or keywords.

Despite independence assumptions about the distributions
of opponent bids and the number of participants, our results
support the ﬁndings of Pin and Key [16] that their model
estimates click-through rates with accuracy comparable to
other methods proposed in the literature. While we use their
model because of its computational tractability and simi-
lar accuracy to more complex methods, any advertiser cost
function could similarly be substituted into Equation (6) to
regress on advertiser values-per-click.

6. EXPERIMENTAL EVALUATION

We ﬁt a separate model to each account to allow for the
fact that the eﬀect of the features used might vary across
accounts. We evaluate the predictive performance of our ap-
proach via synthetic leave-out estimates: for each account,
some ﬁxed proportion of the terms, ad groups, or campaigns
is used for training, and we then record the aggregate re-
gret of the ﬁtted model’s value predictions on the remaining
terms used for testing. The regret on a term is given by (6)—
observe that this does not include the squared loss (8–9) at
the ad group and campaign levels used for model ﬁtting,
which is only for regularization. We considered training-
testing splits where 50%, 75%, and 90% of the terms, ad
groups, or campaigns are randomly selected for training. For
each combination of leave-out and split we did 10 runs and
averaged the results over the runs. Because some accounts
have too few terms to properly develop a value model, we re-

stricted our attention to the 88 accounts in our sample with
at least 40 terms. The remaining accounts on which we
trained and made predictions consisted of nearly 150 thou-
sand terms.

As a baseline prediction for a term’s value, we used the
average value of the other terms in its ad group, as predicted
by the Pin and Key approach. Speciﬁcally, the value on each
i = ∇ci(xi), where ci is the cost
other term i is given by v(cid:48)
function derived as explained in Section 4.1 and xi is the
observed CTR. The regret on the term is again evaluated
using (6). If there are no other terms in the given term’s
ad group or campaign present in the training set (e.g., when
leaving out ad groups or campaigns), we use the average
value over terms in the same campaign or over the account,
respectively.
6.1 Features

We used the following features to develop our models. Fea-
tures can arise at the term, ad group, and campaign levels,
but there were no obvious features to use at the campaign
level so only an intercept is present there for regularization.
The following predictors (or categories of predictors) were
used at the term level. As indicated, some predictors are
logged to account for skew in their distributions. No other
transformations were made to these predictors and no inter-
action eﬀects were introduced.

age ﬁve separate predictors indicating the percent of oﬀers

to users in their twenties, thirties, etc.

gender three separate predictors indicating the percent of

oﬀers to male and female users, or gender unknown.

exact match percent of opponent ads presented due to ex-

act rather than advanced match.

Actual Number of ClicksEstimated CTR / Actual Expected CTR1/41/31/21234Actual Number of ClicksEstimated CTR / Realized CTR1/41/31/212341186user click propensity (log) mean of a metric quantifying,
for each keyword the ad matches to, the propensity of
users who search on the keyword to click on ads—
see [12].

query length mean word length of the user queries leading

to the oﬀer.

north state ﬁve separate predictors indicating the percent
of searches for the keyword yielding a page with zero,
one, two, three, or four ads at the top of the page.

competitors (log) mean number of competitors on the key-

word.

The ﬁrst four features capture aspects of a term that may
have inherent value to the advertiser (e.g., the demographic
proﬁle of searchers), while the last two capture the level of
competition on the term, which may correlate with unseen
aspects of the term that drive value. Statistics on gender or
age come from the self-reports of searchers who are logged
in to their Yahoo account.7 We next turn to the predictors
at the ad group level.

creatives number of creatives in the ad group.

linead length word length of the keyword bid on.

title length word length of the ad title.

These predictors give some indication of the speciﬁcity of the
advertisement, which may correlate with value. To clarify
the diﬀerence between ‘linead length’ and ‘query length’, the
former corresponds to the length of the keyword speciﬁed
for the ad group, while the latter corresponds to the mean
length of the query actually matched to, which can vary in
the case of advanced match.

In terms of goodness of ﬁt, we found that the most pre-
dictive feature varied across accounts. This result is not
surprising, as diﬀerent accounts do not necessarily represent
companies from the same industry and thus could have dif-
ferent levels of importance for diﬀerent user characteristics.
6.2 Results

When evaluating the model and baseline on the testing
set terms we recorded the average regret per term for each
and compared them against each other. We consider two
comparison metrics:

absolute improvement = baseline regret - model regret
baseline regret - model regret

relative improvement =

baseline regret

A positive value for either indicates that the model out-
performed the baseline, and vice-versa. Note however that
there is an asymmetry in reporting relative improvement:
its maximum value is 1 (regret is always non-negative), or
100%, but it can take on arbitrarily large negative values
as the baseline regret approaches zero. As a result, taking
the mean of the relative improvement across accounts leads
to a misleading assessment of model performance: the re-
sult is highly negative for every choice of split and choice of
leave-out (i.e., term, ad group, or campaign) due to outliers.

7Personal account information was not accessed for this
study. Anonymized, aggregate statistics on gender and age
were already available in the sponsored search logs.

To give a clearer picture of the distribution, Table 2 sum-
marizes for each regime the relative improvement quartiles,
which are robust to outliers. Recall that the ﬁrst quartile
cuts oﬀ the lowest 25% of the data, the third cuts oﬀ the
highest 25%, and the second quartile is the median. The
interquartile range (diﬀerence between third and ﬁrst quar-
tiles) is a non-parametric analog of standard deviation.

Leave-out Split

Q1

term

ad group

campaign

50% -130.9
75% -103.9
90% -121.2

50% -107.4
-87.5
75%
90%
-79.3

50%
75%
90%

-24.6
-23.1
-26.9

Q2

-3.9
4.1
5.5

-5.1
6.7
4.6

13.9
14.4
11.3

Q3

30.8
33.8
31.4

29.8
36.5
40.2

38.6
43.4
45.6

IQR

161.8
137.7
152.7

137.2
124.1
119.6

63.2
66.5
72.5

Table 2: Quartiles of percent relative improvement
over the baseline, across accounts, together with in-
terquartile range.

The clearest observation is that the relative performance
against the baseline improves as one moves up the leave-out
hierarchy from terms to ad groups to campaigns. We at-
tribute this to the fact that model performance remains rel-
atively stable in each case while the baseline necessarily de-
grades. There is a monotonic improvement in model perfor-
mance as the size of the training set increases, as expected,
but relative improvement is not necessarily monotone be-
cause the baseline also improves. We see from the second
quartile (median) that the model improves on the baseline
for over half the accounts in all regimes except leaving out
terms or ad groups with 50% training split.

In Figure 6 we present summaries of the model’s absolute
improvement over the baseline. Observe that the diﬀerences
are very small:
for many they are just fractions of cents
more in proﬁt. To give a sense of the scale, for a 90% split
the median improvement was 0.01, 0.14, and 0.23 for terms,
ad groups, and campaigns. However, given the click vol-
ume of these accounts small diﬀerences can translate into
substantial increases in monthly proﬁts. In many cases the
opportunities for improvement may be limited, for instance
when the cost function ci on a term is almost ﬂat. The same
pattern of improvement as earlier can be seen when moving
from term to ad group to campaign value prediction, as the
right tail of the improvement distribution becomes slightly
heavier.

We conclude from this analysis that our modeling ap-
proach holds the most promise for predicting advertiser val-
ues on newly created ad groups and campaigns, rather than
just single terms added to ad groups. In the latter case, the
value for the term is likely close to the average value of the
ad group, which is expected given the close relationship be-
tween terms in an ad group—recall they all share the same
creative.

7. CONCLUSIONS

This paper proposed a regret-based hierarchical model for
estimating advertiser values per click from keyword char-

1187Figure 6: Distribution of the absolute improvement of the model over the baseline. A positive diﬀerence
indicates the model did better than the baseline.

acteristics, observed costs and click-through rates, and ob-
served advertiser bids. Our modeling strategy was evaluated
on nearly 150 thousand terms and outperformed a compet-
itive baseline [16] on the majority of our leave-out experi-
ments. We found that value estimation using this approach
is most fruitful when predicting values on new ad groups
and campaigns. We also independently validated recently
proposed methods for estimating advertiser cost and click
beliefs, and provided data-driven insights into the structure
of advertisers’ sponsored search campaigns.

We see several avenues for improvement and future work.
First, there is room for improvement in the prediction per-
formance of our hierarchical model. We chose this kind of
model with a view towards interpretation as well as predic-
tion, which can be important if advertisers demand expla-
nations for keyword or bid suggestions. We believe good
improvements could be obtained using machine learning al-
gorithms specialized for prediction (e.g, boosting [18]) if that
were the sole concern. We also see the need to move beyond
ad-hoc feature selection. To this end, we intend to apply
techniques such as topic models [4] to uncover conceptual
and semantic regularities among campaign terms.

8. ACKNOWLEDGEMENTS

We thank Amy Greenwald and David Pennock for initiat-
ing this project, and Eliot Li for bringing the collaborators

together. Furcy Pin gave us valuable clariﬁcations on his
clicks and cost modeling methodology. We received helpful
comments and suggestions from Amy Greenwald, Patrick
Jordan, Ashvin Kannan, Prabhakhar Krishnamurthy, Eren
Manavoglu, David Pennock, and Michael Schwarz.

References
[1] Susan Athey and Denis Nekipelov. A structural model

of sponsored search advertising auctions. Technical
report, Microsoft Research, May 2010.

[2] Arindam Banerjee, Srujana Merugu, Inderjit S.

Dhillon, and Joydeep Ghosh. Clustering with
Bregman divergence. Journal of Machine Learning
Research, 6:1–48, 2005.

[3] Dimitri P. Bertsekas. Nonlinear Programming. Athena

Scientiﬁc, 1999.

[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

Latent Dirichlet allocation. In Journal of Machine
Learning Research, volume 3, pages 993–1022, March
2003.

[5] Andrei Broder, Evgeniy Gabrilovich, Vanja Josifovski,
George Mavromatis, and Alex Smola. Bid generation
for advanced match in sponsored search. In

Absolute Improvement (cents)Percent of Total0204060−10−6−4−202468100.5campaign0.75campaign−10−6−4−202468100.9campaign0.5adgroup0.75adgroup02040600.9adgroup02040600.5term−10−6−4−202468100.75term0.9term1188Proceedings of the fourth ACM International
Conference on Web Search and Data Mining, pages
515–524, 2011.

[13] Bernard J. Jansen and Lauren Solomon. Gender

demographic targeting in sponsored search. Working
paper, 2010.

[6] Yifan Chen, Gui-Rong Xue, and Yong Yu. Advertising

[14] S´ebastien Lahaie and David M. Pennock. Revenue

keyword suggestion based on concept hierarchy. In
Proceedings of the International Conference on Web
Search and Web Data Mining, pages 251–260, 2008.

analysis of a family of ranking rules for keyword
auctions. In Proceedings of the 8th ACM Conference
on Electronic Commerce, pages 50–56, 2007.

[7] Quang Duong and S´ebastien Lahaie. Discrete choice

[15] S´ebastien Lahaie, David M. Pennock, Amin Saberi,

models of bidder behavior in sponsored search. In
Proceedings of the 7th International Workshop on
Internet and Network Economics, 2011.

[8] Benjamin Edelman. Strategic bidder behavior in

sponsored search auctions. In Workshop on Sponsored
Search Auctions, pages 192–198, 2005.

[9] Benjamin Edelman, Michael Ostrovsky, and Michael

Schwarz. Internet advertising and the Generalized
Second-Price auction: Selling billions of dollars worth
of keywords. American Economic Review, 97(1),
March 2007.

and Rakesh V. Vohra. Sponsored search auctions. In
Noam Nisan, Tim Roughgarden, ´Eva Taros, and
Vijay V. Vazirani, editors, Algorithmic Game Theory,
pages 699–716. Cambridge University Press, 2007.

[16] Furcy Pin and Peter Key. Stochastic variability in

sponsored search auctions: observations and models.
In Proceedings of the 12th ACM Conference on
Electronic Commerce, pages 61–70, 2011.

[17] Oliver J. Rutz and Randolph E. Bucklin. A model of

individual keyword performance in paid search
advertising. SSRN eLibrary, 2007.

[10] Andrew Gelman and Jennifer Hill. Data analysis

[18] Robert E. Schapire. A brief introduction to boosting.

using regression and multilevel/hierarchical models.
Cambridge University Press, 2007.

[11] Anindya Ghose and Sha Yang. An empirical analysis

of search engine advertising: Sponsored search in
electronic markets. Management Science, 55:
1605–1622, October 2009.

[12] Dustin Hillard, Stefan Schroedl, Eren Manavoglu,

Hema Raghavan, and Chris Leggetter. Improving ad
relevance in sponsored search. In Proceedings of the
third ACM international conference on Web Search
and Data Mining, pages 361–370, New York, NY,
2010. ACM.

In Proceedings of the 16th International Joint
Conference on Artiﬁcial Intelligence, pages 1401–1406,
1999.

[19] Hal R. Varian. Goodness-of-ﬁt in optimizing models.

Journal of Econometrics, 46:125–140, 1990.

[20] Hal R. Varian. Position auctions. International

Journal of Industrial Organization, 25:1163–1178,
2007.

[21] Jun Yan, Ning Liu, Gang Wang, Wen Zhang, Yun
Jiang, and Zheng Chen. How much can behavioral
targeting help online advertising? In Proceedings of
the 18th International World Wide Web Conference,
pages 261–270, Madrid, Spain, 2009.

1189
