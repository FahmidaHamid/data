[1]  The Clever Searching, the Clever project of IBM Almaden

Research Center, www.almaden.ibm.com/cs/k53/clever.html.
[2]  Berman, A. and Plemmons, R.J. Nonnegative matrices in the
mathematical sciences. in Classics in Applied Mathematics,
1994.

[3]  Bharat, K. and Henzinger, M.R., Improved algorithms for

topic distillation in a hyperlinked environment. in 21st ACM
SIGIR International Conference on Research and
Development in Information Retrieval, (Melbourne,
Australia, 1998), 104-111.

[4]  Brin, S. and Page, L. The Anatomy of a Large-Scale

Hypertextual Web Search Engine. Computer Networks and
ISDN Systems, 30. 107-117.

[5]  Chakrabarti, S., Dom, B., Gibson, D., Kleinberg, J.,

Raghavan, P. and Rajagopalan, S., Automatic Resource
Compilation by Analyzing Hyperlink Structure and
Associated Text. in 7th international conference on World
Wide Web, (Brisbane, Australia, 1998), 65 – 74.

[6]  Chakrabarti, S., Dom, B.E., Kumar, S.R., Raghavan, P.,
Rajagopalan, S., Tomkins, A., Gibson, D. and Kleinberg,
J.M. Mining the Web's Link Structure. IEEE Computer, 32
(8). 60-67.

[7]  Cohn, D. and Chang, H., Learning to Probabilistically
Identify Authoritative Documents. in 17th International
Conference on Machine Learning, (Stanford, CA 2000), 167-
174.

[8]  Craswell, N. and Hawking, D., Overview of the TREC-2002

Web Track. in 11th Text Retrieval Conference,
(Gaithersburg, MD,2002).

[9]  Craswell, N., Hawking, D. and Robertson, S., Effective Site

Finding using Link Anchor Information. in 24th annual
international ACM SIGIR conference on Research and

development in information retrieval, (New Orleans, LA,
01), 250-257.

[10] Davison, B.D., Toward a unification of text and link

analysis. in 26th annual international ACM SIGIR
conference on Research and development in information
retrieval, (Toronto, Canada, 2003), 367-368.

[11] DirectHit. http://www.directhit.com.
[12] Garfield, E. Citation analysis as a tool in journal evaluation.

Science, 178. 471-479.

[13] Hayes, B. Graph Theory in Practice, 2000.
[14] Herlocker, J.L., Konstan, J.A., Borchers, A. and Riedl, J., An
algorithmic framework for performing collaborative filtering.
in 22nd annual international ACM SIGIR conference on
Research and development in information retrieval,
(Berkeley, CA 1999), 230-237.

[15] Hubbell, C.H. An input-output approach to clique

identification. Sociometry, 28. 377-399.

[16] Katz, L. A new status index derived from sociometric

analysis. Psychometrika, 18 (1). 39-42.

[17] Kleinberg, J.M. Authoritative sources in a hyperlinked

environment. Journal of the ACM (JACM), 46 (5). 604-632.
[18] Lempel, R., Moran, S. SALSA: the Stochastic Approach for

Link-Structure Analysis (TOIS), 19 (2). 131-160.

[19] Miller, J.C., Rae, G., Schaefer, F., Ward, L.A., LoFaro, T.

and Farahat, A., Modifications of Kleinberg's HITS
algorithm using matrix exponentiation and web log records.
in 24th annual international ACM SIGIR conference on
Research and development in information retrieval, (New
Orleans, LA, 2001), 444-445.

[20] Ng, A.Y., Zheng, A.X. and Jordan, M.I., Stable algorithms

for link analysis. in 24th ACM SIGIR International
Conference on Research and Development in Information
Retrieval, (New Orleans, LA 2001), 258-266.

[21] Pinski, G. and Narin, N. Citation influence for journal

aggregates of scientific publications: Theory, with
application to the literature of physics. Information Process
and Management, 12. 297-312.

[22] Vogt, C.C. and Cottrell, G.W., Predicting the performance of

linearly combined IR systems. in 21st annual international
ACM SIGIR Conference on Research and Development in
Information Retrieval, (Melbourne, Australia, 1998), 190-
196.

[23] Wen, J.-R., Nie, J.-Y. and Zhang, H.-J. Query Clustering

Using User Logs. ACM Transactions on Information
Systems (TOIS), 20 (1). 59-81.


8.  APPENDIX
Proof of convergence for the calculation of unified matrix A
In  the  appendix,  we  will  prove  the  convergence  of  iterative
calculation method of unified matrix A defined by (5). The proof
of convergence would be given, after the proofs of 3 lemmas.

Lemma  A:  The  matrix  A  defined  by  (5)  is  non-negative,  row-
stochastic.

326Proof: Based on (4), we know that matrices
are non-
negative,  row-stochastic.  And  we  also  know  the  constraint  of
parameter  α,  β:
.  Thus,  each

ML and
'

NML
'

β>

0,

=

+

>

0

β

NM

1,
α
M

α
M

NM

∑

∀ ≠

N M

element  in  matrix  A  is  non-negative,  and  sum  of  each  row  of
matrix A is 1. That means the matrix A defined by (5) is a non-
negative, row-stochastic matrix. ■

Lemma  B:  If  A  defined  by  (5)  is  also  reducible,  there  exist  a

permutation matrix P, such that

PAP

T


= 


A
1
0

0
A
2


 . Here, A1 is


T

A
1
B

PAP


= 


a non-negative, row-stochastic and irreducible matrix.
Proof:    Actually,  if  A  is  reducible,  there  exist  a  permutation

matrix P, such that
 . A1 is a non-negative, row-


0
A
2
stochastic and irreducible matrix.
As  metioned  in  the  construction  of  the  unified  matrix  A,  we
MNL is not zero matrix
know, if βMN>0, then βNM>0. That means if
'
MNL and
then
is  not  zero  matrix  too.  Also,
  are  all
'
positive matrices. So A has somewhat symmetry character. That
is, if Aij is non-zero then Aji is non-zero too.
PAP
,  doesn’t  change  the
Notice  that  the  transformation  of  A,
T
symmetry  couple  relation  of  A.  It  mean  that  the  transformed
matrix
  has  the  same  feature  as  original  matrix  A:  if
element  (i,j)  is  non-zero  then  the  element  (j,i)  is  non-zero.  So
PAP

 has the format of

PAP

NML
'

NML
'

■

T

T

1

A
0





0
A

2





i

x

A x
T

1i
+ =
0


Lemma C If one matrix A is non-negative, row-stochastic matrix,
 converge to
and irreducible, then iterative calculation
the  principle  eigenvector  of  A.  (Assume  x   is  positive  and
normalized vector).
Proof: A is non-negative, row-stochastic matrix also irreducible,
thus, A is an ergodic transition matrix of a Markov chain MC.
According the ergodic theory of Markov chain, if we can prove
that  the  MC  has  one  and  only  one  stationary  probability  vector
Sx , then the iterative calculation
 can converge to the
0x . Here, we assume
stationary vector
norm of
To prove the Markov chain has only one stationary vector
get the following 2 points firstly:
1)  For  A  is  non-negative,  row-stochastic  matrix,
spectral radius of A, is equal to 1.

A x
T i
Sx  for any initial vector

0x is normalized to 1, and

0x  is positive.

)Aρ ,  the

Sx , we

1i
+ =

x

(



(

)

x

=

>

0,

xA

kxA

x x

(
1,

Aρ=

xA x= .

with
0,
>

A
={ |

.  From  1),

(considering

of
A xρ
) }
(

)Aρ   is  an
and
multiplicity
A xρ
[2]. Based on 2),
(
) }
=
x ≥
0
 (considering scaling)
Aρ =1.  Hence,  there  exists
(
)
x ≥

scaling)
0

2)  For  A  is  non-negative  and  irreducible  matrix,
eigenvalue
x x
Ax
A x
{ |
T
there exists one and only one vector
satisfying
one  and  only  one  vector
satisfying
If we scale  x to make the sum of  x  is 1, it’s easy to know the
x= existed for any k=1, 2… So  x  is the stationary
equation
vector of Markov chain MC. Also,  x  is the principle eigenvector
of A.
Hence,
irreducible,  then  iterative  method
principle eigenvector of A. ■

Theorem:  For  the  unified  matrix  A  defined  by  (5),  iterative
method
Proof: Firstly, A is a non-negative, row-stochastic matrix. If A is
irreducible,  then  according  to  lemma  C,  we  know  the  iterative
method
If A is reducible, let
A
1
0

is  non-negative,  row-stochastic  matrix,  and
  converge  to  the

 converge to the principle eigenvector of A.

 converge to the principle eigenvector of A.

. Then the iterative method turns to

, here P is the permutation matrix

'w Pw=




w A w

w A w


= 


fitting

if  A

PAP

A x
T i

1i
+ =

0
A

=

=

x

T

T

T

2

be

w


= 


A
T
1
0

0
A
T
2





w

.

By  the  lemma  B,  A1  is  a  non-negative,  row-stochastic  and
irreducible matrix. And A2 is non-negative, row-stochastic. If A2
is  reducible,  we  can  apply  lemma  B  on  it  and  transform  it  to
block-like diagonal matrix, with sub-matrix being irreducible. So,
without  loss  of  generality,  we  assume  A1,  A2  are  irreducible.

Hence, we rewrite

'w

 to be

, then we get two sub-iterative

1w
'

w
'
1

w
'
2

A=
T
1

, and

. Based on lemma C, these 2
methods:
methods all converge. Taking limitation on the original iterative
 is an eigenvector of A associated
method:
with eigenvalue equals to 1. Also, we know spectral radius of A is
1, so

 is the principle eigenvector of A. ■

, we know

w A w

w

w

=

T

w
'


1


w
'


2
A=
2w
T
'
2

327
