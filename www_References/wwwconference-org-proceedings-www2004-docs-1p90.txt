[1]  E. Agichtein and L. Gravano, Snowball: Extracting

relations from large plaintext collections. In Proceedings of
the 5th ACM International Conference on Digital Libraries
(DL'00), 2001, pp. 85-94.

[2]  S. Blair-Goldensohn, K. R. McKeown and A. H.

Schlaikjer. A Hybrid Approach for Answering Definitional
Questions. Technical Report CUCS-006-03. Columbia
University, 2003.

[3]  C. Buckley, G. Salton, J. Allan, and A. Singhal, Automatic
query expansion using SMART, NIST Special Publication
500-225: The Third Text Retrieval conference (TREC 3),
1995, pp. 69-80.

[4]  J. Carbonell and J. Goldstein, The use of MMR, diversity-
based reranking for reordering documents and producing
summaries, in Proceedings of the 21st ACM-SIGIR
International Conference on Research and Development in
Information Retrieval, Melbourne, Australia, 1998, pp.
335-336.

[5]  S. Harabagiu, D. Moldovan, R. Mihalcea M. Pasca, R.

Bunescu, M. Surdeanu, R. Girju, V. Rus, and P.
Morarescu, Falcon: Boosting knowledge for answer
engines, Proc. of Ninth Text Retrieval Conference (TREC
9), pp. 479-488, 2000.

[6]  J. L. Klavans, S. Popper and R. Passonneau, Tackling the

Internet Glossary Glut: Automatic extraction and
Evaluation of Genus Phrases, In Proceedings of Semantic
Web Workshop, SIGIR 2003, July 28 – Aug. 1, 2003,
Toronto, Canada.

[7]  J. Lannon, 1991, Technical Writing, Ch 5, HarperCollins

Publishers Inc., 1991.

[8]  C.-Y. Lin and E. H. Hovy. The Automated Acquisition of

Topic Signatures for Text Summarization. Proc. of the
COLING Conference. Strasbourg, France, 2000, pp. 495-
501.

[9]  B. Liu, C-W. Chin and H-T. Ng, 2003, Mining Topic

Specific Concepts and Definitions on the Web, In
Proceedings of International Conference on World Wide
Web, 2003, Budapest, Hungary, pp. 251-260.

[10] I. Muslea. Extraction patterns for information extraction

tasks: A survey. In AAAI-99 Workshop on Machine
Learning for Information Extraction, 1999, pp. 1-6.

[11] U. Y. Nahm and R. J. Mooney, 2001, Mining softmatching
rules from textual data. In Proceedings of the Seventeenth
International Joint Conference on Artificial Intelligence
(IJCAI-2001), pp. 979-984.

[12] J. M. Prager, D. R. Radev and K. Czuba, Answering What-
Is Questions by Virtual Annotation, Proceedings of Human
Language Technologies Conference, San Diego, CA, pp.
26-30, March 2001.

[13] D. Radev, H. Jing and M. Budzikowska, Centroid based
summarization of multiple documents, in ANLP/NAACL
’00 Workshop on Automatic Summarization (Seattle, WA,
April 2000) pp. 21-29.

[14] D. Ravichandran and E. Hovy, Learning Surface Text

Patterns for a Question Answering System, In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002,
pp. 41-47.

[15] E. Riloff. Automatically generating extraction patterns
from untagged text. In Proceedings of the 13th National
Conference on Artificial Intelligence (AAAI-96), AAAI
Press, 1996, pp. 1044-1049.

[16] G. Salton. Automatic Text Processing. Addison-Wesley,

1989.

[17] B. Schiffman, I. Mani, and K. J. Concepcion. Producing
biographical summaries: Combining linguistic knowledge
with corpus statistics. In Proceedings European
Association for Computational Linguistics, 2001.

[18] S. Soderland, Learning Information Extraction Rules for
SemiStructured and Free Text, Machine Learning: Special
Issue on Natural Language Learning, 34, pp. 233-272.

[19] K. Sudo, S. Sekine, and R. Grishman. Automatic Pattern
Acquisition for Japanese Information Extraction. Proc.
HLT 2001, San Diego, CA, 2001.

[20] E.  M.  Voorhees,  Evaluating  Answers

to  Definition
Questions, In Proceedings of HLT-NAACL 2003, pp. 109-
111.

[21] R. Yangarber, R. Grishman, P. Tapanainen and S.

Huttunen. Automatic Acquisition of Domain Knowledge for
Information Extraction. Proc. 18th Int’l Conf. on
Computational Linguistics (COLING 2000), Saarbrucken,
Germany, July-August 2000, pp. 940-946.

[22] H. Yang, H. Cui, M.-Y. Kan, M. Maslennikov, L. Qiu,   T.-

S. Chua, QUALIFIER in TREC-12 QA Main Task, In
Proceedings of the Twelfth Annual Text Retrieval
Conference (TREC- 12), NIST, November, 2003.

[23] Y. Yang, An evaluation of statistical approaches to text

categorization. Information Retrieval, Vol. 1, Number 1-2,
1999, pp. 69-90.

98APPENDIX



ID
1

2
3
4
5
6

Appendix 1. Hand crafted rules used in HCR.
Regular expressions of rules
<SCH_TERM> (who | which | that)* (is | are) (called |
known as)*
<SCH_TERM> , (a | an | the)
<SCH_TERM> (is | are) (a | an | the)
<SCH_TERM> , or
<SCH_TERM> (- | :)
<SCH_TERM>  (is  |  are)  (used  to  |  referred  to  |
employed to | defined as | described as)
“ (.+) ” by <SCH_TERM>
(called | known as | referred to) <SCH_TERM>

7
8

Legend:




Appendix  2.  The  26  questions  for  the  evaluation  on  Web
corpus.

| - Any one of the elements within the round brackets.
* - Optional field
(.+) – Any characters.

Question ID

1
2
3
4
5
6
7

Questions
Who is Brooke Burke?
Who is Clay Aiken?
Who is Jennifer Lopez?
What is Lord of the Rings?
Who is Pamela Anderson?
What is Hurricane Isabel?
What is Final Fantasy?



8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

Who is Harry Potter?
Who is Carmen Electra?
What is Napster?
What is Xbox?
Who is Martha Stewart?
Who is Osama bin Laden?
What is Cloning?
What is NASA?
Who is Halle Berry?
What is Enron?
What is West Nile Virus?
What is SARS?
Who is Daniel Pearl?
Who is Nostradamus?
Who is James Bond?
Who is Arnold Schwarzenegger?
Who is Mohammed Saeed al-Sahaf?
Who is Uday Hussein?
What is stem cell?



Appendix 3. The evaluation metrics

NR  =  #  essential  nuggets  returned  in  response  /  #  essential
nuggets
NP is defined using

allowance  =  100  *  (#  essential  +  acceptable  nuggets
returned in response)
length  =  total  #  non-white-space  characters  in  answer
strings

NP =   1 if length < allowance

else 1-[(length – allowance) / length]
F = (26 * NP * NR) / ((25 * NP) + NR)

99
