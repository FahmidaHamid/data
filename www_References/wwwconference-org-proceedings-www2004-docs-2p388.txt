[1] J. Aspnes and G. Shah. Skip graphs. In SODA, 2003.
[2] F. Dabek et al. Wide-area cooperative storage with

CFS. In SOSP, 2001.

[3] A. Daskos et al. Peper: A distributed range addressing

space for p2p systems. In DBISP2P, 2003.

[4] P. Druschel et al. PAST: A large-scale, persistent
peer-to-peer storage utility. In HotOS VIII, 2001.

[5] S. Ratnasamy et al. A scalable content-addressable

network. In SIGCOMM, 2001.

[6] A. Rowstron et al. Pastry: Scalable, decentralized

object location, and routing for large-scale peer-to-peer
systems. In Middleware, 2001.

[7] I. Stoica et al. Chord: A scalable peer-to-peer lookup
service for internet applications. In SIGCOMM, 2001.

Figure 2: Mapping
Ranges to Peers

Figure 3: Mapping Val-
ues to Peers

space is mapped to one (and only one) peer. Now, assume
that peer p1 fails.
In this case, the Fault Tolerant Torus
needs to reassign the range (5, 10] to another peer. If a peer
can be responsible for only one region of the space, then p2
or p5 need to increase their range by taking over p1’s range.
The ring in Chord [7] is one possible instantiation of the
Fault Tolerant Torus. Here, the integer space [0, 2m) is the
ring space and peers are assigned an ID in this space. Each
peer is responsible for the region in the ring between its
predecessor ID and its ID. Other examples are the ring in
Pastry [6] and the d-dimensional torus in CAN [5].
2.2 Data Store

The Data Store is responsible for distributing the data
items to peers. Ideally, each peer should store approximately
the same number of items, achieving storage balance. The
Data Store maps each data item to a point in the torus
space, and stores the item at the peer responsible for the
region containing that point.
If a peer ends up with too
many data items (due to insertions) or too few data items
(due to deletions), it will have to re-balance the assignment
of data items to peers. Exactly how this re-balancing is done
depends on the speciﬁc instantiation of this component.

Example In Figure 2, assume that a data item t1 mapped
to value 6 is inserted into the system. In this case, the pair
(6, t1) will be stored at peer p1 as shown in Figure 3.

The equivalent of the Data Store in Chord is implemented
using a hash-based scheme. Data items are hashed to values
on the ring, and assigned to the ﬁrst peer with ID following
the value in the ring (note that since hashing destroys the
ordering of the values, Chord cannot process range queries).
PePeR [3] does not use hashing, but maps data items to the
peers responsible for the respective ranges in the value space.
Re-balancing is done at peer insertion or deletion/failure,
when some ranges are split respectively merged.
2.3 Replication Manager

The Fault Tolerant Torus component is responsible for en-
suring that each point on the torus is assigned to some peer
and the Data Store component is responsible for actually
storing the data items at peers. However, if a peer fails, the
data items it stored will be lost even if another peer takes
over the ”failed” region. The role of the Replication Man-
ager is to ensure that all the data items inserted into the
system are reliably (under reasonable failure assumptions)
stored at some peer until the items are explicitly deleted.

Example In Figure 3, peer p1 stores the pair (6, t1). If p1
fails, peer p2 or p5 will take over the range (5, 10] (as ensured
by the Fault Tolerant Torus component). However, the data

515p1101820p2p3p4p5515p1101820p2p3p4p5(6,t1)389
