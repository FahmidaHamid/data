[1] Alexa. http://www.alexa.com.
[2] S. Dill, et al. Semtag and seeker: Bootstrapping the semantic

web via automated semantic annotation. In Proc 12th
International WWW Conf, Budapest, Hungary, May 2003.

[3] M. Hearst. User interfaces and visualization. In R.

Baeza-Yates and B. Ribeiro-Neto (Eds.) Modern information
retrieval. NY: ACM Press., 1999.

[4] Y. Maarek and I. Shaul. Webcutter: A system for dynamic and

tailorable site mapping. In Proceedings of the 6th
International World Wide Web Conference, 1997.

[5] G. Marchionini and B. Brunk. Toward a general relation
browser: A GUI for information architects. In Journal of
Digital Information, volume 4, 2003.

[6] D. Nation, C. Plaisant, G. Marchionini, and A. Komlodi.

Visualizing websites using a hierarchical table of contents
browser: WebTOC. In Designing for the Web: Practices and
Reﬂections, 1997.

[7] A. J. Sellen, R. Murphy, and K. L. Shaw. How knowledge

workers use the web. In Proc. SIGCHI, 2002.

Figure 2: LiveJournal Directories and Links

Figure 1 shows the summary statistics for a large community site.
From this page we can see how frequently it has been crawled, and
view miscellaneous statistics. This view contributes to the assess-
ment task, and also allows navigation into particular subcommuni-
ties, such as the French language subcommunity.

Figure 2 shows the directory structure for this community site.
Most pages are in the /users/ directory, but distinguished users
and administration pages are clear. This is the best point of entry
for navigation within the site: indeed, the directory structure can be
as useful as the site’s own site map, and often more complete.

Figure 2 also shows that LiveJournal users link most to quizilla
and to news sites, and, of course, to other blogging sites. This is an
ideal starting point for discovery of related sites.

4. ARCHITECTURE

The system is designed to minimize the number of times a given
piece of data is read or written to disk. The ﬁrst processing stage,
Gather, assigns each site to one of 8 dual-2.4GHz CPU, 4 GB RAM
build machines. A stream of new pages is extracted from the main
cluster of 256 machines, where each machine stores a distinct set
of pages partitioned by hash of the URL. Thus, on each of the build
machines we begin with 256 separate input “packets”. For each
page we store a record consisting of counts of all the features de-
scribed in section 2. Each packet is sorted by site, by using host-
names. In the Update phase we accumulate this newpages data into
the main pages store, which is also sorted by site. Older versions of
pages are discarded. Finally, the Merge phase performs a 256-way
merge to extract a contiguous sequence of pages on each site, accu-

497
