
[1] R. Agrawal, T. Imielinski and A. Swami. Mining
Association Rules Between Sets of Items in Large
Databases. SIGMOD, 1993.

[2] S. Aya, C. Lagoze and T. Joachims. Citation Classiﬁcation

and its Applications. ICKM’05.

[3] K. Banaszek, G. D’Ariano, M. Paris and M. Sacchi.

Maximum-likelihood estimation of the density matrix.
Physical Review A, 1999.

[4] C. Basu, H. Hirsh, W. Cohen and C. Nevill-Manning.

Technical Paper Recommendation: A Study in Combining
Multiple Information Sources. J. of Artiﬁcial Intelligence
Research, 2001.

[5] D. Blei, A. Ng and M. Jordan. Latent dirichlet allocation.

J. Machine Learning Research 2003.

[6] K. Chandrasekaran, S. Gauch, P. Lakkaraju and H. Luong.

Concept-Based Document Recommendations for CiteSeer
Authors. Adaptive Hypermedia and Adaptive Web-Based
Systems, Springer, 2008.

[7] D. Cohn and T. Hofmann. The missing link - a

probabilistic model of document content and hypertext
connectivity. NIPS’01.

[8] F. Diaz. Regularizing Ad Hoc Retrieval Scores. CIKM’05.
[9] E. Erosheva, S. Fienberg and J. Laﬀerty. Mixed

membership models of scientiﬁc publications. PNAS 2004.
[10] A. Gleason. Measures on the Closed Subspaces of a Hilbert

Space. J. of Mathematics and Mechanics, 1957.

[11] S. Huang, G. Xue, B. Zhang, Z. Chen, Y. Yu and W. Ma.

TSSP: A Reinforcement Algorithm to Find Related Papers.
WI’04.

[12] A. Jeﬀrey and H. Dai. Handbook of Mathematical

Formulas and Integrals. Academic Press, 2008.

[13] J. Kleinberg. Authoritative sources in a hyperlinked

environment. J. of the ACM, 1999.

[14] J. Laﬀerty and G. Lebanon. Diﬀusion Kernels on Statistical

Manifolds. J. of Machine Learning Research, 2005.

[15] D. Liben-Nowell and J. Kleinberg. The link prediction

problem for social networks. CIKM’03.

[16] S. McNee, I. Albert, D. Cosley, P. Gopalkrishnan, S. Lam,
A. Rashid, J. Konstan and J. Riedl. On the Recommending
of Citations for Research Papers. CSCW’02.

[17] M. Melucci. A basis for information retrieval in context.

TOIS, 2008.

[18] R. Nallapati, A. Ahmed, E. Xing and W. Cohen. Joint
latent topic models for text and citations. SIGKDD’08.

[19] Z. Nie, Y. Zhang, J. Wen and W. Ma. Object-Level

Ranking: Bringing Order to Web Objects. WWW’05.

[20] C. Rijsbergen. The Geometry of Information Retrieval.

Cambridge University Press, 2004.

[21] A. Ritchie. Citation context analysis for information

retrieval. PhD thesis, University of Cambridge, 2008.

[22] B. Shaparenko and T. Joachims. Identifying the Original

Contribution of a Document via Language Modeling.
ECML, 2009.

[23] T. Strohman, W. Croft and D. Jensen. Recommending

Citations for Academic Papers. SIGIR’07 and Technical
Report,
http://ciir-publications.cs.umass.edu/getpdf.php?id=610.

[24] J. Tang and J. Zhang. A Discriminative Approach to
Topic-Based Citation Recommendations PAKDD’09.

[25] R. Torres, S. McNee, M. Abel, J. Konstan and J. Riedl.

Enhancing Digitial Libraries with Techlens. JCDL’04.
[26] F. Wang, B. Chen and Z. Miao. A Survey on Reviewer

Assignment Problem. IEA/AIE’08.

[27] D. Zhou, S. Zhu, K. Yu, X. Song, B. Tseng, H. Zha and

L. Giles. Learning Multiple Graphs for Document
Recommendations. WWW’08.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA429APPENDIX
A. PROOF OF PROPOSITION 5.2
orthogonal. Each ti can also be expressed as αi1si + ··· +
By the spectral theorem, we can choose all the ti to be
αiasa +βi1u1 +···+βibub where the si and uj are orthogonal
unit vectors such that the si span the same space as the ci.
The log likelihood is then

k(cid:3)
k(cid:6)

i=1

=

k(cid:6)

i=1

=

i=1

log

k(cid:6)

i=1

log pgen(ci) +

log pd(ci)

k(cid:3)
k(cid:6)

i=1

(cid:6)

k(cid:6)

(cid:6)

log pgen(ci) +

i

r

r

log

i=1

ci =

tj tT
j

j=1 cT

log pgen(ci)+
j=1([αi1si + · · · + αiasa + βi1u1 + · · · + βibub] · ci)
j=1([αi1si + · · · + αiasa] · ci)
2.

k(cid:6)

(cid:6)

i=1

log

2

r

log pgen(ci) +

i=1

i

i

· t(cid:2)
t(cid:2)
d) =
i1 + · · · + α2
i
ia)
i1 + · · · + α2
ia + β2

(cid:2)T
i without changing the likelihood. However,

likelihood by replacing each ti = αi1si +···+αiasa +βi1u1 +
Since the ui are orthogonal to the ci, we can increase the
(cid:6)
·· · + βibub with t
i = αi1si + ··· + αiasa to get the matrix
(cid:2)
(cid:2)
(cid:2)
T
i t
it
d =
(cid:6)
(cid:6)
T race(T (cid:2)
(cid:6)
i(α2
=
i(α2
<

since the αi and βj are all orthogonal
i1 + · · · + β2
(cid:2)
i by a constant γ > 1 to
Thus we need to multiply each t
(cid:2)
increase the trace of T
d to 1. Multiplication by such a γ will
then increase each of the quantities in the log terms of the log
(cid:2)
likelihood. Thus γT
d would have a higher log likelihood than
i titT
Td. Thus in the maximum likelihood solution Td =
i ,
each ti is in the subspace spanned by the contexts ci and
representing all of the ti requires O(r2) parameters. So, a
numerical solution will have polynomial complexity in r2.

ib) = T race(Td) = 1.

(cid:6)

(cid:16)(cid:17)(cid:17)(cid:18) |W|(cid:3)

Cartesian coordinates to hyperspherical coordinates as:

w1 = r cos θ1, w2 = r sin θ1 cos θ2, w3 = r sin θ1 sin θ2 cos θ3,
. . . , w|W |−1 = r sin θ1 sin θ2 . . . sin θ|W|−2 cos θ|W|−1,
w|W | = r sin θ1 sin θ2 . . . sin θ|W|−2 sin θ|W|−1;

≥ 0;

w2
i

r =
θ1 ∈ [0, π], θ2 ∈ [0, π], . . . , θ|W|−2 ∈ [0, π], θ|W|−1 ∈ [0, 2π);

i=1

the absolute value of the determinant of the Jacobian is

|W|−2(cid:19)

|W|−1

r

|W|−1−i(θi).

sin

Let us also deﬁne
(cid:9) ∞

(cid:9)

(cid:9)

(cid:9) 2π0

π0 ...

π0

0 r

Φ ≡

i=1

|W |−1

|W |−2(cid:2)
i=3
(2π)|W |/2

sin|W|−1−i(θi

)drdθ3...dθ|W|−1

.

)/2dx1 . . . dx|W|

The moment calculations are thus as follows:
E[w4

(cid:6)
)2 e−(

−∞ . . .

2
i

1

x

(cid:9) ∞
(cid:9) ∞

(cid:6)
−∞ x
(
4 cos4(θ1)

4
1
2
x
i

=

r

0

π
0

r|W|−1×
|W|−1−i(θi)drdθ1 . . . dθ|W|−1

r4

(cid:9)

1

1] =

(cid:9) ∞
(cid:9)
(cid:9) 2π
(2π)|W |/2
|W |−2(cid:2)
(2π)|W |/2
(cid:9)
(cid:10)
(cid:9)
0 (1 − sin

i=1

sin

π
0 . . .

π

0

2

|W|−2

= Φ

= Φ

π
0
π
0

= 2Φπ
= 2Φπ
= 2Φπ

π
0

sin

|W|−3
(cid:13)(cid:10)
sin
(|W |−3)!!
(|W |−2)!! − 2
(cid:13)(cid:10)
(|W |−4)!!
(|W |−3)!!
(|W |−4)!!
(|W |−3)!!

2

sin

(θ1))
(θ1) − 2 sin
(θ2)dθ1dθ2
(cid:13)(cid:10)
(|W |−1)!!
(|W |)!! +
(cid:13)(cid:10)
(|W |−3)!!
(|W |−2)!!
(|W |−3)!!
(|W |−2)!!

(cid:13)(cid:10)
(cid:13)

(|W|+1)!!
(|W|+2)!!
|W|−1
1 − 2
|W| +
,
(|W |+2)|W|

3

(cid:9)
(cid:9)
(cid:10)
(cid:10)
(cid:10)

|W|−2
|W|

|W|−3

(θ1) sin
(θ1) + sin

|W|+2

(θ2)dθ1dθ2
×

(θ1)

(cid:13)
(cid:13)

(|W |−4)!!
(|W |−3)!!
(|W |+1)(|W |−1)

(|W |+2)|W|

(cid:13)

(cid:9) 2π
(cid:9)
(2π)|W |/2
|W |−2(cid:2)

0

(2π)|W |/2
(cid:9)
(cid:10)
(cid:9)

by Wallis’s formula [12] where n!! is the double factorial (1
if n = 0 or 1; n!! = n × (n − 2)!! otherwise). Also,
E[w2

(cid:9) ∞

2
i

(cid:6)
−∞ x
4 cos2(θ1) sin2(θ1) cos2(θ2)

2
2
1x
2
2
x
i

)/2dx1 . . . dx|W|
r|W|−1×

(cid:9) ∞
(cid:9)

1w2
1

(cid:9) ∞

−∞ . . .

2] =

1

x

r

(

(cid:6)
)2 e−(
r4

π
0 . . .

π
0

0

sin

= Φ

= Φ

π
0
π
0

dθ1dθ2

|W|+2

sin
2

|W|−3

|W|−2

(cid:9)
(cid:9)
(cid:10)
(cid:10)
(cid:10)

= 2Φπ
= 2Φπ
= 2Φπ

|W|−1−i(θi)drdθ1 . . . dθ|W|−1
2
(θ1) sin
(θ1) cos

i=1
2
π
(θ1) sin
0 cos
|W |
π
sin
0

(cid:13)(cid:10)
(θ2) sin
(cid:13)(cid:10)
(cid:13)
(θ1)
(cid:13)(cid:10) |W|−1
(|W |−3)!! − (|W |−2)!!
(|W |−4)!!
(|W |−1)!!
(cid:13)(cid:10)
|W | − (|W |+1)(|W |−1)
(|W |+2)|W |
.
|W|(|W |+2)
i w2

(θ1) − sin
(cid:13)(cid:10)
(|W |−1)!!
|W |!! − (|W |+1)!!
(|W |+2)!!
(cid:13)(cid:10)
(|W |−3)!!
(|W |−4)!!
(|W |−2)!!
(|W |−3)!!
(|W |−4)!!
(|W |−3)!!
(|W |−2)!!
(|W |−3)!!
i ] = 3E[w2
j ]. Thus sim(d1; d2) is pro-
j ], a universal constant. We drop this

|W|−3
(θ2) − sin
(cid:13)(cid:10)

Thus we have E[w4
portional to E[w2
i w2
constant to simplify calculations. Thus our similarity is:
|W|(cid:6)
|W|(cid:6)

ci,δci,(cid:7)bj,δ bj,(cid:7)

sim(d1; d2)

b2
j,γ + 4

b2
i,α +

|W|(cid:6)

c2
i,α

(cid:6)

(cid:6)

c2
i,β

α=1

k1 k2

j=1

i=1

(cid:13)

δ>(cid:7)

=

3

1

1

(θ2)dθ1dθ2
|W |−1
(θ2)

1 − |W|−2
|W|−1

(cid:13)

(cid:13)

(cid:15)

(cid:15)

c2
i,β

b2
j,γ + 4

(cid:15)

δ>(cid:7)

ci,δ ci,(cid:7)bj,δ bj,(cid:7)

β(cid:3)=γ

(cid:6)
|W|(cid:6)
|W |(cid:6)

β=1

c2
i,α

b2
i,α +

|W|(cid:6)

2

2

α=1
2(ci · bj )
2(cid:21)
2(ci · bj )

+

β=1

γ=1

γ=1

c2
i,β

b2
j,γ

k1(cid:6)
k1(cid:6)
k1(cid:6)
k1(cid:6)

(cid:14)
(cid:14)
(cid:14)
(cid:20)

k2(cid:6)
k2(cid:6)
k2(cid:6)
k2(cid:6)

i=1

j=1

i=1

j=1

i=1

j=1

=

1

k1 k2

=

1

k1 k2

=

1

k1 k2

+ 1,

since the ci and bj are unit vectors.

B. PROOF OF PROPOSITION 5.3

=

Recall that we treat concepts as one-dimensional vector
spaces and thus can represent them as unit vectors. Let
p be the uniform distribution over unit vectors. It is well-
known that sampling from p is equivalent to sampling |W|
independent standard Gaussian random variables (one for
each dimension) and dividing them by the square root of
their sum of squares. The similarity between d1 and d2 is:

sim(d1; d2) =

(cid:9) k1(cid:6)
(cid:10) k1(cid:6)

i=1

E

i=1

(cid:9)
k2(cid:6)
T race(Td1 wwT )T race(Td2 wwT )p(w)dw
(cid:11)
(ci · w)
|W|(cid:6)
k2(cid:6)
|W|(cid:6)
|W|(cid:6)

2p(w)dw
|W|(cid:6)
|W|(cid:6)
(cid:12) (cid:13)

(bj · w)

α + 2

c2
i,α

β=γ+1

α=1

γ=1

w2

j=1

2

j=1

ci,β wβ ci,γ wγ

(cid:12)

×

b2
j,δ

w2

δ + 2

ω=(cid:7)+1

(cid:7)=1

bj,ω wωbj,(cid:7)w(cid:7)

.

=

=

(cid:11)

1

k1 k2

k1 k2

1

|W|(cid:6)

δ=1

Now, if all coordinates of w other than wi (for some i)
and wi = −x are the same, and so the coordinates of w
are ﬁxed, it is easy to see that the probability of wi = x
are jointly uncorrelated. This means that all terms with an
odd power of some coordinate will become 0 in the expected
value. Thus sim(d1; d2) simpliﬁes to:

sim(d1; d2) =

1

k1 k2

(cid:14)

|W|(cid:6)

α=1

E

c2
i,α

b2
i,α

w4

α +

k1(cid:6)
k2(cid:6)
(cid:6)

i=1

β(cid:3)=γ

j=1
c2
i,β

b2
j,γ

w2
β

w2

γ + 4

(cid:6)

δ>(cid:7)

(cid:15)

ci,δci,(cid:7)bj,δ bj,(cid:7)w2

δ

w2
(cid:7)

.

Finally, after removing the additive and multiplicative con-
stants (they don’t aﬀect ranking), we can use the following
equivalent formula:

sim(d1; d2) =

1

k1 k2

k1(cid:6)

k2(cid:6)

i=1

j=1

(ci · bj )

2.

To compute the necessary moments, we change basis from

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA430
