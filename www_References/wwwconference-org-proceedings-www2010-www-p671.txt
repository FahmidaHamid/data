[1] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms
for approximate nearest neighbor in high dimensions. In Commun.
ACM, volume 51, pages 117–122, 2009.

[2] Michael Bendersky and W. Bruce Croft. Finding text reuse on the

web. In WSDM, pages 262–271, 2009.

[3] Sergey Brin, James Davis, and Hector Garcia-Molina. Copy

detection mechanisms for digital documents. In SIGMOD, pages
398–409, 1995.

[4] Andrei Z. Broder. On the resemblance and containment of

documents. In Sequences, pages 21–29, 1997.

[5] Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael
Mitzenmacher. Min-wise independent permutations. Journal of
Computer Systems and Sciences, 60(3):630–659, 2000.

[6] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and

Geoffrey Zweig. Syntactic clustering of the web. In WWW, pages
1157 – 1166, 1997.

[7] Gregory Buehrer and Kumar Chellapilla. A scalable pattern mining
approach to web graph compression with communities. In WSDM,
pages 95–106, 2008.

[8] Moses S. Charikar. Similarity estimation techniques from rounding

algorithms. In STOC, pages 380–388, 2002.

[9] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, Michael

Mitzenmacher, Alessandro Panconesi, and Prabhakar Raghavan. On
compressing social networks. In KDD, pages 219–228, 2009.

[10] Dietzfelbinger, Martin and Hagerup, Torben and Katajainen, Jyrki

and Penttonen, Martti A reliable randomized algorithm for the
closest-pair problem. Journal of Algorithms, 25(1):19–51, 1997.

[11] Yon Dourisboure, Filippo Geraci, and Marco Pellegrini. Extraction
and classiﬁcation of dense implicit communities in the web graph.
ACM Trans. Web, 3(2):1–36, 2009.

[12] D. Fetterly, M. Manasse, M. Najork, and J. Wiener. A large-scale

study of the evolution of web pages. In WWW, pages 669–678, 2003.

[13] R.A. Fisher and F. Yates. Statistical Tables for Biological,
Agricultural and Medical Research. Oliver & Boyd, 1948.

[14] George Forman, Kave Eshghi, and Jaap Suermondt. Efﬁcient
detection of large-scale redundancy in enterprise ﬁle systems.
SIGOPS Oper. Syst. Rev., 43(1):84–91, 2009.

[15] Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel Fisher,

Matthew Hurst, and Arnd Christian König. Blews: Using blogs to
provide context for news articles. In AAAI, 2008.

10−4



101

102

Sample size k

103

Figure 9: MSEs for comparing ˆR1/2 (25) with ˆR1 and ˆRM . Due
to the bias of ˆR1/2, the theoretical variances Var
, i.e.,
(26), deviate from the empirical MSEs when k is small.

ˆR1/2

In a summary, for applications which care about very high simi-

larities, combining bits can reduce storage even further.

6. CONCLUSION

The minwise hashing technique has been widely used as a stan-
dard duplicate detection approach in the context of information re-
trieval, for efﬁciently computing set similarity in massive data sets.
Prior studies commonly used 64 bits to store each hashed value.

This study proposes b-bit minwise hashing, by only storing the
lowest b bits of each hashed value. We theoretically prove that,
when the similarity is reasonably high (e.g., resemblance ≥ 0.5),
using b = 1 bit per hashed value can, even in the worst case, gain a
21.3-fold improvement in storage space, compared to storing each
hashed value using 64 bits. We also discussed the idea of com-
bining 2 bits from different hashed values, to further enhance the
improvement, when the target similarity is very high.

Our proposed method is simple and requires only minimal mod-
iﬁcation to the original minwise hashing algorithm. We expect our
method will be adopted in practice.

103

[16] Aristides Gionis and Dimitrios Gunopulos and Nick Koudas.

Efﬁcient and Tunable Similar Set Retrieval. In SIGMOD, pages
247-258, 2001.

[17] Sreenivas Gollapudi and Aneesh Sharma. An axiomatic approach for

result diversiﬁcation. In WWW, pages 381–390, 2009.

[18] Monika .R. Henzinge. Algorithmic challenges in web search engines.

Internet Mathematics, 1(1):115–123, 2004.

[19] Piotr Indyk. A small approximately min-wise independent family of

hash functions. Journal of Algorithm, 38(1):84–90, 2001.

[20] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors:

Towards removing the curse of dimensionality. In STOC, 1998.

[21] Toshiya Itoh, Yoshinori Takei, and Jun Tarui. On the sample size of

k-restricted min-wise independent permutations and other k-wise
distributions. In STOC, pages 710–718, 2003.

[22] P. Li and K. Church. A Sketch Algorithm for Estimating Two-way

and Multi-way Associations Computational Linguistics, pages
305–354, 2007. (Preliminary results appeared in HLT/EMNLP 2005.)

[23] P. Li, K. Church and T. Hastie. One Sketch For All: Theory and
Applications of Conditional Random Sampling. In NIPS, 2008.

[24] Nitin Jindal and Bing Liu. Opinion spam and analysis. In WSDM,

pages 219–230, 2008.

[25] Konstantinos Kalpakis and Shilang Tang. Collaborative data

gathering in wireless sensor networks using measurement
co-occurrence. Computer Commu., 31(10):1979–1992, 2008.
[26] Eyal Kaplan, Moni Naor, and Omer Reingold. Derandomized

constructions of k-wise (almost) independent permutations.
Algorithmica, 55(1):113–133, 2009.

[27] Ludmila, Kave Eshghi, Charles B. Morrey III, Joseph Tucek, and
Alistair Veitch. Probabilistic frequent itemset mining in uncertain
databases. In KDD, pages 1087–1096, 2009.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA678[28] Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma.

Detecting Near-Duplicates for Web-Crawling. In WWW, 2007.
[29] Marc Najork, Sreenivas Gollapudi, and Rina Panigrahy. Less is
more: sampling the neighborhood graph makes salsa better and
faster. In WSDM, pages 242–251, 2009.

[30] Sandeep Pandey, Andrei Broder, Flavio Chierichetti, Vanja

Josifovski, Ravi Kumar, and Sergei Vassilvitskii. Nearest-neighbor
caching for content-match applications. In WWW, 441–450, 2009.

[31] Martin Theobald, Jonathan Siddharth, and Andreas Paepcke.

Spotsigs: robust and efﬁcient near duplicate detection in large web
collections. In SIGIR, pages 563–570, 2008.

[32] Mikkel Thorup and Yin Zhang. Tabulation based 4-universal hashing

with applications to second moment estimation. In SODA, 2004.

[33] Tanguy Urvoy, Emmanuel Chauveau, Pascal Filoche, and Thomas

Lavergne. Tracking web spam with html style similarities. ACM
Trans. Web, 2(1):1–28, 2008.

[34] Henry S. Warren. Hacker’s Delight. Addison-Wesley, 2002.
APPENDIX
A. PROOF OF THEOREM 1
Consider two sets, S1, S2 ⊆ Ω = {0, 1, 2, ..., D − 1}. Denote
f1 = |S1|, f2 = |S2|, and a = |S1 ∩ S2|. Apply a random
permutation π on S1 and S2: π : Ω −→ Ω. Deﬁne the minimum
values under π to be z1 and z2:
z1 = min (π (S1)) ,

z2 = min (π (S2)) .

Deﬁne e1,i = ith lowest bit of z1, and e2,i = ith lowest bit of z2.
The task is to derive Pr

(cid:3)(cid:7)b
(cid:4)
i=1 1{e1,i = e2,i} = 1

,

which can be decomposed to be

(cid:9)
(cid:9)

b(cid:10)
b(cid:10)

i=1

i=1

Pr

+Pr

(cid:11)
(cid:11)

1{e1,i = e2,i} = 1, z1 = z2

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:9)

b(cid:10)

The expressions for P1, P2, and P3 can be understood by the
experiment of randomly throwing f1+f2−a balls into D locations,
labeled 0, 1, 2, ..., D − 1. Those f1 + f2 − a balls belong to three
disjoint sets: S1 − S1 ∩ S2, S2 − S1 ∩ S2, and S1 ∩ S2. Without
any restriction, the total number of combinations should be P3.
To understand P1 and P2, we need to consider two cases:
1. The jth element is not in S1 ∩ S2: =⇒ P1.
D−j−1

We ﬁrst allocate the a = |S1 ∩ S2| overlapping elements ran-
domly in [j + 1, D − 1], resulting in
combinations.
Then we allocate the remaining f2−a−1 elements in S2 also
randomly in the unoccupied locations in [j + 1, D − 1], re-
combinations. Finally, we allocate the
sulting in
remaining elements in S1 randomly in the unoccupied loca-
tions in [i + 1, D − 1], which has
combinations.

D−j−1−a
f2−a−1

D−i−1−f2
f1−a−1

(cid:16)

(cid:15)

(cid:16)

(cid:15)

(cid:15)

(cid:16)

a

2. The jth element is in S1 ∩ S2: =⇒ P2.

After conducing expansions and cancelations, we obtain

(cid:3)

=

=

=

Pr (z1 = i, z2 = j, i < j) =

P1 + P2

P3

(cid:4)

1
a

+ 1

f2−a

(a−1)!(f1−a−1)!(f2−a−1)!(D−j−f2)!(D−i−f1−f2+a)!

(D−j−1)!(D−i−1−f2)!

f2(f1 − a)(D − j − 1)!(D − f2 − i − 1)!(D − f1 − f2 + a)!

a!(f1−a)!(f2−a)!(D−f1−f2+a)!

D!

f2(f1 − a)

=

f2
D

f1 − a
D − 1

(cid:7)j−i−2
(cid:7)i−1
D!(D − f2 − j)!(D − f1 − f2 + a − i)!
t=0(D − f1 − f2 + a − t)
j−i−2(cid:10)

(cid:7)j
(D − f2 − i − 1 − t)
t=0(D − t)
i−1(cid:10)

D − f2 − i − 1 − t

t=0

D − f1 − f2 + a − t
D + i − j − 1 − t

D − 2 − t

t=0

t=0

(cid:11)

For convenience, we introduce the following notation:

r1 =

f1
D

,

r2 =

f2
D

,

s =

a
D

.

.

Also, we assume D is large (which is always satisﬁed in practice).
Thus, we can obtain a reasonable approximation:

=Pr (z1 = z2) + Pr

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:9)

b(cid:10)

=R + Pr

i=1

1{e1,i = e2,i} = 1, z1 (cid:7)= z2

(cid:11)

where R =

i=1
|S1∩S2|
|S1∪S2| = Pr (z1 = z2) is the resemblance.

When b = 1, the task boils down to estimating

Pr (e1,1 = e2,1, z1 (cid:7)= z2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)

j(cid:9)=i,j=0,2,4,...

i=0,2,4,...

i=1,3,5,...

j(cid:9)=i,j=1,3,5,...

⎧⎨
⎩
⎧⎨
⎩

=

+

⎫⎬
⎭
⎫⎬
⎭ .

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

Therefore, we need the following basic probability formula:

Pr (z1 = i, z2 = j, i (cid:9)= j) .

We start with

P1 + P2

P3

a

(cid:4)

(cid:3)D
(cid:4)(cid:3) D − a
(cid:3)D − j − 1
f1 − a
(cid:3)D − j − 1
a − 1

(cid:4)(cid:3)D − f1
(cid:4)(cid:3)D − j − 1 − a
f2 − a
(cid:4)(cid:3)D − j − a
f2 − a − 1
f2 − a

a

,

(cid:4)

,

(cid:4)(cid:3)D − i − 1 − f2
(cid:4)(cid:3)D − i − 1 − f2
(cid:4)
f1 − a − 1
f1 − a − 1

.

P3 =

P1 =

P2 =

Pr (z1 = i, z2 = j, i < j)
=r2(r1 − s) [1 − r2]

j−i−1

[1 − (r1 + r2 − s)]

i

Similarly, we obtain, for large D,

Pr (z1 = i, z2 = j, i > j)
=r1(r2 − s) [1 − r1]

i−j−1

[1 − (r1 + r2 − s)]

j

Now we have the tool to calculate the probability

Pr (e1,1 = e2,1, z1 (cid:9)= z2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)

j(cid:9)=i,j=0,2,4,...

i=0,2,4,...

=

+

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

⎫⎬
⎭
⎫⎬
⎭

For example, (again, assuming D is large)

Pr (z1 = 0, z2 = 2, 4, 6, ...)
=r2(r1 − s)
=r2(r1 − s)

[1 − r2] + [1 − r2]
1 − r2
1 − [1 − r2]2

(cid:16)

+ ...

3

+ [1 − r2]

5

⎧⎨
⎩
⎧⎨
⎩

(cid:15)

Pr (z1 = i, z2 = j, i < j) =

, where

i=1,3,5,...

j(cid:9)=i,j=1,3,5,...

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA679(cid:16)
Pr (z1 = 1, z2 = 3, 5, 7, ...) = r2(r1 − s)[1 − (r1 + r2 − s)]
+ ...

×(cid:15)

3

5

[1 − r2] + [1 − r2]
1 − [1 − r2]2

1 − r2

+ [1 − r2]
.

=r2(r1 − s)[1 − (r1 + r2 − s)]
Therefore,(cid:2)
(cid:2)

i<j,j=0,2,4,...

i=0,2,4,...

(cid:12) (cid:2)
(cid:12) (cid:2)
1 − r2

+

i=1,3,5,...

i<j,j=1,3,5,...

×

1 − [1 − r2]2

=r2(r1 − s)
(cid:15)
=r2(r1 − s)

By symmetry, we know(cid:2)

1 − r2
1 − [1 − r2]2
(cid:12) (cid:2)
(cid:12) (cid:2)
1 − r1

j=0,2,4,...

i>j,i=0,2,4,...

(cid:2)

+

j=1,3,5,...

i>j,i=1,3,5,...

=r1(r2 − s)

1 − [1 − r1]2

(cid:13)
(cid:13)

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

(cid:16)

+ ...

(cid:13)
(cid:13)

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

1

r1 + r2 − s

.

1 + [1 − (r1 + r2 − s)] + [1 − (r1 + r2 − s)]

2

1

r1 + r2 − s

.

2b−1

r2 [1 − r2]
1 − [1 − r2]

.

2b

Therefore, we can obtain the desired probability, for b = 1,

Combining the probabilities, we obtain

=

Pr (e1,1 = e2,1, z1 (cid:9)= z2)
r2(1 − r2)
1 − [1 − r2]2
r2 − s
=A1,1

r1 + r2 − s + A2,1

r1 − s

r1 + r2 − s +

r1 + r2 − s

,

where

,

2b

A2,b =

A1,b =

2b−1

r1 [1 − r1]
1 − [1 − r1]
b=1(cid:10)

(cid:11)
1{e1,i = e2,i} = 1

(cid:9)

Pr

i=1

=R + A1,1

=R + A1,1

r2 − s
f2 − a

r1 + r2 − s + A2,1
f1 + f2 − a + A2,1

r1 − s
r1 + r2 − s
f1 − a
f1 + f2 − a

f2 − R

1+R (f1 + f2)

=R + A1,1

f1 + f2 − R
f2 − Rf1
=R + A1,1
f1 + f2
=C1,1 + (1 − C2,1)R

+ A2,1

1+R (f1 + f2)

f1 − Rf2
f1 + f2

+ A2,1

f1 − a

f1 + f2 − a

where

C1,b = A1,b

C2,b = A1,b

r2

r1 + r2

r1

r1 + r2

+ A2,b

+ A2,b

r1

r1 + r2

r2

r1 + r2

.

To this end, we have proved the main result for b = 1.

r1(1 − r1)
1 − [1 − r1]2
r1 − s

r2 − s

r1 + r2 − s

i=1

r2 − s
=R + A1,b
=C1,b + (1 − C2,b)R,

r1 + r2 − s + A2,b

r1 − s

r1 + r2 − s

Next, we consider b > 1. Due to the space limit, we only provide

a sketch of the proof. When b = 2, we need

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

i=0,4,8,...

j(cid:9)=i,j=0,4,8,...

Pr (e1,1 = e2,1, e1,2 = e2,2, z1 (cid:9)= z2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)

(cid:2)
(cid:2)
(cid:2)
(cid:2)

j(cid:9)=i,j=2,6,10,...

j(cid:9)=i,j=1,5,9,...

i=2,6,10,...

i=1,5,9,...

⎧⎨
⎩
⎧⎨
⎩
⎧⎨
⎩
⎧⎨
⎩

i=3,7,11,...

j(cid:9)=i,j=3,7,11,...

=

+

+

+

⎫⎬
⎭
⎫⎬
⎭
⎫⎬
⎭
⎫⎬
⎭

Pr (z1 = i, z2 = j)

Pr (z1 = i, z2 = j)

We again use the basic probability formula Pr (z1 = i, z2 = j, i < j)
and the sum of (different) geometric series, for example,

[1 − r2]

3

+ [1 − r2]

7

+ [1 − r2]

11

+ ... =

[1 − r2]22−1
1 − [1 − r2]22 .

Similarly, for general b, we will need
[1 − r2]

2×2b−1

2b−1

+ [1 − r2]
(cid:9)
b(cid:10)

+ [1 − r2]
(cid:11)
1{e1,i = e2,i} = 1

Pr

After more algebra, we prove the general case:

3×2b−1

+ ... =

[1 − r2]2b−1
1 − [1 − r2]2b

.

∂A1,b
∂b =

It remains to show some useful properties of A1,b (same for

A2,b). The ﬁrst derivative of A1,b with respect to b is

Thus, A1,b is a monotonically decreasing function of b. Also,
[1 − r1]2b−2

2b−1 − r1

−
≤0

(cid:4)

−[1 − r1]2b

(cid:3)
(cid:15)
(cid:16)2
r1[1 − r1]2b−1 log(1 − r1) log 2
1 − [1 − r1]2b
(cid:15)
log(1 − r1) log 2 r1
1 − [1 − r1]2b

1 − [1 − r1]2b
(cid:3)
(cid:16)2
1 − [1 − r1]2b−1
(Note that log(1 − r1) ≤ 0)
(cid:16)
2b − 1
(cid:16)
(cid:15)
2b[1 − r1]2b−1
(cid:16)
[1 − r1]2b−2
2b − 1
1 − [1 − r1]2b
(cid:16)2
− 2b[1 − r1]2b−1r1 [1 − r1]
1 − [1 − r1]2b
(cid:15)
[1 − r1]2b−2
br1 − [1 − r1]
1 − 2
1 − [1 − r1]2b

2b−1 − r1
(cid:15)

[1 − r1]
(cid:15)

≤ 0.

(cid:16)2

2b−1

(cid:4)

(cid:3)

(cid:15)

=

=

=

2b

(cid:4)

,

1
2b

lim
r1→0

A1,b = lim
r1→0
[1 − r1]

∂A1,b
∂r1

Note that (1 − x)c ≥ 1− cx, for c ≥ 1 and x ≤ 1. Therefore A1,b
is a monotonically decreasing function of r1.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA680
