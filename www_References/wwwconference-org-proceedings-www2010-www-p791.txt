[1] H. Becker, A. Broder, E. Gabrilovich, V. Josifovski,
and B. Pang. Context transfer in search advertising.
In SIGIR ’09: Proc. of 32nd International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 656–657, 2009.

[2] S. Bickel and T. Scheﬀer. Dirichlet-enhanced spam

ﬁltering based on biased samples. In Advances in
Neural Information Processing Systems 19, pages
161–168. MIT Press, 2007.

[3] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P.

Kriegel, B. Sch¨olkopf, and A. J. Smola. Integrating

structured biological data by kernel maximum mean
discrepancy. In ISMB (Supplement of Bioinformatics),
pages 49–57, 2006.

[4] K. Brinker. Incorporating diversity in active learning
with support vector machines. In ICML ’03: Proc. of
the 20th International Conference on Machine
learning, pages 408–415, 2003.

[5] C. J. C. Burges. A tutorial on support vector

machines for pattern recognition. Data Mining and
Knowledge Discovery, 2:121–167, 1998.

[6] P. Chan and S. J. Stolfo. Toward scalable learning

with non-uniform distributions: Eﬀects and a
multi-classiﬁer approach. In KDD ’99: Proc. of the 4th
International Conference on Knowledge Discovery and
Data Mining, pages 164–168. AAAI Press, 1999.

[7] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.

Kegelmeyer. Smote: Synthetic minority over-sampling
technique. Journal of Artiﬁcial Intelligence Research,
16:321–357, 2002.

[8] J. Davis and M. Goadrich. The relationship between

precision-recall and roc curves. In In ICML ˇS06:
Proceedings of the 23rd international conference on
Machine learning, pages 233–240, 2006.

[9] M. DeGroot and S. Fienberg. The comparison and

evaluation of forecasters. The Statistician, 32:12–22,
1983.

[10] P. Domingos. Metacost: A general method for making

classiﬁers cost-sensitive. In Proc. of the 5th

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA799Model
Sensitivity Class 1
Sensitivity Class 2
Sensitivity Class 3
Sensitivity Class 4
Sensitivity Class 5
Sensitivity Class 6
Sensitivity Class 7
Sensitivity Class 8
Sensitivity Class 9
Sensitivity Class 10
Sensitivity Class 11
Sensitivity Class 12

% Lift in Prec@Rec=0.3 % Lift in Prec@Rec=0.8 % Lift in Area Under PRC AL Iterations

1113%

82%
265%
80%
46%
27%
655%
71%
144%
29%
940%
-1%

7400%

36%
97%
21%
36%
13%
512%
42%
24%
24%
633%
6%

963%
50%
186%
49%
32%
22%
255%
38%
129%
18%
860%
5%

10
46
40
42
35
41
10
20
30
23
17
24

Table 2: Performance of the deployed models. Percentage improvement in Precision at recall 30% and 80%
and Area under the Precision-Recall curve of the active learned model over the model prior to active learning.

International Conference on Knowledge Discovery and
Data Mining, pages 155–164, 1999.

[11] S. Ertekin, J. Huang, L. Bottou, and L. Giles.

Learning on the border: active learning in imbalanced
data classiﬁcation. In CIKM ’07: Proc. of the 16th
ACM Conference on Information and Knowledge
Management, pages 127–136, New York, NY, USA,
2007. ACM.

[12] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,
and C.-J. Lin. Liblinear: A library for large linear
classiﬁcation. J. Mach. Learn. Res., 9:1871–1874, 2008.

[21] D. Mladenic and M. Grobelnik. Feature selection for

unbalanced class distribution and naive bayes. In
Proc. of the 16th International Conference on
Machine Learning (ICML), pages 258–267, 1999.

[22] K. Nigam. Using maximum entropy for text

classiﬁcation. In IJCAI-99 Workshop on Machine
Learning for Information Filtering, pages 61–67, 1999.

[23] F. Provost. Machine learning from imbalanced data

sets 101 (extended abstract). In Proc. of AAAI
Workshop on Imbalanced Data Sets, 2000.

[24] J. R. Quinlan. C4.5: programs for machine learning.

[13] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi,

Morgan Kaufmann Publishers Inc., 1993.

and S. Sundararajan. A dual coordinate descent
method for large-scale linear svm. In ICML ’08: Proc.
of the 25th International Conference on Machine
Learning, pages 408–415, 2008.

[14] T. Joachims. Text categorization with support vector

machines: learning with many relevant features. In
Proc. of 10th European Conference on Machine
Learning, pages 137–142, 1998.

[15] M. V. Joshi, R. C. Agarwal, and V. Kumar.

Predicting rare classes: can boosting make any weak
learner strong? In KDD ’02: Proc. of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 297–306, 2002.

[16] M. V. Joshi, V. Kumar, and R. C. Agarwal.

Evaluating boosting algorithms to classify rare classes:
Comparison and improvements. In ICDM ’01: Proc.
of 1st IEEE International Conference on Data
Mining, 2001.

[17] M. Kubat, R. C. Holte, and S. Matwin. Machine

learning for the detection of oil spills in satellite radar
images. Mach. Learn., 30(2-3):195–215, 1998.

[18] J. Langford, L. Li, and T. Zhang. Sparse online

learning via truncated gradient. J. Mach. Learn. Res.,
10:777–801, 2009.

[19] A. McCallum and K. Nigam. A comparison of event

models for naive bayes text classiﬁcation, 1998.
[20] A. Mccallum and K. Nigam. Employing em in

pool-based active learning for text classiﬁcation. In
Proc. of the 15th International Conference on
Machine Learning, pages 350–358, 1998.

[25] M. Sugiyama and K. robert M¨uller. Model selection

under covariate shift. In Proc. of the International
Conference on Artiﬁcial Neural Networks. Springer,
2005.

[26] Y. Tang, S. Rrasser, P. Judge, and Y.-Q. Zhang. Fast
and eﬀective spam sender detection with granular svm
on highly imbalanced mail server behavior data.
International Conference on Collaborative Computing:
Networking, Applications and Worksharing, 0:27, 2006.
[27] S. Tong and D. Koller. Support vector machine active

learning with applications to text classiﬁcation. J.
Mach. Learn. Res., 2:45–66, 2002.

[28] K. Woods, J. Solka, C. Priebe, C. Doss, K. Bowyer,

and L. Clarke. Comparative evaluation of pattern
recognition techniques for detection of
microcalciﬁcations. J. of Intelligent Automation, 1993.

[29] R. Yan, Y. Liu, R. Jin, and A. Hauptmann. On

predicting rare classes with svm ensembles in scene
classiﬁcation. In In ICASSP, pages 21–24, 2003.

[30] B. Zadrozny and C. Elkan. Transforming classiﬁer

scores into accurate multiclass probability estimates.
In Proc. of the 8th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pages 694–699, 2002.

WWW 2010 • Full PaperApril 26-30 • Raleigh • NC • USA800
