
[1] N. Amado, J. Gama, and F. Silva. Parallel

implementation of decision tree learning algorithms.
Progress in Artiﬁcial Intelligence, pages 34–52, 2001.
[2] Y. Ben-Haim and E. Yom-Tov. A streaming parallel

decision tree algorithm. The Journal of Machine
Learning Research, 11:849–872, 2010.

[3] L. Breiman. Bagging predictors. Machine learning,

24(2):123–140, 1996.

[4] L. Breiman. Random forests. Machine learning,

45(1):5–32, 2001.

[5] L. Breiman, J. Friedman, C. J. Stone, and R. A.

Olshen. Classiﬁcation and regression trees. Chapman
& Hall/CRC, 1984.

[6] C. Burges. From RankNet to LambdaRank to

LambdaMART: An Overview. 2010.

[7] C. Burges, T. Shaked, E. Renshaw, M. Deeds,

N. Hamilton, and G. Hullender. Learning to rank
using gradient descent. In Internation Conference on
Machine Learning, pages 89–96, 2005.

[19] A. Lazarevic and Z. Obradovic. Boosting algorithms
for parallel and distributed learning. Distributed and
Parallel Databases, 11(2):203–229, 2002.

[20] P. Li, C. J. C. Burges, and Q. Wu. Mcrank: Learning

to rank using multiple classiﬁcation and gradient
boosting. In J. C. Platt, D. Koller, Y. Singer, and
S. T. Roweis, editors, NIPS. MIT Press, 2007.

[21] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor:
Benchmark dataset for research on learning to rank
for information retrieval. In Proceedings of SIGIR
2007 Workshop on Learning to Rank for Information
Retrieval, pages 3–10, 2007.

[22] A. Mohan, Z. Chen, and K. Q. Weinberger.

Web-search ranking with initialized gradient boosted
regression trees. Journal of Machine Learning
Research, Workshop and Conference Proceedings,
14:77–89, 2011.

[23] B. Panda, J. Herbach, S. Basu, and R. Bayardo.

Planet: Massively parallel learning of tree ensembles
with mapreduce. Proceedings of the Very Large
Database Endowment, 2(2):1426–1437, 2009.

[8] Z. Cao and T.-Y. Liu. Learning to rank: From

[24] D. Pavlov and C. Brunk. Bagboo: Bagging the

pairwise approach to listwise approach. In Proceedings
of the 24th International Conference on Machine
Learning, pages 129–136, 2007.

gradient boosting. Talk at Workshop on Websearch
Ranking at the 27th International Conference on
Machine Learning, 2010.

[9] O. Chapelle and Y. Chang. Yahoo! Learning to Rank

[25] J. Shafer, R. Agrawal, and M. Mehta. SPRINT: A

Challenge overview. Journal of Machine Learning
Research, Workshop and Conference Proceedings,
14:1–24, 2011.

scalable parallel classiﬁer for data mining. In
Proceedings of the International Conference on Very
Large Data Bases, pages 544–555, 1996.

[10] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.

[26] M. Snir. MPI–the Complete Reference: The MPI core.

Expected reciprocal rank for graded relevance. In
Proceeding of the 18th ACM Conference on
Information and Knowledge Management, pages
621–630. ACM, 2009.

[11] O. Chapelle and M. Wu. Gradient descent

optimization of smoothed information retrieval
metrics. Information Retrieval Journal, Special Issue
on Learning to Rank for Information Retrieval, 2010.
to appear.

[12] J. Darlington, Y. Guo, J. Sutiwaraphun, and H. To.

Parallel induction algorithms for data mining.
Advances in Intelligent Data Analysis Reasoning about
Data, pages 437–445, 1997.

[13] A. Freitas and S. Lavington. Mining very large

databases with parallel processing. Springer, 1998.

[14] J. Friedman. Greedy function approximation: a
gradient boosting machine. Annals of Statistics,
29:1189–1232, 2001.

[15] J. Gehrke, R. Ramakrishnan, and V. Ganti.

RainForestˆa ˘AˇTa framework for fast decision tree
construction of large datasets. Data Mining and
Knowledge Discovery, 4(2):127–162, 2000.

The MIT Press, 1998.

[27] A. Srivastava, E. Han, V. Kumar, and V. Singh.

Parallel formulations of decision-tree classiﬁcation
algorithms. High Performance Data Mining, pages
237–261, 2002.

[28] M. Taylor, J. Guiver, S. Robertson, and T. Minka.
SoftRank: optimizing non-smooth rank metrics. In
Proc. 1st ACM Int’l Conf. on Web Search and Data
Mining, pages 77–86, 2008.

[29] N. Uyen and T. Chung. A new framework for

distributed boosting algorithm. Future Generation
Communication and Networking, 1:420–423, 2007.

[30] G. Webb. Multiboosting: A technique for combining

boosting and wagging. Machine learning,
40(2):159–196, 2000.

[31] J. Ye, J. Chow, J. Chen, and Z. Zheng. Stochastic

gradient boosted distributed decision trees. In CIKM
’09: Proceeding of the 18th ACM Conference on
Information and Knowledge Management, pages
2061–2064. ACM, 2009.

[32] C. Yu and D. Skillicorn. Parallelizing boosting and

bagging. 2001.

[16] R. Herbrich, T. Graepel, and K. Obermayer. Large

[33] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A

margin rank boundaries for ordinal regression, pages
115–132. MIT Press, Cambridge, MA, 2000.

[17] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4):422–446, 2002.

[18] T. Joachims. Optimizing search engines using
clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Mining
(KDD). ACM, 2002.

support vector method for optimizing average
precision. In Proc. 30th Int’l ACM SIGIR Conf. on
Research and Development in Information Retrieval,
pages 271–278, 2007.

[34] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen,

and G. Sun. A general boosting method and its
application to learning ranking functions for web
search. In NIPS, 2007.

WWW 2011 – Session: RankingMarch 28–April 1, 2011, Hyderabad, India396
