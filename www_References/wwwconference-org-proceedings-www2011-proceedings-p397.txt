
[1] P. Bailey, N. Craswell, I. Soboroﬀ, P. Thomas, A. P.
de Vries, and E. Yilmaz. Relevance assessment: are
judges exchangeable and does it matter. In SIGIR ’08:
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 667–674, 2008.

[2] C. Buckley and E. M. Voorhees. Retrieval evaluation

with incomplete information. In SIGIR ’04:
Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 25–32, 2004.

[3] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal

test collections for retrieval evaluation. In E. N.
Efthimiadis, S. T. Dumais, D. Hawking, and
K. J¨arvelin, editors, SIGIR, pages 268–275, 2006.

[4] B. Carterette and R. Jones. Evaluating search engines

by modeling the relationship between relevance and
clicks. In J. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 217–224. MIT Press, Cambridge,
MA, 2008.

[5] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.

Expected reciprocal rank for graded relevance. In
CIKM ’09: Proceeding of the 18th ACM conference on
Information and knowledge management, pages
621–630, 2009.

[6] O. Chapelle and Y. Zhang. A dynamic bayesian
network click model for web search ranking. In
WWW. 2009.

[7] N. Craswell, O. Zoeter, M. J. Taylor, and B. Ramsey.

An experimental comparison of click position-bias
models. In M. Najork, A. Z. Broder, and
S. Chakrabarti, editors, WSDM, pages 87–94, 2008.

[8] G. E. Dupret and B. Piwowarski. A user browsing

model to predict search engine click data from past
observations. In SIGIR ’08: Proceedings of the 31st
annual international ACM SIGIR conference on

Research and development in information retrieval,
pages 331–338, New York, NY, USA, 2008. ACM.

[9] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of IR techniques. ACM Trans. Inf. Syst.,
20(4):422–446, 2002.

[10] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In SIGIR ’05: Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 154–161, New York, NY, USA, 2005. ACM
Press.

[11] T. Joachims and F. Radlinski. Search engines that

learn from implicit feedback. IEEE Computer,
40(8):34–40, August 2007.

[12] A. Moﬀat and J. Zobel. Rank-biased precision for

measurement of retrieval eﬀectiveness. ACM Trans.
Inf. Syst., 27(1):1–27, 2008.

[13] B. Piwowarski, G. Dupret, and R. Jones. Mining user
web search activity with layered bayesian networks or
how to capture a click in its context. In WSDM ’09:
Proceedings of the Second ACM International
Conference on Web Search and Data Mining, pages
162–171, New York, NY, USA, 2009. ACM.

[14] F. Radlinski and T. Joachims. Active exploration for

learning rankings from clickthrough data. In ACM
SIGKDD International Conference On Knowledge
Discovery and Data Mining (KDD), 2007.

[15] I. Soboroﬀ. A comparison of pooled and sampled

relevance judgments. In SIGIR ’07: Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 785–786, New York, NY, USA, 2007. ACM.

[16] A. Trotman, N. Phrao, and D. Jenkinson. Can we at
least agree on something? In Proc. SIGIR Workshop
on Focused Retrieval. 2007.

WWW 2011 – Session: EvaluationMarch 28–April 1, 2011, Hyderabad, India406
