
[1] Xyleme XML server & business document

management.
http://www.xyleme.com/page/xml storage/.

[2] S. Abiteboul, R. Hull, and V. Vianu. Foundations of

Databases. Addison-Wesley, 1995.

[3] M. Arenas, W. Fan, and L. Libkin. On verifying

consistency of XML speciﬁcations. In PODS, pages
259–270, 2002.

[4] M. Arenas and L. Libkin. A normal form for XML

documents. ACM Trans. Database Syst., 29:195–232,
2004.

WWW 2007 / Track: XML and Web DataSession: Parsing, Normalizing, and Storing XML1090[5] M. Arenas and L. Libkin. An information-theoretic

approach to normal forms for relational and XML
data. J. ACM, 52(2):246–283, 2005.

XML-to-SQL query translation literature: The state
of the art and open problems. In XSym’03, LNCS
2824, pages 1–18, 2003.

[6] D. Barbosa, J. Freire, and A. O. Mendelzon.

[27] T. T. Lee. An information-theoretic analysis of

Designing information-preserving mapping schemes for
XML. In VLDB, pages 109–120, 2005.

[7] C. Beeri, P. A. Bernstein, and N. Goodman. A

sophisticate’s introduction to database normalization
theory. In VLDB, pages 113–124, 1978.

[8] P. A. Bernstein. Synthesizing third normal form

relations from functional dependencies. ACM Trans.
Database Syst., 1(4):277–298, 1976.

[9] J. Biskup. Achievements of relational database schema

design theory revisited. In Semantics in Databases,
pages 29–54, 1995.

[10] P. Buneman, S. B. Davidson, W. Fan, C. S. Hara, and
W. C. Tan. Keys for XML. In World Wide Web, pages
201–210, 2001.

[11] R. Cavallo and M. Pittarelli. The theory of

relational databases - part i: Data dependencies and
information metric. IEEE Trans. Software Eng.,
13(10):1049–1061, 1987.

[28] M. Levene, M. Levene, and G. Loizou. A Guided Tour
of Relational Databases and Beyond. Springer-Verlag,
London, UK, 1999.

[29] M. Levene and G. Loizou. Why is the snowﬂake

schema a good data warehouse design? Inf. Syst.,
28(3):225–240, 2003.

[30] Z. H. Liu, M. Krishnaprasad, and V. Arora. Native
XQuery processing in oracle XMLDB. In SIGMOD,
pages 828–833, 2005.

[31] M. Nicola and B. van der Linden. Native XML

support in DB2 universal database. In VLDB, pages
1164–1174, 2005.

probabilistic databases. In VLDB, pages 71–81, 1987.

[32] A. Schmidt, M. L. Kersten, M. Windhouwer, and

[12] E. F. Codd. Normalized database structure: A brief

tutorial. In ACM SIGFIDET Workshop on Data
Description, Access and Control, 1971.

[13] E. F. Codd. Further normalization of data base
relational model. In Courant Computer Science
Symposium 6: Data Base Systems, pages 33–64, 1972.

[14] E. F. Codd. Recent investigations in relational

database systems. In Information Processing, pages
1017–1021, 1974.

[15] T. M. Cover and J. A. Thomas. Elements of

Information Theory. John Wiley & sons, 1991.

[16] M. M. Dalkilic and E. L. Roberston. Information

dependencies. In PODS, pages 245–253, 2000.

[17] S. Davidson, W. Fan, C. Hara, and J. Qin.

Propagating XML constraints to relations. In ICDE,
pages 543–554, 2003.

[18] A. Deutsch, M. Fernandez, and D. Suciu. Storing
semistructured data with STORED. In SIGMOD,
pages 431–442, 1999.

[19] A. Deutsch and V. Tannen. MARS: A system for

publishing XML from mixed and redundant storage.
In VLDB, pages 201–212, 2003.

[20] D. W. Embley and W. Y. Mok. Developing XML
documents with guaranteed “good” properties. In
ER’01, pages 426–441.

[21] D. Florescu and D. Kossmann. Storing and querying

XML data using an RDBMS. IEEE Data Eng.
Bulletin, 22(3):27–34, 1999.

[22] P. C. Kanellakis. Elements of relational database

theory. pages 1073–1156, 1990.

[23] C.-C. Kanne and G. Moerkotte. A linear-time

algorithm for optimal tree sibling partitioning and
approximation algorithms in Natix. In VLDB, 2006.

[24] M. Kifer, A. Bernstein, and P. M. Lewis. Database

systems : an application-oriented approach.
Addison-Wesley, 2006.

[25] S. Kolahi and L. Libkin. On redundancy vs

dependency preservation in normalization: An
information-theoretic study of 3NF. In PODS, pages
114–123, 2006.

[26] R. Krishnamurthy, R. Kaushik, and J. F. Naughton.

F. Waas. Eﬃcient relational storage and retrieval of
XML documents. In Selected papers from the Third
International Workshop WebDB 2000 on The World
Wide Web and Databases, pages 137–150.
Springer-Verlag, 2001.

[33] J. Shanmugasundaram, K. Tufte, C. Zhang, G. He,

D. J. DeWitt, and J. F. Naughton. Relational
databases for querying XML documents: Limitations
and opportunities. In VLDB, pages 302–314, 1999.

[34] I. Tatarinov, S. D. Viglas, K. Beyer,

J. Shanmugasundaram, E. Shekita, and C. Zhang.
Storing and querying ordered XML using a relational
database system. In SIGMOD, pages 204–215, 2002.
[35] M. W. Vincent and J. Liu. Functional dependencies

for XML. In APWEB, pages 22–34, 2003.

[36] M. W. Vincent, J. Liu, and C. Liu. A redundancy free

4NF for XML. In XSym, pages 254–266, 2003.

[37] M. W. Vincent, J. Liu, and C. Liu. Strong functional

dependencies and their application to normal forms in
XML. ACM TODS, 29(3):445–462, 2004.

[38] J. Wang and R. W. Topor. Removing XML data

redundancies using functional and equality-generating
dependencies. In Australian Database Conference,
pages 65–74, 2005.

[39] M. Yoshikawa, T. Amagasa, T. Shimura, and

S. Uemura. XRel: a path-based approach to storage
and retrieval of XML documents using relational
databases. ACM Trans. Inter. Tech., 1(1):110–141,
2001.

[40] C. Zaniolo. A new normal form for the design of

relational database schemata. ACM Transactions on
Database Systems, 7(3):489–499, 1982.

APPENDIX

A. DEFINITION OF THE INFORMATION(cid:173)

THEORETIC MEASURE

A.1 Basics of information theory

The main concept of information theory is that of entropy,
which measures the amount of information provided by a

WWW 2007 / Track: XML and Web DataSession: Parsing, Normalizing, and Storing XML1091certain event. Assume that an event can have n diﬀerent
outcomes s1, . . ., sn. Then for a probability space A =
({s1, . . . , sn}, PA), where PA is a probability distribution,
its entropy is deﬁned as

H(A) =

n

Xi=1

PA(si) log

1

PA(si)

.

0 = 0, since limx→0 x log 1

For probabilities that are zero, we adopt the convention that
0 log 1
It is known that
0 ≤ H(A) ≤ log n, with H(A) = log n only for the uniform
distribution PA(si) = 1/n [15].

x = 0.

We shall also need the concept of conditional entropy of

B assuming A.

Suppose we have two probability spaces

A = ({s1, . . . , sn}, PA),
B = ({s0
m}, PB)

1, . . . , s0

j, si) of all the events (s0

and probabilities P (s0
j, si). Note
that PA and PB need not be independent. Then the con-
ditional entropy of B given A, denoted by H(B | A), gives
the average amount of information provided by B if A is
known [15]. If

P (s0

j | si) =

P (s0
j, si)
PA(si)

are conditional probabilities, then the conditional entropy is
deﬁned as

H(B | A) =

n

Xi=1

(cid:18)PA(si)

m

Xj=1

P (s0

j | si) log

1
P (s0

j | si)(cid:19).

A.2 Relative information content

We now give a detailed deﬁnition of relative information
content from [5] that was used to justify relational and XML
normal forms [5, 25].

Unlike other proposed information-theoretic measures [27,
11, 16, 29] that work only at the level of data, this measure
takes into account both data and schema constraints.

Fix a schema S and a set Σ of constraints, and let
I ∈ inst(S, Σ). We want to deﬁne RicI (p | Σ), the relative
information content of a position p ∈ Pos(I) with respect
to the set of constraints Σ.

Formally, we assume that I has n positions (which we
enumerate as 1, . . . , n), and ﬁx an n-element set of variables
{vi | 1 ≤ i ≤ n}. Let Ω(I, p) be the set of all 2n−1 vectors
(a1, . . . , ap−1, ap+1, . . . , an) such that for every i ∈ [1, n] −

{p}, ai is either vi or the value in the i-th position of I. We
make this into a probability space A(I, p) = (Ω(I, p), Pu)
with the uniform distribution Pu(¯a) = 21−n.

We next deﬁne conditional probabilities Pk(a | ¯a) that
show how likely a is to occur in position p, if values are re-
moved from I according to the tuple ¯a ∈ Ω(I, p) Let I(a,¯a) be
obtained from I by putting a in position p, and ai in position
i 6= p. A substitution is a map σ : ¯a → [1, k] that assigns
a value to each ai which is a variable, and leaves other ais
intact. We let SATk
Σ(I(a,¯a)) be the set of all substitutions
σ such that σ(I(a,¯a)) |= Σ and |σ(I(a,¯a))| = |I| (the latter
ensures that no two tuples collapse as the result of applying
σ). Then Pk(a | ¯a) is deﬁned as:

|SATk

Σ(I(a,¯a))|

.

Σ(I(b,¯a))|

Pb∈[1,k] |SATk

I (p | Σ) as

Pk(a | ¯a) log

1

Pk(a | ¯a)(cid:19).

Pk(a | ¯a) =

With this, we deﬁne Rick

X¯a∈Ω(I,p)

(cid:18) 1
2n−1 Xa∈[1,k]
Since Pa∈[1,k] Pk(a | ¯a) log

1

Pk (a|¯a) measures the amount of
information in p, given constraints Σ and some missing val-
ues in I, represented by the variables in ¯a, our measure
Rick
I (p | Σ) is the average such amount over all ¯a ∈ Ω(I, p).

To see that Rick

I (p | Σ) is a conditional entropy, deﬁne

P 0

k(a) =

1

2n−1 X¯a∈Ω(I,p)

Pk(a | ¯a) .

It is a probability distribution on [1, k] (intuitively, it says
how likely an element from [1, k] is to satisfy Σ when put
in position p, given all possible interactions between p and
sets of positions in I). If Bk
Σ(I, p) is the probability space
([1, k], P 0

I (p | Σ) is the conditional entropy:

k), then Rick

Rick

I (p | Σ) = H(Bk

Σ(I, p) | A(I, p)).

Since the domain of Bk

Σ(I, p) is [1, k], we have 0 ≤ Rick

I (p |
Σ) ≤ log k. To normalize this, we consider the ratio
Rick
I (p | Σ)/ log k. The key observation of [5] is that for
most reasonable constraints Σ (certainly for all deﬁnable in
ﬁrst-order logic), this sequence converges as k → ∞, and we
thus deﬁne

RicI (p|Σ) = lim
k→∞

Rick

I (p | Σ)
log k

.

WWW 2007 / Track: XML and Web DataSession: Parsing, Normalizing, and Storing XML1092
