[1]  Ahmad  N.  and  Beg  M.  M.  S.    Fuzzy  Logic  Based  Rank
Aggregation  Methods  for
the  World  Wide  Web,  In
Proceedings  of  the  International  Conference  on  Artificial
Intelligence in Engineering and Technology, Malaysia, 2002,
363-368.

[2]  Aslam,  J.  A.  and  Montague,  M.   Models  for  Metasearch.  In
Proceedings  of  the  24th  Annual  International  ACM  SIGIR
Conference  on  Research  and  Development  in  Information
Retrieval. ACM Press, New York, 2001, 276-284.

[3]  Baeza-Yates,  R.  and  Ribeiro-Neto,  B.  Modern  Information

Retrieval. Addison Wesley, 1999.

[4]  Beg,  M.  M.  S.    Parallel  Rank  Aggregation  for  the  World
Wide Web. World Wide Web. Kluwer Academic Publishers,
vol 6, issue 1, 5-22. March 2004.

[5]  Borda, J. C. Mémoire sur les élections au scrutin. Histoire de

l’Acad´emie Royale des Sciences, 1781

[6]  Boyd,  S.  and  Vendenberghe,  L.  Convex  Optimization.

Cambridge, U. K. Cambridge Univ. Press 2003.

[7]  Dwork,  C.,  Kumar,  R.,  Naor,  M.,  and  Sivakumar,  D.  Rank
Aggregation  Methods  for  the  Web.  In  Proceedings  of  the
10th International World Wide Web Conference. 2001, 613-
622.

[8]  Dwork,  C.,  Kumar,  R.,  Naor,  M.,  and  Sivakumar,  D.  Rank

Aggregation revisited. 2001.  Manuscript.

[9]  Fagin, R., Kumar, R., and Sivakumar, D. Efficient Similarity
Search  and  Classification  via  Rank  Aggregation.
In
Proceedings  of  the  2003  ACM  SIGMOD  International
Conference on Management of Data. San Diego, 2003, 301-
312.

[10] Fagin,  R.,  Lotem,  A.,  and  Naor,  M.  Optimal  Aggregation
Algorithm  for  Middleware.  In  Proceedings  of  the  Twentieth
ACM  SIGMOD-SIGACT-SIGART  Symposium  on  Principles
of  Database  Systems.  Santa  Barbara,  California,  United
States, 2001, 102-113.

[11] Fox,  E.  A.  and  Shaw,  J.  A.  Combination  of  Multiple
Searches.  In  Proceedings  of  the  Second  Text  Retrieval
Conference, 1994.

[12] Fujisawa,  K.,  Fukuda,  M.,  Kojima,  M.,  and  Nakata,  K.
Numerical  Evaluation  of
(SemiDefinite
Programming  Algorithm).  High  Performance  Optimization,
Kluwer Academic Press, 267-301, 2000.

the  SDPA

[13] Hull,  D.  A.,  Pedersen,  J.  O.,  and  Schütze,  H.  Method
Combination  for  Document  Filtering.  In  Proceedings  of  the
19th  Annual  International  ACM  SIGIR  Conference  on
Research  and  Development  in  Information  Retrieval.  ACM
Press, 1996, 279-287.

WWW 2007 / Track: SearchSession: Search Quality and Precision487[14] Jarvelin,  K.and  Kekalainen,  J.    IR  Evaluation  Methods  for
Retrieving  Highly  Relevant  Documents.  In  Proceedings  of
the  23rd  Annual  International  ACM  SIGIR  Conference  on
Research  and  Development  in  Information  Retrieval.  ACM
Press, 2000, 41-48.

[15] Jarvelin,  K..  and  Kekalainen,  J.  Cumulated  Gain-Based
Evaluation  of  IR  Techniques.  ACM  Transactions  on
Information Systems, 2002.

[16] Klerk,  E.  Aspects  of  Semidefinite  Programming:  Interior
Point  Algorithms  and  Selected  Applications.  Applied
Optimization  Series,  Volume  65.  Kluwer  Academic
Publishers, March 2002, 300 pp.

[17] Lam,  K.  W.  and  Leung,  C.  H.  Rank  Aggregation  for  Meta-
search  Engines.  In  Proceedings  of  the  13th  International
World Wide Web Conference. 2004.

[18] Manmatha,  R..,  Rath,  T.,    and  Feng,  F.  Modeling  Score
Distributions  for  Combining  the Outputs of  Search  Engines.
In Proceedings of the 24th Annual International ACM SIGIR
Conference  on  Research  and  Development  in  Information
Retrieval. ACM Press, 2001, New York.

[19] Manmatha,  R.  and  Sever,  H.  A  Formal  Approach  to  Score
Normalization  for  Meta-search.  In  Proceedings  of  HLT’02,
2002, 88-93.

[20] Montague,  M.  and  Aslam,  J.  A.  Relevance  Score
Normalization  for  Meta-search.  In  Proceedings  of  the  10th
Conference  on  Information  and  Knowledge  Management.
Atlanta, GA, 2001, 427-433.

[21] Monteiro,  R.  D.  C.  First-  and  Second-Order  Methods  for

Semidefinite Programming. Georgia Tech, January 2003.

[22] Nallapati,  R.  Discriminative  Models

Information
Retrieval.  In  Proceedings  of  the  27th  Annual  International
ACM  SIGIR  Conference  on  Research  and  Development  in
Information retrieval. ACM Press, 2004, 64-71.

for

[23] Pardalos,  P.M.  and  Wolkowicz,  H.  Topics  in  Semidefinite
and Interior Point Methods. Fields Institute Communications
18, AMS, Providence, Rhode Island, 1998.

[24] Randa,  M.  E.  and  Straccia,  U.  Web  metasearch:  Rank  vs.
Score  based  Rank  Aggregation  Methods.  In  Proceedings  of
the 2003 ACM Symposium on Applied Computing, March 09-
12, 2003, Melbourne, Florida.

[25] Robertson, S. E.  Overview of the Okapi Projects. Journal of

Documentation, Vol. 53, No. 1, 1997, pp. 3-7.



APPENDIX


Proof of Lemma 1



Lemma 1: Let 𝛯 =  𝜉𝑖 𝑇

𝑙
𝑖=1,⋯,𝑛 =
𝑘=1

𝛼𝑘 𝑃𝑘

𝑇


𝑥 − 𝑥, we have

  𝜩 1 ≤ 2 − 2𝛼𝑇𝐴𝑥, where 𝐴 =

(1)

(1) ⋯ 𝑝𝑛𝑛
𝑝11
⋮
⋮
(𝑙)
(𝑙) ⋯ 𝑝𝑛𝑛
𝑝11

⋱

 .

Proof:

𝑙
Define  𝑃 =
𝑘=1
(𝑘)
𝛼𝑘 𝑝𝑗𝑖

𝑙
𝑓𝑖𝑗 =
𝑘=1
rewrite Ξ as

 and  𝐹 =  𝑓𝑖𝑗

= 𝑃𝑇.  It  is  clear  that
𝛼𝑘 𝑃𝑘
.  Using  fi  to  denote  the  ith  row  of  F,  we  can

𝑛×𝑛

 𝛯 =  𝜉𝑖 𝑇

𝑖=1,⋯,𝑛 = 𝐹𝑥 − 𝑥

or,

𝑛
𝜉𝑖 = 𝑓𝑖 𝑥 − 𝑥𝑖 =  𝑓𝑖𝑖 − 1 𝑥𝑖 +
𝑗 =1,𝑗 ≠𝑖

𝑓𝑖𝑗 𝑥𝑗

       (8.1.1)

Because  the  transition  probability  matrix  has  identical  rows,  for
the right-hand side of equation (8.1.1),  𝑓𝑖𝑖 − 1 𝑥𝑖  is non-positive
and  the  others  are  non-negative.  Therefore,  we  can  get  an  upper
bound of |𝜉𝑖| as follows by using the properties of ℓ1-norm
𝑛
|𝜉𝑖| ≤  1 − 𝑓𝑖𝑖  𝑥𝑖 +
𝑗 =1,𝑗 ≠𝑖

𝑛
=  1 − 2𝑓𝑖𝑖  𝑥𝑖 +
𝑗 =1

𝑓𝑖𝑗 𝑥𝑗

𝑓𝑖𝑗 𝑥𝑗





Applying the result to each element in Ξ yields

 𝜩 1 ≤    1 − 2𝑓𝑖𝑖  𝑥𝑖

𝑛
+
𝑗 =1

𝑓𝑖𝑗 𝑥𝑗



𝑛
𝑛
𝑖=1
𝑖=1
𝑛
𝑖=1 = 1 and    𝑓𝑖𝑗
𝑥𝑗    𝑓𝑖𝑗

𝑛
=
𝑗 =1

𝑛
𝑖=1

𝑛
𝑖=1

𝑓𝑖𝑗 𝑥𝑗

= 1, we obtain



𝑛
=
𝑗 =1 = 1

𝑥𝑗

Considering that   𝑥𝑖

𝑛

𝑗 =1

𝑛
𝑖=1

and

𝑛
𝑖=1

   1 − 2𝑓𝑖𝑖  𝑥𝑖

=   𝑥𝑖
𝑙
If further considering 𝑓𝑖𝑖 =
𝑘=1

(𝑘)

𝛼𝑘 𝑝𝑖𝑖

, we obtain

𝑛
𝑖=1 − 2   𝑓𝑖𝑖 𝑥𝑖

𝑛
𝑖=1

= 1 − 2   𝑓𝑖𝑖 𝑥𝑖

𝑛
𝑖=1



 𝜩 1 ≤ 2 − 2   𝑓𝑖𝑖 𝑥𝑖

𝑛
𝑖=1

𝑙
= 2 − 2
𝑘=1

𝑛
𝑖=1

(𝑘)

𝛼𝑘 𝑝𝑖𝑖

  𝑥𝑖



By  using  matrix  form  to  represent  this  inequality,  we  eventually
have

 𝜩 1 ≤ 2 − 2𝛼𝑇

(1)

(1) ⋯ 𝑝𝑛𝑛
𝑝11
⋮
⋮
(𝑙)
(𝑙) ⋯ 𝑝𝑛𝑛
𝑝11

⋱

  𝑥 = 2 − 2𝛼𝑇𝐴𝑥.

[26] Sese,  J.  and  Morishita,  S.  Rank  Aggregation  Method  for
Biological  Databases.  Genome  Informatics,  12:  506-507,
2001.

[27] Van Erp M. and Schomaker, L. Variants of the Borda Count
Method  for  Combining  Ranked  Classifier  Hypotheses.  In
Proceedings of the 7th International Workshop on Frontiers
in Handwriting Recognition.  Amsterdam, 2000, 443-452.

[28] Vogt,  C.  and  Cottrel,  G.  W.  Fusion  via  a  Linear
Combination  of  Scores.  Information  Retrieval,  v.1  n.3,
p.151-173, October 1999.

[29] Semidefinite  Programming.  http://www-user.tu-chemnitz.de

/~helmberg/semidef.html.

[30] SDPA Online for Your Future. http://grid.r.dendai.ac.jp/sdpa/.

.

Proof of Proposition 2



Proposition 2: The optimization problem in (4.3) is equivalent
to the following Semidefinite Programming problem,

max𝜆,γ         γ
𝑠. 𝑡.   𝜆 ≥ 0

1

2𝑈𝑇

−𝛬2

𝑇𝑒2 − 2𝜆0 − 𝛾

  ≽ 0

 (8.2.1)

𝐻0 + 𝜆0𝐷



1

2𝑈
𝑇𝐻1 + 𝛬2

where 𝑈 = 𝛬1

𝑇𝐻2 + 𝛬3

𝑇𝐻3, and 𝜆 = (𝜆0, 𝛬1

𝑇, 𝛬2

𝑇, 𝛬3

𝑇)𝑇.

Proof:

Considering that 𝛽 = (𝛼1, … , 𝛼𝑙, 𝑥1, … , 𝑥𝑛 , 𝑡1, … , 𝑡𝑚 )𝑇, we always
have

𝑙
𝛽𝑇𝐷𝛽 = 𝛼𝑇𝛼 + 𝑥𝑇𝑥 ≤
𝑘 =1

𝛼𝑘



2

+    𝑥𝑖

𝑛
𝑖=1

 2 = 2

where 𝐷 = 𝑑𝑖𝑎𝑔(𝑒𝑙+𝑛

𝑇 , 0𝑚

𝑇 ).

WWW 2007 / Track: SearchSession: Search Quality and Precision488It  is  clear  that  if  we  add  this  redundant  constraint  to  the
optimization  problem  (4.3),  its  optimal  solution  will  not  change
because the feasible region has not changed. In this way,  we  can
transform  the  optimization  problem  (4.3)  into  the  following
quadratically constrained quadratic optimization (QCQP) problem.

min𝛽 𝛽𝑇𝐻0𝛽
𝑠. 𝑡.   𝛽𝑇𝐷𝛽 ≤ 2

                            (8.2.2)

     𝐻1𝛽 ≤ 0
       𝐻2𝛽 = 𝑒2
     𝐻3𝛽 < 0

The Lagrangian of (8.2.2) is

𝐿 𝜆, 𝛽

𝑇𝐻1𝛽 + 𝛬2

𝑇(𝐻2𝛽 − 𝑒2) + 𝛬3

= 𝛽𝑇𝐻0𝛽 + 𝜆0 𝛽𝑇𝐷𝛽 − 2  + 𝛬1
= 𝛽𝑇(𝐻0 + 𝜆0𝐷)𝛽 + 𝑈𝛽 − 𝛬2
where 𝛬1 =  𝜆1, 𝜆2, … , 𝜆𝑙+𝑚  𝑇, 𝛬2 =  𝜆𝑙+𝑚 +1, 𝜆𝑙+𝑚 +2 𝑇 ,
𝑇, 𝛬3
𝛬3 =  𝜆𝑙+𝑚 +3, 𝜆𝑙+𝑚 +4, … , 𝜆𝑙+𝑛+2𝑚 +2 𝑇, 𝜆 = (𝜆0, 𝛬1
and 𝑈 = 𝛬1

𝑇𝑒2 − 2𝜆0

𝑇𝐻3.

𝑇𝐻1 + 𝛬2

𝑇𝐻2 + 𝛬3

𝑇, 𝛬2

𝑇)𝑇,

𝑇𝐻3𝛽

to  β  exists,  one  can

According  to  the  optimization  theory  [6],  if  the  infimum  of
𝐿 𝜆, 𝛽   with  respect
the
minimization  of  the  objective  function  in  the  primal  problem
the  dual  function  𝑔 𝜆  =
(8.2.2)
inf𝛽 𝐿 𝜆, 𝛽 . The condition for this transformation is existence of
the  infimum  of  𝐿 𝜆, 𝛽  .  We  will  discuss  this  condition  in  the
following three cases.

the  maximization  of

transform

to

1)

If (𝐻0 + 𝜆0𝐷) is  positive-definite,  then  function 𝐿 𝜆, 𝛽 is  a
convex  quadratic  function  of  β.  Therefore,  we  can  find  the
infimum from the optimality condition:

∇𝛽 𝐿 𝜆, 𝛽  = 2(𝐻0 + 𝜆0𝐷)𝛽 + 𝑈𝑇 = 0

which yields

𝛽 = −

1

2

(𝐻0 + 𝜆0𝐷)−1𝑈𝑇                    (8.2.3)

If  (𝐻0 + 𝜆0𝐷)  is  strict  positive  semidefinite,  using  the
pseudo-inverse in [6], we can get a  relaxation on the above
condition.  That  is  if 𝑈 ∈ 𝑟𝑎𝑛(𝐻0 + 𝜆0𝐷 ),  we  can  get  the
dual function as follows2,

𝑔 𝜆  = −𝛬2

𝑇𝑒2 − 2𝜆0 −

1
4

𝑈(𝐻0 + 𝜆0𝐷)†𝑈𝑇



2)  Otherwise  function 𝐿 𝜆, 𝛽   has  no  lower  bound,  thus  the

problem (8.2.2) has no solution.

With  the  above  discussions,  we  conclude  if  and  only  if (𝐻0 +
𝜆0𝐷) ≽ 0 ,  𝐿 𝜆, 𝛽  has  an  infimum  and  the  corresponding
optimization  problem  (8.2.2)  can  be  transformed  to  its  dual
problem3.  As  a  result,  we  can  solve  the  dual  problem  in  (8.2.4)
and get the solution for (8.2.2).

max𝜆  𝑔(𝜆)
𝑠. 𝑡.   𝜆 ≥ 0

            (8.2.4)

(𝐻0 + 𝜆0𝐷) ≽ 0

One  can  also  find  that  𝑔 𝜆   is  the  Schur  complement  [6]  of

(𝐻0 + 𝜆0𝐷) in  the  matrix

𝐻0 + 𝜆0𝐷

1

2𝑈

1

2𝑈𝑇
𝑇𝑒2 − 2𝜆0

−𝛬2

   .  In  this

situation,  (8.2.4)  can  be  further  formulated  as  a  Semidefinite
Programming (SDP) problem with respect to variables 𝛾 and 𝜆.

max𝜆,γ         𝛾
𝑠. 𝑡.     𝜆 ≥ 0

    (8.2.5)

𝐻0 + 𝜆0𝐷



1

2𝑈𝑇

1

2𝑈

−𝛬2

𝑇𝑒2 − 2𝜆0 − 𝛾

  ≽ 0

For problem (8.2.2), there exists a β that makes the following two
inequalities  true:  𝛽𝑇𝐷𝛽 < 2 ,  𝐻1𝛽 < 0 ,  and  𝐻3𝛽 < 0 .  That  is,
problem (8.2.2) is strictly feasible, and thus the optimal values of
(8.2.2) and its Lagrange dual problem (8.2.4) are equivalent [6].

Recall that optimization problem (4.3) and (8.2.2) are equivalent;
according  to  the  strong  duality  theorem  [6],  problem  (4.3)  is
equivalent to the SDP problem (8.2.5).

Accordingly, we get the dual function



𝑔 𝜆  = −𝛬2

𝑇𝑒2 − 2𝜆0 −

1
4

𝑈(𝐻0 + 𝜆0𝐷)−1𝑈𝑇

which

is  a  concave  quadratic

function  of

.


2 𝑀† is the pseudo-inverse of matrix M [6].
3 𝑀 ≽ 0 means that matrix M is semi-positive definite [6].

WWW 2007 / Track: SearchSession: Search Quality and Precision489
