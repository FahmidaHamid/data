[1] S. Robertson and D. A. Hull. The trec-9 ﬁltering track ﬁnal

report. In TREC9, pages 25–40, 2000.

[2] J. Laﬀerty and C. Zhai. Document language models, query
models, and risk minimization for information retrieval. In
SIGIR, pages 111–119, 2001.

[3] D. M. Pennock, E. Horvitz, S. Lawrence, and C. L. Giles.

Collaborative ﬁltering by personality diagnosis. In UAI,
2000.

[4] R. Herbrich, T. Graepel, and K. Obermayer. Large margin

rank boundaries for ordinal regression. In Advances in
Large Margin Classiﬁers, pages 115–132, 2000.

[5] T. Joachims. Optimizing search engines using clickthrough

data. In SIGKDD, pages 133–142, 2002.

[6] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An eﬃcient

boosting algorithm for combining preferences. J. Machine
Learning Research, 4:933–969, 2003.

[7] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier,

M. Deeds, N. Hamilton, and G. N. Hullender. Learning to
rank using gradient descent. In ICML, pages 89–96, 2005.
[8] J. Gao, H. Qi, X. Xia, and J.-Y. Nie. Discriminant model
for information retrieval. In SIGIR, pages 290–297, 2005.

Figure 7: Running time of the MMR algorithm for
diﬀerent numbers of movies rated by test users

5. CONCLUSION

In this paper, we propose the problem of ranking reﬁne-
ment, whose goal is to improve a given ranking function
by a small number of labeled instances. The key challenge
in combining the ranking information from the base ranker
and the labeled instances arises from the fact that the in-
formation in the base ranker tends to be inaccurate and the
information from the training data tends to be noisy. We
present a boosting algorithm for ranking reﬁnement that is
resilient to the errors. Empirical studies with relevance feed-
back and recommender system show promising performance

05101520250.10.20.30.40.50.60.70.8Top DocumentsPrecisionBase RankerRocchioSVMMRRLRR−WorstLRR−Best051015202500.10.20.30.40.50.60.7Top DocumentsPrecisionBase RankerRocchioSVMMRRLRR−WorstLRR−Best051015202500.10.20.30.40.50.60.7Top DocumentsPrecisionBase RankerRocchioSVMMRRLRR−WorstLRR−Best05101520250.30.40.50.60.70.80.91Top DocumentsNDCGBase RankerRocchioSVMMRRLRR−WorstLRR−Best051015202500.10.20.30.40.50.60.7Top DocumentsNDCGBase RankerRocchioSVMMRRLRR−WorstLRR−Best051015202500.10.20.30.40.50.60.7Top DocumentsNDCGBase RankerRocchioSVMMRRLRR−WorstLRR−Best0501001502002503003504004500.511.522.533.5Number of MoviesTime (Seconds)404WWW 2008 / Refereed Track: Search - Ranking & Retrieval EnhancementApril 21-25, 2008 · Beijing, China(a) OHSUMED data set

(b) Trec 2003 data set

(c) Trec 2004 data set

Figure 4: Precision of MRR with diﬀerent base rankers for relevance feedback

(a) OHSUMED data set

(b) Trec 2003 data set

(c) Trec 2004 data set

Figure 5: Precision of MRR with diﬀerent numbers of feedback documents for relevance feedback

[9] Y. Cao, J. Xu, H. Li, Y. Huang, and H.-W. Hon. Adapting

ranking svm to document retrieval. In SIGIR, pages
186–193, 2006.

[10] J. Xu and H. Li. A boosting algorithm for information

retrieval. In SIGIR, pages 473–480, 2007.

[11] A. Shashua and A. Levin. Ranking with large margin

principle: Two approaches. In NIPS, 2003.

[12] W. Chu and Z. Ghahramani. Gaussian processes for ordinal

regression. Technical report, 2004.

[13] D. Harman. Relevance feedback revisited. In SIGIR, 1992.
[14] Mark Montague and Javed A. Aslam. Condorcet fusion for

improved retrieval. In CIKM ’02: Proceedings of the
eleventh international conference on Information and
knowledge management, pages 538–548. ACM, 2002.

[15] R.T. Rockafellar. Convex analysis. Princeton University

Press, Princeton, N.J., 1970.

[16] R. E. Steuer. Multiple Criteria Optimization: Theory,

Computation and Application. John Wiley, 546 pp, 1986.

[17] Robert E. Schapire. Theoretical views of boosting and

applications. In Algorithmic Learning Theory, 10th
International Conference, ALT ’99, volume 1720, pages
13–25. Springer, 1999.

[18] Christopher J. C. Burges. A tutorial on support vector

machines for pattern recognition. Data Min. Knowl.
Discov., 2(2):121–167, 1998.

[19] GroupLens. MovieLens Data sets.

http://www.grouplens.org/node/12, 2006.

[20] Microsoft Research Asia. LETOR: Benchmark Datasets for

Learning to Rank.
http://research.microsoft.com/users/tyliu/letor/, 2006.

[21] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulated

gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422–446, 2002.

APPENDIX
A. PROOF OF THEOREM 1

Proof. First, note that the objective function Lp is con-
vex in terms of F. This is because Lp can be expanded as
follows:

n(cid:88)

Lp =

Ti,jWi,j exp(Fj − Fi + Fk − Fl)

i,j,k,l=1

Since exp(Fj − Fi + Fk − Fl) is a convex function, Lp is
convex. Since Lp is a convex function, the solution found
by minimizing Lp will always be global optimal, instead of
local optimal.
Second, to show that the optimal solution found by min-
imizing Lp is Pareto eﬃcient, we prove by contradiction.
Let F∗ denote the global minimizer of function Lp. By
assuming that Theorem 1 is not correct, there will exist

a solution F (cid:54)= F∗ that either (1) (cid:99)errw(F) < (cid:99)errw(F∗)
and (cid:99)errt(F) ≤ (cid:99)errt(F∗), or (2) (cid:99)errw(F) ≤ (cid:99)errw(F∗) and
(cid:99)errt(F) < (cid:99)errt(F∗). We can easily infer Lp(F) < Lp(F∗)
since (1) both (cid:99)errw and (cid:99)errt are non-negative for any solu-
tion F, and (2) Lp = (cid:99)errw × (cid:99)errt. Clearly, this conclusion

contracts the fact that F∗ is a global minimizer of Lp.

051015202500.10.20.30.40.50.60.70.8Top DocumentsPrecisionBase Ranker−7MRR−7Base Ranker−11MRR−11Base Ranker−21MRR−21051015202500.10.20.30.40.50.60.7Top DocumentsPrecisionBase Ranker−16MRR−16Base Ranker−21MRR−21Base Ranker−36MRR−36051015202500.10.20.30.40.50.60.7Top DocumentsPrecisionBase Ranker−16MRR−16Base Ranker−21MRR−21Base Ranker−36MRR−3605101520250.20.30.40.50.60.70.80.91Top DocumentsPrecisionBase RankerMRR−5MRR−10MRR−15MRR−20051015202500.10.20.30.40.50.60.7Top DocumentsPrecisionBase RankerMRR−5MRR−10MRR−15MRR−20051015202500.10.20.30.40.50.60.70.8Top DocumentsPrecisionBase RankerMRR−5MRR−10MRR−15MRR−20405WWW 2008 / Refereed Track: Search - Ranking & Retrieval EnhancementApril 21-25, 2008 · Beijing, China(a) Precision chart

(b) NDCG chart

(c) Robustness

Figure 6: Precision and NDCG of recommender system for diﬀerent algorithms

(cid:33)
(cid:33)

(cid:33)

(cid:33)

(cid:33)

(cid:33)

Using the deﬁnition of α in (16), we have

(cid:33)(cid:195)

˜Lp
Lp

≤ −2+

(cid:118)(cid:117)(cid:117)(cid:116)(cid:195)

2

i,j=1

i,j=1

log

γi,jδ(fj, 0)δ(fi, 1)

√
µν

n(cid:88)

γi,jδ(fj, 1)δ(fi, 0)

= −2 + 2

n(cid:88)
(cid:80)n
In the above, we use the deﬁnitions of µ and ν in Theorem 1
i,j=1 γi,j ≥ µ + ν,
to simplify the expression. Since 2 =
we have
µν = −(cid:161)√
µt − √

≤ −2 + 2
µν
√
≤ −µ − ν + 2

√
≤ rt = − (

µ − √

We thus have

(cid:162)2

˜Lp
Lp

νt)2

√

log

log

ν

Lt
p
Lt−1

p

Substituting the above expression for rt into (20), and fur-
ther using the fact

n(cid:88)

L0 =

Ti,j + Wi,j,

(ai,j + bi,j) exp(α(fj − fi))

we obtain the result in Theorem 2.

i,j=1

B. PROOF OF LEMMA 1

Proof. Since ˜F (x) = F (x) + αf (x), we have

(cid:195)
(cid:195)
(cid:195)

i,j=1

n(cid:88)
n(cid:88)
n(cid:88)

i,j=1

˜Lp
Lp

=

=

Wi,j exp(Fj − Fi + α(fj − fi))

×

Ti,j exp(Fj − Fi + α(fj − fi))

(cid:33)(cid:195)

n(cid:88)

ai,j exp(α(fj − fi)

bi,j exp(α(fj − fi)

i,j=1

i,j=1

where ai,j and bi,j are deﬁned in (11) and (12). Thus, we
have an upper bound of the log ratio as follows

log

˜Lp
L

= log

ai,j exp(α(fj − fi))

bi,j exp(α(fj − fi))

(cid:195)

n(cid:88)
(cid:195)
n(cid:88)
n(cid:88)

i,j=1

i,j=1

+ log

≤ −2 +

i,j=1

The second inequality follows the concaveness of the loga-
rithm function, i.e., log x ≤ x − 1 for any x > 0.

C. PROOF OF THEOREM 2

Proof. Using the upper bound expressed in Lemma 1,

we have

log

˜Lp
Lp

+ 2 ≤

=

i,j=1

n(cid:88)
(cid:195)
n(cid:88)
(cid:195)
n(cid:88)

i,j=1

+

i,j=1

γi,j exp(α(fj − fi))

γi,jδ(fj, 1)δ(fi, 0)

exp(α)

(cid:33)

(cid:33)

D. PROOF OF THEOREM 3

Proof. We rewrite the quantity θ as follows:

=

fi

i=1

i=1

θ =

(cid:33)

fiyi|wi|

γi,j − γj,i

(cid:195)
n(cid:88)

n(cid:88)
n(cid:88)
n(cid:88)
(cid:161)√
≥ (cid:161)√
µ − √
µ − ν =
(cid:162)2. Substituting this result into the
µ − √
µ − √

γi,j(fi − fj) = µ − ν

(cid:162)(cid:161)√
(cid:162)2 ,

√
ν

(cid:162)

µ +

i,j=1

j=1

=

ν

ν

ν

Since

we have θ ≥(cid:161)√

γi,jδ(fj, 0)δ(fi, 1)

exp(−α)

expression of Theorem 2, we have Theorem 3.

05101520250.20.30.40.50.60.70.80.9Top DocumentsPrecisionBase RankerRocchioSVMMRRLRR−WorstLRR−Best05101520250.740.760.780.80.820.840.860.880.90.92Top DocumentsNDCGBase RankerRocchioSVMMRRLRR−WorstLRR−Best05101520250.30.350.40.450.50.550.60.650.70.750.8Top DocumentsPrecisionBase RankerMRR−5MRR−10MRR−15MRR−20406WWW 2008 / Refereed Track: Search - Ranking & Retrieval EnhancementApril 21-25, 2008 · Beijing, China
