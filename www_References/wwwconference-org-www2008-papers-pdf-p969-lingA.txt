[1] Google translator.

http://www.google.com/language_tools.

[2] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and

D. S. Modha. A generalized maximum entropy
approach to bregman co-clustering and matrix
approximation. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 509–514, 2004.

[3] N. Bel, C. Koster, and M. Villegas. Cross-Lingual Text
Categorization. Proceedings ECDL, 200:126–139, 2003.

[4] A. Blum and T. Mitchell. Combining labeled and

unlabeled data with co-training. In Proceedings of the
Eleventh Annual Conference on Computational
Learning Theory, pages 92–100, 1998.

[5] B. E. Boser, I. Guyon, and V. Vapnik. A training

algorithm for optimal margin classiﬁers. In
Proceedings of the Fifth Annual Workshop on
Computational Learning Theory, pages 144–152, 1992.
[6] T. M. Cover and P. E. Hart. Nearest neighbor pattern

classiﬁcation. IEEE Transactions on Information
Theory, 13:21–27, 1967.

[7] T. M. Cover and J. A. Thomas. Elements of

information theory. Wiley-Interscience, New York,
NY, USA, 1991.

[10] I. S. Dhillon, S. Mallela, and D. S. Modha.

Information-theoretic co-clustering. In Proceedings of
the Ninth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2003.

[11] A. Gliozzo and C. Strapparava. Cross language Text

Categorization by acquiring Multilingual Domain
Models from Comparable Corpora. Proc. of the ACL
Workshop on Building and Using Parallel Texts (in
conjunction of ACL-05), 2005.

[12] T. Joachims. SVM light. Software available at

http://svmlight.joachims.org.

[13] T. Joachims. Transductive inference for text

classiﬁcation using support vector machines. In
Proceedings of the Sixteenth International Conference
on Machine Learning, pages 200–209, 1999.

[14] S. Kullback and R. A. Leibler. On information and

suﬃciency. Annals of Mathematical Statistics,
22(1):79–86, 1951.

[15] K. Lang. Newsweeder: Learning to ﬁlter netnews. In
Proceedings of the Twelfth International Conference
on Machine Learning, pages 331–339, 1995.
[16] D. D. Lewis. Reuters-21578 test collection.

http://www.daviddlewis.com/.

[17] D. D. Lewis. Representation and learning in

information retrieval. PhD thesis, Amherst, MA,
USA, 1992.

[18] Y. Li and J. Shawe-Taylor. Advanced learning

algorithms for cross-language patent retrieval and
classiﬁcation. Information Processing and
Management, 43(5):1183–1199, 2007.

[19] Y. Liu, Y. Fu, M. Zhang, S. Ma, and L. Ru.

Automatic search engine performance evaluation with
click-through data analysis. Proceedings of the 16th
international conference on World Wide Web, pages
1133–1134, 2007.

[20] X. Ni, G. Xue, X. Ling, Y. Yu, and Q. Yang. Exploring

in the weblog space by detecting informative and
aﬀective articles. Proceedings of the 16th international
conference on World Wide Web, pages 281–290, 2007.

[21] K. Nigam, A. K. McCallum, S. Thrun, and

T. Mitchell. Text classiﬁcation from labeled and
unlabeled documents using em. Machine Learning,
39(2-3):103–134, 2000.

[22] ODP. Open directory project. http://www.dmoz.com/.
[23] J. Olsson, D. Oard, and J. Hajiˇc. Cross-language text

classiﬁcation. Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 645–646,
2005.

[24] M. F. Porter. An algorithm for suﬃx stripping.

Program, 14(3):130–137, 1980.

[8] M. Day, C. Ong, and W. Hsu. Question Classiﬁcation

[25] J. R. Quinlan. C4.5: Programs for Machine Learning.

in English-Chinese Cross-Language Question
Answering: An Integrated Genetic Algorithm and
Machine Learning Approach. In Proceedings of the
2007 IEEE International Conference on Information
Reuse and Integration, pages 203–208, 2007.

[9] I. S. Dhillon, S. Mallela, and R. Kumar. Enhanced

word clustering for hierarchical text classiﬁcation. In
Proceedings of the Eighth ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 191–200, 2002.

Morgan Kaufmann, 1993.

[26] L. Rigutini, M. Maggini, and B. Liu. An em based

training algorithm for cross-language text
categorization. In Proceedings of the 2005
IEEE/WIC/ACM International Conference on Web
Intelligence, pages 529–535, 2005.

[27] N. Slonim. The Information Bottleneck: Theory and

Applications. Unpublished doctoral dissertation,
Hebrew University, Jerusalem, Israel, 2002.

977WWW 2008 / Alternate Track: WWW in China - Mining the Chinese WebApril 21-25, 2008 · Beijing, China[28] N. Slonim and Y. Weiss. Maximum likelihood and the

information bottleneck. Advances in Neural
Information Processing Systems, 15, 2002.

[29] N. Tishby, F. C. Pereira, and W. Bialek. The

information bottleneck method. In Proceedings of the
Thirty-seventh Annual Allerton Conference on
Communication, Control and Computing, pages
368–377, 1999.

[30] Y.-C. Wu, K.-C. Tsai, and J.-C. Yang. Two-pass

named entity classiﬁcation for cross language question
answering. In Proceedings of NTCIR-6 Workshop
Meeting, pages 168–174, 2007.

[31] Y. Yang and J. O. Pedersen. A comparative study on
feature selection in text categorization. In Proceedings
of Fourteenth International Conference on Machine
Learning, pages 144–152, 1997.

[32] X. Zhu. Semi-supervised learning literature survey.

Technical Report 1530, University of
Wisconsin–Madison, 2006.

APPENDIX
In this appendix, we provide the detailed proof to Lemmas
1 and 2, and Theorem 1.

A. PROOF TO LEMMA 1

Proof.

X
I(X; Y ) − I( ˜X; Y )
 X

y∈Y

p(x, y) log

X
X
X
X

˜x∈ ˜X

x∈˜x

X
X
X
X

y∈Y

x∈˜x

y∈Y

=

=

˜x∈ ˜X
−
X
X

˜x∈ ˜X

p(x, y) log

p(x, y) log
=
=D(p(X, Y )||˜p(X, Y )) .

˜x∈ ˜X

y∈Y

x∈˜x

p(x, y)
p(x)p(y)

!

p(x, y)

p(˜x, y) p(x)
p(˜x)
p(x, y)
˜p(x, y)

p(x, y)

log

p(˜x, y)
p(˜x)p(y)

x∈˜x

B. PROOF TO LEMMA 2
X
X

Proof.
D(p(X, Y )||˜p(X, Y )) =

X

˜x∈ ˜X

y∈Y

x∈˜x

p(x, y) log

p(x, y)
˜p(x, y)

.

Since

we have

˜p(x, y) = p(˜x, y)p(x|˜x) = p(˜x, y)

p(x)
p(˜x)
= p(x)p(y|˜x) = p(x)˜p(y|˜x) ,

D(p(X, Y )||˜p(X, Y )) =

=

=

X

x∈˜x

p(x)p(y|x) log
X

p(x)p(y|x)
p(x)˜p(y|˜x)
p(y|x)
˜p(y|˜x)
p(x)D(p(Y |x)||˜p(Y |˜x)) .

p(y|x) log

p(x)

y∈Y

X
X
X

˜x∈ ˜X

˜x∈ ˜X

y∈Y

X
X
X

x∈˜x

˜x∈ ˜X

x∈˜x

C. PROOF TO THEOREM 1
X
Proof. Based on Lemma 2, we have

X

X

D(p(X, Y )||˜p(t)

(X, Y )) =

p(x)

(t)

˜x:h

x∈˜x

y∈Y

From the Steps 6 and 9 in Algorithm 1,

p(y|x) log

p(y|x)
˜p(t)(y|˜x)

.

D(p(Y |x)||˜p(t−1)(Y |˜x))

arg min˜x∈ ˜X
h(t)(x) = h(t−1)(x)

x ∈ Xo
x ∈ Xi

.

j

h(t)

(x) =

Thus,

«

.

1

˜p(t)(y|˜x)

p(x)

(X, Y ))

X
X
D(p(X, Y )||˜p(t)
p(y|x) log
X
X
X
X

X
X
X

p(x)

y∈Y

y∈Y

x∈˜x

x∈˜x

(t+1)

˜x:h

˜x:h

(t)

p(y|x)

˜p(t)(y|h(t+1)(x))
p(y|x) log
„

p(y|x)
˜p(t)(y|˜x)
log p(y|x) + log

≥

=

=

(t+1)

˜x:h

˜x:h

Here, X
X
X
X

˜x:h

˜x:h

=

=

≥

p(x)

x∈˜x

y∈Y

p(y|x)
X
X

X
 X

x∈˜x

p(x)

(t+1)

(t+1)

x∈˜x

y∈Y

(t+1)

˜p(t+1)

˜p(t+1)

(˜x)

(˜x)

(t+1)

˜x:h

y∈Y

p(y|x) log
!

y∈Y
p(x)p(y|x)
X
X

˜p(t+1)

y∈Y

1

˜p(t)(y|˜x)

log

(y|˜x) log

1

˜p(t)(y|˜x)
1

˜p(t)(y|˜x)

˜p(t+1)

(y|˜x) log

1

˜p(t+1)(y|˜x)

.

Note that, the last inequality follows by the non-negativity
of the Kullback-Leibler divergence, that

X

y∈Y

˜p(t+1)

(y|˜x) log

1

˜p(t)(y|˜x)

−

X

y∈Y

˜p(t+1)

(y|˜x) log
(Y |˜x)||˜p(t)

= D(˜p(t+1)

Thus,

≥

D(p(X, Y )||˜p(t)
X
X

X
X

x∈˜x

(t+1)

p(x)

˜x:h

X
X

y∈Y

p(x)
=
y∈Y
=D(p(X, Y )||˜p(t+1)

x∈˜x

(t+1)

˜x:h

(X, Y ))

p(y|x)

„

log p(y|x) + log

p(y|x) log

p(y|x)

˜p(t+1)(y|˜x)

(X, Y )) .

1

˜p(t+1)(y|˜x)
(Y |˜x)) ≥ 0 .
«

1

˜p(t+1)(y|˜x)

978WWW 2008 / Alternate Track: WWW in China - Mining the Chinese WebApril 21-25, 2008 · Beijing, China
