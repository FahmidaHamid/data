[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.

Learning user interaction models for predicting web
search result preferences. In Proceedings of the 29th
annual international ACM SIGIR conference on
Research and development in information retrieval
(SIGIR), pages 3–10, 2006.

[2] M. Beal and Z. Ghahraman. Variational bayesian
learning of directed graphical models with hidden
variables. Bayesian Analysis, 1(4):793–832, 2006.

[3] A. Broder. A taxonomy of web search. SIGIR Forum,

36(2):3–10, 2002.

[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier,

M. Deeds, N. Hamilton, and G. Hullender. Learning to
rank using gradient descent. In Proceedings of the
22nd international conference on Machine learning,
pages 89–96, 2005.

[5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W.
Hon. Adapting ranking svm to document retrieval. In
Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
informat ion retrieval, 2006.

[6] B. Carlin and T. Louis. Bayes and Empirical Bayes
Methods for Data Analysis. Chapman & Hall/CRC,
2000.

[7] B. Carterette and R. Jones. Evaluating search engines

by modeling the relationship between relevance and
clicks. In J. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 217–224. MIT Press, 2008.

[8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An
experimental comparison of click position-bias models.
In WSDM ’08: Proceedings of the international
conference on Web search and web data mining, pages
87–94. ACM, 2008.

[9] N. M. Dempster, A. Laird, and D. B. Rubin.

Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:185–197, 1977.

[10] G. Dupret and B. Piwowarski. User browsing model to

predict search engine click data from past
observations. In SIGIR 08: Proceedings of the 31st
Annual International Conference on Research and
Development in Information Retrieval, 2008.

[11] Z. Ghahramani. Learning dynamic bayesian network.

In C. L. Giles and M. Gori, editors, Adaptive
processing of temporal information, Lecture notes in
artiﬁcial intelligence. Springer-Verlag, 1998.

[12] K. Jarvelin and J. Kekalainen. Cumulated gain-based

evaluation of IR techniques. ACM Transactions on
Information Systems, 20(4):422–446, 2002.

[13] T. Joachims. Optimizing search engines using

clickthrough data. In ACM SIGKDD Conference on

WWW 2009 MADRID!Track: Data Mining / Session: Click Models9Knowledge Discovery and Data Mining (KDD), pages
133–142, 2002.

[14] T. Joachims. Evaluating retrieval peformance using

clickthrough data. In Text mining, pages 79–96, 2003.

[15] T. Joachims, L. A. Granka, B. Pan, H. Hembrooke,

and G. Gay. Accurately interpreting clickthrough data
as implicit feedback. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 154–161,
2005.

[16] M. Richardson, E. Dominowska, and R. Ragno.

Predicting clicks: estimating the click-through rate for
new ads. In Proceedings of the 16th international
conference on World Wide Web (WWW), pages
521–530, 2007.

[17] M. Richardson, E. Dominowska, and R. Ragno.

Predicting clicks: estimating the click-through rate for
new ads. In WWW ’07: Proceedings of the 16th
international conference on World Wide Web, pages
521–530. ACM, 2007.

[18] S. E. Robertson and S. Walker. Some simple eﬀective

approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in informat ion retrieval,
1994.

[19] V. Zhang and R. Jones. Comparing click logs and

editorial labels for training query rewriting. In Query
Log Analysis: Social And Technological Challenges. A
workshop at the 16th International World Wide Web
Conference, 2007.

[20] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen,

and G. Sun. A general boosting method and its
application to learning ranking functions for web
search. In Advances in Neural Information Processing
Systems 20, pages 1697–1704. MIT Press, 2008.

[21] D. Zhou, L. Bolelli, J. Li, C. L. Giles, and H. Zha.
Learning user clicks in web search. In International
Joint Conference on Artiﬁcial Intelligence (IJCAI07),
2007.

APPENDIX
We give here some details about the inference in our DBN
outlined in section 3.3. Suppose that there are N sessions
and denote Aj, Sj and Ej the vector of hidden variables
associated with the j-th session. Also let dj
i be the url in
position i of the j-th session.

M step
Given some posterior distributions Q(Aj
hidden variables, the update of au and su are as follows:

i ) and Q(Sj

i ) on the

NX

10X

j=1

i=1

I(dj

i = u)

au = arg max

a

“

”

Q(Aj

i = 0) log(1 − a) + Q(Aj

i = 1) log(a)

+ log P (a).

j=1

i=1

NX

10X

j=1

i=1

I(dj

i = u, C j

i = 1)

su = arg max

s

“

Q(Sj

i = 0) log(1 − s) + Q(Sj

i = 1) log(s)

”

+ log P (s).

In the above equations, I is the indicator function; P (a)
and P (s) are the prior beta distributions. We simply took
a beta distribution with parameters (1,1), but these priors
can be learned using a variational approximation [2]. The
maximizers can of course be easily computed in closed form.
Because of the priors, this EM algorithm does not converge
to the maximum likelihood solution but to a mode of the
posterior: it is a maximum a posteriori (MAP) solution.
E step
The M steps consists in computing the posterior probabili-
ties:

Q(Aj

Q(Sj

i ) := P (Aj
i ) := P (Sj

i|C j, au, su, γ)
i |C j, au, su, γ).

In the rest of this section, we drop for convenience the con-
ditioning on au, su and γ. As in the forward-backward algo-
rithm, we deﬁne the following variables:

αi(e) = P (C j

1, . . . , C j

βi(e) = P (C j

i , . . . , C j

i−1, Ei = e),
10|Ei = e)

And one can easily derived the recursion formula:
)P (Ei+1 = e, Ci|Ei = e
(cid:48)

αi+1(e) =

αi(e

(cid:48)

).

X
X

e(cid:48)∈{0,1}

e(cid:48)∈{0,1}

βi−1(e) =

(cid:48)

βi(e

)P (Ei = e

(cid:48)

, Ci−1|Ei−1 = e).

The conditional probabilities in the above equation are

computed as follows:

P (Ei+1, Ci|Ei) =X

P (Ei+1|Si = s, Ei)P (Si = s|Ci)P (Ci|Ei).

s∈{0,1}
Conﬁdence
Remember that the latent variables au and su will later be
used as targets for learning a ranking function. It is thus im-
portant to know the conﬁdence associated with these values.
A standard way of deriving a conﬁdence is to compute the
second derivative of the log likelihood function at the MAP
solution. This can be seen as doing a Laplace approximation
of the posterior distribution.

ple expression because P (C j|au, su, γ) =P

The second derivative in our case turns out to have a sim-
aj ,ej ,sj P (C j|Aj =
aj, Ej = ej)P (Aj = aj, Ej = ej, Sj = sj|au, su, γ) is linear
in au and su. The result is simply the average squared gra-
dient (similar equation stands for su):

∂2
∂a2
u

NX
10X
NX

j=1

log P (C j|au, su, γ) =

„ Q(Aj

i = 1)
au

«2

.

(8)

− Q(Aj

i = 0)

1 − au

I(dj

i = u)

WWW 2009 MADRID!Track: Data Mining / Session: Click Models10
