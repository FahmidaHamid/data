[1] Gabriel Altmann. Das Problem der Datenhomogenit¨at.

In Glottometrika 13. Brockmeyer, 1992.

[2] D. Gibson, K. Punera, and A. Tomkins. The volume

and evolution of web page templates. In WWW’05.
[3] Chr. Kohlsch¨utter and W. Nejdl. A Densitometric

Approach to Web Page Segmentation. In CIKM 2008.
[4] D. Lavalette. A general purpose ranking variable with
applications to various ranking laws. In Exact Methods
in the Study of Language and Text. 2007.

Figure 1: Density Distribution Model

3. TERM TYPICALITY

To make a statement on the meaning of the determined
two classes, the content of these classes, i.e., the term vocab-
ulary, needs to be analyzed. If the two classes are diﬀerent,
then the contained token vocabulary should also expose no-
ticeable diﬀerences. As we want to understand the peculiar-
ities of the two classes C1 and C2, which are roughly repre-
sented by the two partitions π1 ((cid:48) ≤ 8) and π2 ((cid:48) ≥ 9), the
partition-speciﬁc term document frequencies are compared.
We may expect that terms that are typical for C1 appear
much more often in π1 than in π2, and vice versa. I examine
this relationship by computing the corresponding document
frequency ratios. The normalized ratio follows a power law
distribution of the form y = c (x/(1− x))−a1 with a1 = 0.39
and c = 0.01 (R2 = 0.9468, RMSE = 0.0034). This type is a
generalization of Zipf’s law [4]. In our case, we can interpret
the ratio x/(1 − x) as the combination of two Zipﬁan sub-
sets, a top-ranked and a bottom-ranked one, which mutually
inﬂuence the curve. In fact the frequencies of the considered
terms apparently are Zipﬁan, too, and for both partitions
enough typical terms exist. To avoid over-interpreting the
impact of rarely occurring terms, the analysis is limited to
terms with a collection-wide document frequency w1∪2 of at
least 100. For these terms, I compute the term typicality
ε(t), which I deﬁne as the logarithmic ratio of the corre-
sponding document frequencies w1, w2 of the examined term
t in the two partitions. The ratio is normalized by the loga-
rithm to base N + 1 with N being the number of documents
in the corpus (i.e., the maximum document frequency):

ε(t) = logN +1

w2(t) + 1
w1(t) + 1

(2)

0510152025Text Density05x1061x1071.5x1072x107Number of wordsMeasuredClass 1 (Beta d.)Normal DistributionClass 2 (Beta d.)Complete FitWWW 2009 MADRID!Poster Sessions: Thursday, April 23, 20091166
