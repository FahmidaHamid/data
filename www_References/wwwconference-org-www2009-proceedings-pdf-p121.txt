[1] R. Bradley and M. Terry. Rank analysis of incomplete

block designs, i. the method of paired comparisons.
Biometrika, pages 324–345, 1952.

[2] B. Carterette, P. N. Bennett, D. M. Chickering, and

S. T. Dumais. Here or there: Preference judgments for
relevance. In ECIR 2008, 2008.

[3] W. Chu and Z. Ghahramani. Preference learning with

gaussian processes. In ICML 2005, 2005.

[4] C. Cleverdon. The signiﬁcance of the cranﬁeld tests on

index languages. In SIGIR ’91, pages 3–12, 1991.

[5] W. Cohen, R. Schapire, and Y. Singer. Learning to

order things. Journal of Artiﬁcial Intelligence
Research, 10:243–270, 1999.

[6] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.

Rank aggregation methods for the web. In WWW ’10,
pages 613–622, 2001.

[7] J. F¨urnkranz and E. H¨ullermeier. Pairwise preference

learning and ranking. In ECML 2003, 2003.

[8] R. Herbrich and T. Graepel. TrueskillTM: A bayesian

skill rating system. Technical Report
MSR-TR-2006-80, Microsoft Research, June 2006.
ftp://ftp.research.microsoft.com/pub/tr/TR-2006-
80.pdf.

[9] T. Joachims. Optimizing search engines using

clickthrough data. In KDD 2002, 2002.

[10] E. Law, L. von Ahn, R. Dannenberg, and

M. Crawford. Tagatune: A game for music and sound
annotation. In Proceedings of the 8th International
Conference on Music Information Retrieval, 2007.

WWW 2009 MADRID!Track: Data Mining / Session: Opinions129[11] R. Luce. Individual Choice Behaviours: A Theoretical

Analysis. J. Wiley, New York, 1959.

[12] F. Radlinski and T. Joachims. Minimally invasive

randomization for collecting unbiased preferences from
clickthrough logs. In AAAI 2005, 2005.

[13] F. Radlinski, R. Kleinberg, and T. Joachims. Learning

diverse rankings with multi-armed bandits. In ICML
2008, 2008.

[14] S. E. Robertson. The probability ranking principle in

ir. Journal of Documentation, 33:294–304, 1977.
Reprinted in: K. Sparck Jones and P. Willett (eds),
Readings in Information Retrieval. Morgan
Kaufmann, 1997. (pp 281-286).

[15] D. Stern, R. Herbrich, and T. Graepel. Bayesian

pattern ranking for move prediction in the game of go.
In ICML ’06, 2006.

[16] L. Thurstone. A law of comparative judgment.

Psychological Review, pages 273–286, 1927.

[17] L. Thurstone. Essays in Philosophy by Seventeen

Doctors of Philosophy of the University of Chicago,
chapter The Measurement of Psychological Value.
Open Court, Chicago, 1929.

[18] L. Thurstone. The Measurement of Values. The

University of Chicago Press, Chicago, 1959.

[19] L. von Ahn and L. Dabbish. Labeling images with a

computer game. In ACM CHI, 2004.

[20] L. von Ahn, R. Liu, and M. Blum. Peekaboom: A
game for locating objects in images. In ACM CHI,
2006.

APPENDIX
A. PAIRWISE MODEL DERIVATIONS

In this appendix, we derive results related to the pairwise
model of Section 4.2.2. We assume terminology introduced
throughout the paper and particularly that introduced in
Sections 4.1 and 4.2.2. First, we demonstrate the derivation
of Equation 4. If we desire to set the relevance score param-
eters by maximum likelihood, then we must maximize the
following log-likelihood.

Λ(D) = log

i=1

Let B+

(cid:89)

P (ri (cid:31) ci,j)

j∈{j|j∈{1,...,S(i)},ri(cid:54)=ci,j}

N(cid:89)
i,j be the number of Ck ∈ D s.t. i, j ∈ Ck, rk = i.
I(cid:89)
I(cid:89)
I(cid:89)
I(cid:89)
I(cid:88)
(cid:2)B+

j,i log[1 − P (i (cid:31) j)](cid:3)

i,j log P (i (cid:31) j) + B+

i,j [1 − P (i (cid:31) j)]B

P (i (cid:31) j)B

Since P (i (cid:31) j) = 1 − P (j (cid:31) i)
P (i (cid:31) j)B

= log

I(cid:88)

j=1|j(cid:54)=i

= log

j=i+1

+
i,j

+
j,i

i=1

i=1

=

+

i=1

j=i+1

Let ni,j = |{Ck | Ck ∈ D, i, j ∈ Ck, (rk = i or rk = j)}|
Note B+

j,i = ni,j = nj,i

i,j + B+

I(cid:88)
I(cid:88)

i=1

i=1

I(cid:88)
I(cid:88)

j=i+1

j=i+1

[B+

i,j log P (i (cid:31) j)
+ (ni,j − B+
[ni,j log[1 − P (i (cid:31) j)]
P (i (cid:31) j)

i,j) log[1 − P (i (cid:31) j)]]

+ B+

]

i,j log

[ni,j log

1 − P (i (cid:31) j)

I(cid:88)
I(cid:88)
I(cid:88)
I(cid:88)
i,j(si − sj) − ni,j log[1 + exp{si − sj}](cid:3) (8)
(cid:2)B+

1 + exp{si − sj}
i,j log exp{si − sj}]

+ B+

j=i+1

i=1

1

i=1

j=i+1

=

=

=

=

1

Next we demonstrate brieﬂy that given the right encoding
of a labeled example, the model is identical to a two-class
(c ∈ {0, 1}) logistic regression model that does not have
a bias term. That is, a model where P (c = 1 | (cid:126)s, (cid:126)x) =
1+exp{(cid:126)s·(cid:126)x} and P (c = 0 | (cid:126)s, (cid:126)x) =
exp{(cid:126)s·(cid:126)x}
1+exp{(cid:126)s·(cid:126)x} where (cid:126)x is the
feature encoding for a pairwise preference described below.
In particular, the original comparisons should be encoded
in the following way. To represent this as a logistic regres-
sion problem, each comparison set S(i) gets converted to
|S(i)| − 1 labeled examples for logistic regression. Similar
to how a choice was modeled as the pairs of preferences
needed to indicate the chosen item was preferred to each of
the remaining items, each example in the logistic regression
representation will encode one of these preferences. There is
one feature in the model for each of the images. To encode
a preference that i (cid:31) j, all features are set to zero except for
the ith and jth feature. Then an arbitrary choice is made to
set one of these features to 1 and the other to −1. The class
variable is set to “1” if the image whose feature bit was set
to 1 was chosen, and is set to “0” otherwise. Without loss of
generality, for convenience we will assume that the feature
with the lowest feature index is always set to 1 and the one
with the higher index is set to -1. Thus the class label will
be “1” if the image corresponding to the lower feature index
was chosen and “0” otherwise. Assume we have indices i, j
where the index i < j, then to encode i (cid:31) j, the ith bit is
set to 1, the jth bit is set to −1 and the class is 1. Now, we
simply must show that maximizing this likelihood results in
the same optimization formula as Equation 8.

11(j ∈ S(d)) [11(i = rd) (si − sj)

j=i+1

− (11(i = rd) + 11(j = rd)) log (1 + exp{si − sj})]

i,j(si − sj) − ni,j log[1 + exp{si − sj}](cid:3)
(cid:2)B+

(9)

Grouping like terms

I(cid:88)

I(cid:88)

=

i=1

j=i+1

11(i ∈ S(d))

I(cid:88)

j=i+1

N(cid:88)
N(cid:88)

d=1

d=1

i=1

I(cid:88)
I(cid:88)
I(cid:88)

i=1

=

11(i ∈ S(d)) ·

11(j ∈ S(d)) [11(i = rd) P (1 | (cid:126)s, (cid:126)x)
+ 11(j = rd) P (0 | (cid:126)s, (cid:126)x)]

WWW 2009 MADRID!Track: Data Mining / Session: Opinions130
