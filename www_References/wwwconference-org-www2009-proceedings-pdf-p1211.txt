[1] S. Bao, G. Xue, X. Wu, Y. Yu, B. Fei, and Z. Su.

Optimizing web search using social annotations. In WWW,
pages 501–510, 2007.

[2] O. Ben-Yitzhak, N. Golbandi, N. Har’El, R. Lempel,

A. Neumann, S. Ofek-Koifman, D. Sheinwald, E. Shekita,
B. Sznajder, and S. Yogev. Beyond basic faceted search. In
WSDM, 2008.

[3] R. Cilibrasi and P. M. B. Vit´anyi. Automatic meaning
discovery using google. In Kolmogorov Complexity and
Applications, 2006.

[4] P. A. Dmitriev, N. Eiron, M. Fontoura, and E. Shekita.
Using annotations in enterprise search. In WWW, pages
811–817, 2006.

[5] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can

social bookmarks improve web search. In WSDM, 2008.

Figure 1: The social search application.

tion (such as name, title, and group). Finally, people were
connected, as facets, to the pages they bookmarked, and to
their blog entries (as an author or as a commenter). Tags
were connected as facets to their related documents.

A static-score (boost) was given to each document based
on the amount of activity around it.
In essence, a page
which was bookmarked by many people, or a blog entry
that was heavily commented or rated, is more likely to be a
good search result than a document in which hardly anyone
expressed interest. Our evaluation showed that this boosting
signiﬁcantly increased the document search precision.

The social search Web application, codenamed Cow Search,
was made available to all users of IBM’s intranet. As ex-
plained above, the application enables searching with any of
the supported entity types (terms, people, tags), or a mix-
ture thereof, as queries. Using the faceted search library,
three ranked result lists are generated: documents (book-
marked pages, blog threads, and directory entries), related
people, and related tags. Tags are shown in a tag cloud (a
tag’s size is determined by its score) and people are shown
as a ranked list.

Figure 1 shows a screenshot of the application, given the
query “openID”. On the left (marked by 4) we see the most
relevant documents — a mix of blogs, Web pages and per-
sonal proﬁles. On the right (1) is the list of related people.
The related-tags cloud is shown in (2). (3) shows some addi-
tional facets, which aid navigation within the search results.

3. EVALUATION

A signiﬁcant part of this research was evaluating the eﬀec-
tiveness of social search in the enterprise. Our uniﬁed search
system returns three result lists for each query: documents,
people and tags. Therefore, to evaluate the quality of our
system we needed to evaluate the quality of all three lists.

The quality of document results was evaluated using the
standard IR evaluation methodology: We picked 50 real
users’ queries, and ran them on our search engine and IBM’s
default one. We took the top 30 documents returned for each
search, and asked human evaluators to examine each result
and judge its relevance to the query with three relevance
levels (not relevant, marginally relevant or highly relevant),
without knowing where the result came from. These judg-
ments were then compared against our system’s result set
using the NDCG (Normalized Discounted Cumulative Gain)
and and p@k (precision at top k) measures for each query,
and ﬁnally averaged over all 50 queries.

WWW 2009 MADRID!Poster Sessions: Friday, April 24, 20091212
