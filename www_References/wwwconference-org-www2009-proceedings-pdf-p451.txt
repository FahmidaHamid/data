[1] T. M. Apostol. Introduction to Analytic Number Theory.

Springer-Verlag, 1976.

[2] R. Baeza-Yates and G. Navarro. Block-addressing indices

for approximate text retrieval. Journal of the American
Society for Information Science, 51(1):69–82, 2000.

[3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information

Retrieval. Addison Wesley, 1999.

[4] A.-L. Barabasi. Linked: How Everything is Connected to

Everything Else and What It Means. Penguin Group, 2003.

[5] D. Bladford and G. Blelloch. Index compression through

document reordering. In Proceedings of the Data
Compression Conference, pages 342–351, 2002.

[6] P. Boldi and S. Vigna. The Webgraph framework i:
Compression techniques. In Proceedings of the 13th
International Conference on World Wide Web, pages
595–602, 2004.

[7] P. Boldi and S. Vigna. The Webgraph framework ii: Codes
for the world-wide web. In Data Compression Conference,
2004.

[8] A. Gelbukh and G. Sidorov. Zipf and Heaps laws’

coeﬃcients depend on language. In Proceedings of the 2nd
International Conference on Computational Linguistics
and Intelligent Text Processing, pages 332–335, 2001.

[9] L. Q. Ha, E. I. Sicilia-Garcia, J. Ming, and F. J. Smith.

Extension of Zipf’s law to word and character n-grams for
English and Chinese. Computational Linguistics and
Chinese Language Processing, 8(1):77–102, 2003.

[10] H. S. Heaps. Information Retrieval: Computational and

Theoretical Aspects. Academic Press, New York, 1978.

[11] R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins.

Trawling the Web for emerging cyber-communities.
Computer Networks, 31(11–16):1481–1493, 1999.

[12] W. Li. Random texts exhibit Zipf’s-law-like word frequency

distribution. IEEE Transactions on Information Theory,
38(6):1842–1845, 1992.

[13] B. Mandelbrot. An information theory of the statistical

structure of language. In W. Jackson, editor,
Communication Theory, pages 486–502. Academic Press,
1953.

[14] C. D. Manning, P. Raghavan, and H. Sch¨utze. Introduction

to Information Retrieval. Cambridge University Press,
2008.

[15] C. D. Manning and H. Sch¨utze. Foundations of Statistical
Natural Language Processing. MIT Press, Cambridge, MA,
1999.

[16] M. Mitzenmacher. Dynamic models for ﬁle sizes and double

Pareto distributions. Internet Mathematics, 1(3):305–333,
2003.

[17] M. Molloy and B. Reed. Graph Coloring and the

Probabilistic Method. Springer-Verlag, 2002.

[18] M. Newman, A.-L. Barabasi, and D. J. Watts. The

Structure and Dynamics of Networks. Princeton University
Press, 2006.

[19] W. J. Reed and M. Jorgensen. The double

Pareto-lognormal distribution - A new parametric model
for size distributions. Communications in Statistics:
Theory and Methods, 33(8):1733–1753, 2004.

[20] W.-Y. Shieh, T.-F. Chen, J. J.-J. Shann, and C.-P. Chung.

Inverted ﬁle compression through document identiﬁer
reassignment. Information Processing and Management,
39(1):117–131, 2003.

[21] F. Silvestri, R. Perego, and S. Orlando. Assigning

document identiﬁers to enhance compressibility of web
search indexes. In Proceedings of the Symposium on
Applied Computing, pages 600–605, 2004.

[22] H. A. Simon. On a class of skew distribution functions.

Biometrika, 42:425–440, 1955.

[23] D. C. van Leijenhorst and T. P. van der Weide. A formal

derivation of Heap’s law. Information Sciences,
170:263–272, 2005.

[24] D. Watts. Six Degrees: The Science of a Connected Age.

W. W. Norton, 2003.

[25] H. E. Williams and J. Zobel. Searchable words on the web.

International Journal on Digital Libraries, 5(2):99–105,
2005.

[26] I. H. Witten and T. C. Bell. Source models for natural

language text. International Journal Man-Machine Studies,
32(5):545–579, 1990.

[27] I. H. Witten, A. Moﬀat, and T. C. Bell. Managing

Gigabytes: Compressing and Indexing Documents and
Images. Morgan Kaufmann, 1999.

[28] G. K. Zipf. Human Behavior and the Principle of Least

Eﬀort. Addison-Wesley, Cambridge MA, 1949.

WWW 2009 MADRID!Track: Search / Session: Caching and Indices459That is,

+ 2(1− Pr[F]).

(T + H)

−1 =

APPENDIX
A. TAIL BOUNDS

To prove concentration bounds (i.e., to prove that random
variables are strongly concentrated around their expecta-
tion), we use the following version of Talagrand’s inequality
[17].

Theorem 7

(Talagrand’s inequality). Let X be a
non-negative random variable, not identically 0, determined
by n independent random variables X1, . . . , Xn such that
X = f (X1, . . . , Xn). Fix some D > 0, and let F be the
event that for the outcome x = (x1, . . . , xn) of the trials,
there exists a list of non-negative weights b1, . . . , bn such that

Pn
2. for any outcome y, it holds X(y) ≥ X(x) −P

i ≤ D, and

i=1 b2

1.

bi.

xi6=yi

Then, for any 0 ≤ t ≤ E[X],
Pr[|X − E[X]| > t + 60

√

D] ≤ 4 exp

„

«

− t2
8D

We will also use the well-known Chernoﬀ bound to prove

the concentration bounds.

Theorem 8

(Chernoff bound). Let X1, . . . , Xn be i.i.d.

Bernoulli random variables, 1 with probability p and 0 oth-
erwise. Let X =

i=1 Xi. Then

Pn

Pr[|X − np| > t] < 2 exp

„

«

− t2
3np

.

B. PROOF OF THEOREM 1

` k

´−α.

PC
PC
Z C

C

k=1

bution, H =

Proof. Let us use H to denote the “head” of the distri-
k=1 k−α, then H = C αh. By applying simple
−αdx ≤ C 1−α
1 − α

−αdx ≤ h ≤ 1 +

If h =
calculus,
C 1−α − 1

Z C

1 − α

(x + 1)

≤

x

1

0

we obtain

thus

Pm

h =

C 1−α
1 − α

± O(1),

H =

C
1 − α

± O(C α) =

C
1 − α

Pm

± o(C).

` k

´−β. Let

Now let us consider the “tail” T =
t =

It is known [1] that, for β > 1,

k=C+1 k−β.
sX

−β = ζ(β) − s1−β
β − 1

k

k=1

k=C+1

C

± O(s

−β).

This implies

t =

−β − CX

−β

k

k

mX

k=1

k=1

= ζ(β) − m1−β
β − 1
± O(m1−β + C

± O(m

=

C 1−β
β − 1

−β),

−β) − ζ(β) +

C 1−β
β − 1

± O(C

−β)

and

T = C βt =

C
β − 1

± O(m1−βC β + 1) =

C
β − 1

± o(C).

where the third equality is justiﬁed by m1−β = o(C 1−β),
which holds as C = o(m).
The normalizing factor of the probabilities is (T + H)−1.

„
„

+

C
β − 1

C
1 − α
C(β − α) ± o(C)
(1 − α)(β − 1)
(1 − α)(β − 1)
C(β − α) ± o(C)
· (1 − α)(β − 1)
1
C

β − α

=

=

=

«−1

± o(C)

«−1

± o(C

−1).

This proves the ﬁrst part of the theorem.

The probability that a newly drawn term t is chosen within
the ﬁrst c terms (that is, the probability that it happens to
be in the “head”) is
Pr[r(t) ≤ C] =

H

T + H
C

1−α ± o(C)

C

1−α + C

β−1 ± o(C)

1−α ± o(1)

1

β−1 ± o(1)
·

1

1−α + 1
1 ± o(1)
1 − α
β − 1 ± o(1)
β − α ± o(1)
β − 1
± o(1).
β − α

=

=

=

=

=

The claim follows.

(1 − α)(β − 1)

(β − 1) + (1 − α) ± o(1)

WWW 2009 MADRID!Track: Search / Session: Caching and Indices460
